{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPC-DI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection details\n",
    "host = \"localhost\"\n",
    "user = \"root\"\n",
    "password = \"password\"\n",
    "database = \"tpcdi_sf5\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{user}:{password}@{host}/{database}?allow_local_infile=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"..\\\\data\\\\sf5\\\\Batch1\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.read_csv(\n",
    "    r\"..\\data\\sf5\\Batch1\\Date.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"SK_DateID\", \"DateValue\", \"DateDesc\", \"CalendarYearID\", \"CalendarYearDesc\", \n",
    "        \"CalendarQtrID\", \"CalendarQtrDesc\", \"CalendarMonthID\", \"CalendarMonthDesc\", \n",
    "        \"CalendarWeekID\", \"CalendarWeekDesc\", \"DayOfWeekNum\", \"DayOfWeekDesc\", \n",
    "        \"FiscalYearID\", \"FiscalYearDesc\", \"FiscalQtrID\", \"FiscalQtrDesc\", \"HolidayFlag\"\n",
    "    ],\n",
    "    parse_dates=[\"DateValue\"],\n",
    "    dtype={\n",
    "        \"SK_DateID\": \"uint32\", \"DateDesc\": \"str\", \"CalendarYearID\": \"uint16\", \"CalendarYearDesc\": \"str\",\n",
    "        \"CalendarQtrID\": \"uint16\", \"CalendarQtrDesc\": \"str\", \"CalendarMonthID\": \"uint32\", \"CalendarMonthDesc\": \"str\",\n",
    "        \"CalendarWeekID\": \"uint32\", \"CalendarWeekDesc\": \"str\", \"DayOfWeekNum\": \"uint8\", \"DayOfWeekDesc\": \"str\",\n",
    "        \"FiscalYearID\": \"uint16\", \"FiscalYearDesc\": \"str\", \"FiscalQtrID\": \"uint16\", \"FiscalQtrDesc\": \"str\",\n",
    "        \"HolidayFlag\": \"bool\"\n",
    "    }\n",
    ")\n",
    "date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"SK_DateID\": sqlalchemy.types.BigInteger,\n",
    "    \"DateValue\": sqlalchemy.types.Date,\n",
    "    \"DateDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarYearID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarYearDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarQtrID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarQtrDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarMonthID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarMonthDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarWeekID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarWeekDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"DayOfWeekNum\": sqlalchemy.types.SmallInteger,\n",
    "    \"DayOfWeekDesc\": sqlalchemy.types.CHAR(length=10),\n",
    "    \"FiscalYearID\": sqlalchemy.types.Integer,\n",
    "    \"FiscalYearDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"FiscalQtrID\": sqlalchemy.types.Integer,\n",
    "    \"FiscalQtrDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"HolidayFlag\": sqlalchemy.types.Boolean\n",
    "}\n",
    "\n",
    "date_df.to_sql(name='dimdate', con=engine, if_exists='replace', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM dimdate limit 10\"\n",
    "result_df = pd.read_sql_query(query, engine)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "file_path = r\"..\\data\\sf5\\Batch1\\Time.txt\"\n",
    "dim_time_df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"SK_TimeID\", \"TimeValue\", \"HourID\", \"HourDesc\", \n",
    "        \"MinuteID\", \"MinuteDesc\", \"SecondID\", \"SecondDesc\",\n",
    "        \"MarketHoursFlag\", \"OfficeHoursFlag\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"SK_TimeID\": \"uint32\", \"HourID\": \"uint8\", \n",
    "        \"HourDesc\": \"str\", \"MinuteID\": \"uint8\", \"MinuteDesc\": \"str\", \n",
    "        \"SecondID\": \"uint8\", \"SecondDesc\": \"str\", \"MarketHoursFlag\": \"bool\", \n",
    "        \"OfficeHoursFlag\": \"bool\"\n",
    "    },\n",
    "    parse_dates=[\"TimeValue\"],\n",
    "    date_format=\"%H:%M:%S\"\n",
    ")\n",
    "dim_time_df['TimeValue'] = dim_time_df['TimeValue'].dt.time\n",
    "\n",
    "dim_time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"SK_TimeID\": sqlalchemy.types.BigInteger,\n",
    "    \"TimeValue\": sqlalchemy.types.Time,\n",
    "    \"HourID\": sqlalchemy.types.SmallInteger,\n",
    "    \"HourDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"MinuteID\": sqlalchemy.types.SmallInteger,\n",
    "    \"MinuteDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"SecondID\": sqlalchemy.types.SmallInteger,\n",
    "    \"SecondDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"MarketHoursFlag\": sqlalchemy.types.Boolean,\n",
    "    \"OfficeHoursFlag\": sqlalchemy.types.Boolean\n",
    "}\n",
    "\n",
    "dim_time_df.to_sql(name='dimtime', con=engine, if_exists='replace', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimBroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_df = pd.read_csv(\n",
    "    r\"..\\data\\sf5\\Batch1\\HR.csv\",\n",
    "    sep=\",\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"EmployeeID\", \"ManagerID\", \"EmployeeFirstName\", \"EmployeeLastName\",\n",
    "        \"EmployeeMI\", \"EmployeeJobCode\", \"EmployeeBranch\",\n",
    "        \"EmployeeOffice\", \"EmployeePhone\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"EmployeeID\": \"uint32\",\n",
    "        \"ManagerID\": \"uint32\",\n",
    "        \"EmployeeFirstName\": \"str\",\n",
    "        \"EmployeeLastName\": \"str\",\n",
    "        \"EmployeeMI\": \"str\",\n",
    "        \"EmployeeJobCode\": pd.UInt16Dtype(),\n",
    "        \"EmployeeBranch\": \"str\",\n",
    "        \"EmployeeOffice\": \"str\",\n",
    "        \"EmployeePhone\": \"str\"\n",
    "    }\n",
    ")\n",
    "hr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for SCD operations as no duplicate natural keys\n",
    "hr_df.duplicated(subset=[\"EmployeeID\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter Records\n",
    "filtered_df = hr_df[hr_df['EmployeeJobCode'] == 314]\n",
    "\n",
    "# 2. Map Columns\n",
    "DimBroker = filtered_df.rename(columns={\n",
    "    'EmployeeID': 'BrokerID',\n",
    "    'ManagerID': 'ManagerID',\n",
    "    'EmployeeFirstName': 'FirstName',\n",
    "    'EmployeeLastName': 'LastName',\n",
    "    'EmployeeMI': 'MiddleInitial',\n",
    "    'EmployeeBranch': 'Branch',\n",
    "    'EmployeeOffice': 'Office',\n",
    "    'EmployeePhone': 'Phone'\n",
    "})\n",
    "# 3. Handle Surrogate Key (SK_BrokerID)\n",
    "# Using cumcount to generate a unique ID for each row\n",
    "DimBroker['SK_BrokerID'] = range(1, len(DimBroker) + 1)\n",
    "\n",
    "# 4. Set Default Values for New Fields\n",
    "DimBroker['IsCurrent'] = True\n",
    "# EffectiveDate is set to the earliest date in the DimDate table and EndDate is set to 9999- 12-31\n",
    "DimBroker['EffectiveDate'] = pd.read_sql_query(\"SELECT MIN(datevalue) FROM dimdate\", engine).iloc[0, 0]\n",
    "DimBroker['EndDate'] = pd.Timestamp('9999-12-31')\n",
    "\n",
    "# 5 & 6. Insert New Records\n",
    "# Since DimBroker is empty, all records from filtered_df are new and can be directly inserted.\n",
    "# No need to check for existing records or expire old records.\n",
    "\n",
    "# Sort by BrokerID and another timestamp column if available (for handling multiple updates per day)\n",
    "# DimBroker.sort_values(by=['BrokerID', 'TimestampColumn'], inplace=True)\n",
    "\n",
    "# Drop duplicates, keeping only the last (which is the latest update for the day)\n",
    "# DimBroker.drop_duplicates(subset='BrokerID', keep='last', inplace=True)\n",
    "\n",
    "# Display the first few rows of the newly created DimBroker DataFrame\n",
    "DimBroker.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"SK_BrokerID\": sqlalchemy.types.BigInteger,\n",
    "    \"BrokerID\": sqlalchemy.types.BigInteger,\n",
    "    \"ManagerID\": sqlalchemy.types.BigInteger,\n",
    "    \"FirstName\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"LastName\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"MiddleInitial\": sqlalchemy.types.CHAR(length=1),\n",
    "    \"Branch\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"Office\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"Phone\": sqlalchemy.types.CHAR(length=14),\n",
    "    \"IsCurrent\": sqlalchemy.types.Boolean,\n",
    "    \"BatchID\": sqlalchemy.types.Integer,\n",
    "    \"EffectiveDate\": sqlalchemy.types.Date,\n",
    "    \"EndDate\": sqlalchemy.types.Date\n",
    "}\n",
    "\n",
    "DimBroker.to_sql(name='dimbroker', con=engine, if_exists='replace', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM dimbroker LIMIT 10\"\n",
    "result_df = pd.read_sql_query(query, engine)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "file_path = r\"..\\data\\sf5\\Batch1\\Industry.txt\"\n",
    "industry_df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"IN_ID\", \"IN_NAME\", \"IN_SC_ID\"],\n",
    "    dtype={\n",
    "        \"IN_ID\": \"str\",\n",
    "        \"IN_NAME\": \"str\",\n",
    "        \"IN_SC_ID\": \"str\"\n",
    "    }\n",
    ")\n",
    "industry_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"IN_ID\": sqlalchemy.types.CHAR(length=2),\n",
    "    \"IN_NAME\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"IN_SC_ID\": sqlalchemy.types.CHAR(length=4)\n",
    "}\n",
    "\n",
    "industry_df.to_sql(name=\"industry\", con=engine, if_exists='replace', index=False, dtype=dtypes)\n",
    "query = \"SELECT * FROM industry LIMIT 10\"\n",
    "result_df = pd.read_sql_query(query, engine)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StatusType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read StatusType data\n",
    "filepath = r\"..\\data\\sf5\\Batch1\\StatusType.txt\"\n",
    "status_type_df = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ST_ID\", \"ST_NAME\"],\n",
    "    dtype={\"ST_ID\": \"str\", \"ST_NAME\": \"str\"}\n",
    ")\n",
    "status_type_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"ST_ID\": sqlalchemy.types.CHAR(length=4),\n",
    "    \"ST_NAME\": sqlalchemy.types.CHAR(length=10)\n",
    "}\n",
    "status_type_df.to_sql(name=\"statustype\", con=engine, if_exists='replace', index=False, dtype=dtypes)\n",
    "query = \"SELECT * FROM statustype LIMIT 10\"\n",
    "result_df = pd.read_sql_query(query, engine)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TradeType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TradeType data\n",
    "filepath = r\"..\\data\\sf5\\Batch1\\TradeType.txt\"\n",
    "trade_type_df = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"TT_ID\", \"TT_NAME\", \"TT_IS_SELL\", \"TT_IS_MRKT\"],\n",
    "    dtype={\"TT_ID\": \"str\", \"TT_NAME\": \"str\", \"TT_IS_SELL\": \"uint8\", \"TT_IS_MRKT\": \"uint8\"}\n",
    ")\n",
    "\n",
    "print(trade_type_df.shape)\n",
    "trade_type_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"TT_ID\": sqlalchemy.types.CHAR(length=3),\n",
    "    \"TT_NAME\": sqlalchemy.types.CHAR(length=12),\n",
    "    \"TT_IS_SELL\": sqlalchemy.types.SmallInteger,\n",
    "    \"TT_IS_MRKT\": sqlalchemy.types.SmallInteger\n",
    "}\n",
    "trade_type_df.to_sql(name=\"tradetype\", con=engine, if_exists='replace', index=False, dtype=dtypes)\n",
    "query = \"SELECT * FROM tradetype LIMIT 10\"\n",
    "result_df = pd.read_sql_query(query, engine)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TaxRate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TaxRate data\n",
    "filepath = r\"..\\data\\sf5\\Batch1\\TaxRate.txt\"\n",
    "tax_rate_df = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"TX_ID\", \"TX_NAME\", \"TX_RATE\"],\n",
    "    dtype={\"TX_ID\": \"str\", \"TX_NAME\": \"str\", \"TX_RATE\": \"float64\"}\n",
    ")\n",
    "print(tax_rate_df.shape)\n",
    "tax_rate_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"TX_ID\": sqlalchemy.types.CHAR(length=4),\n",
    "    \"TX_NAME\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"TX_RATE\": sqlalchemy.types.Numeric(precision=6, scale=5)\n",
    "}\n",
    "\n",
    "tax_rate_df.to_sql(name=\"taxrate\", con=engine, if_exists='replace', index=False, dtype=dtypes)\n",
    "query = \"SELECT * FROM taxrate LIMIT 10\"\n",
    "result_df = pd.read_sql_query(query, engine)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimCompany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_finwire(file_path):\n",
    "    # Define the column widths and names\n",
    "    col_widths = [15, 3, 60, 10, 4, 2, 4, 8, 80, 80, 12, 25, 20, 24, 46, 150]\n",
    "    col_names = [\n",
    "        \"PTS\", \"RecType\", \"CompanyName\", \"CIK\", \"Status\", \"IndustryID\",\n",
    "        \"SPrating\", \"FoundingDate\", \"AddrLine1\", \"AddrLine2\", \"PostalCode\",\n",
    "        \"City\", \"StateProvince\", \"Country\", \"CEOname\", \"Description\"\n",
    "    ]\n",
    "    # Read the fixed-width file\n",
    "    df = pd.read_fwf(file_path, widths=col_widths, header=None, names=col_names)\n",
    "\n",
    "    # Filter the DataFrame for CMP records\n",
    "    df_cmp = df[df['RecType'] == 'CMP']\n",
    "    return df_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query StatusType table and create a mapping dictionary\n",
    "with engine.connect() as conn:\n",
    "    statustype_df = pd.read_sql(\"SELECT * FROM statustype\", conn)\n",
    "status_mapping = dict(statustype_df[['ST_ID', 'ST_NAME']].values)\n",
    "\n",
    "# Query Industry table and create a mapping dictionary\n",
    "with engine.connect() as conn:\n",
    "    industry_df = pd.read_sql(\"SELECT * FROM industry\", conn)\n",
    "industry_mapping = dict(industry_df[['IN_ID', 'IN_NAME']].values)\n",
    "\n",
    "# Valid SPrating values\n",
    "valid_spratings = ['AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-', 'BBB+', 'BBB', 'BBB-', 'BB+', 'BB', 'BB-', 'B+', 'B', 'B-', 'CCC+', 'CCC', 'CCC-', 'CC', 'C', 'D']\n",
    "\n",
    "sql_dtypes = {\n",
    "    \"SK_CompanyID\": sqlalchemy.types.BigInteger,\n",
    "    \"CompanyID\": sqlalchemy.types.BigInteger,\n",
    "    \"Status\": sqlalchemy.types.CHAR(length=10),\n",
    "    \"Name\": sqlalchemy.types.CHAR(length=60),\n",
    "    \"Industry\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"SPrating\": sqlalchemy.types.CHAR(length=4),\n",
    "    \"isLowGrade\": sqlalchemy.types.Boolean,\n",
    "    \"CEO\": sqlalchemy.types.CHAR(length=100),\n",
    "    \"AddressLine1\": sqlalchemy.types.CHAR(length=80),\n",
    "    \"AddressLine2\": sqlalchemy.types.CHAR(length=80),\n",
    "    \"PostalCode\": sqlalchemy.types.CHAR(length=12),\n",
    "    \"City\": sqlalchemy.types.CHAR(length=25),\n",
    "    \"StateProv\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"Country\": sqlalchemy.types.CHAR(length=24),\n",
    "    \"Description\": sqlalchemy.types.CHAR(length=150),\n",
    "    \"FoundingDate\": sqlalchemy.types.Date,\n",
    "    \"IsCurrent\": sqlalchemy.types.Boolean,\n",
    "    \"BatchID\": sqlalchemy.types.Integer,\n",
    "    \"EffectiveDate\": sqlalchemy.types.Date,\n",
    "    \"EndDate\": sqlalchemy.types.Date\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(DATA_DIR)\n",
    "finwire_files = [file for file in files if file.startswith(\"FINWIRE\") and 'audit' not in file]\n",
    "len(finwire_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dimcompany(filename, is_first_batch=False):\n",
    "    file_path = DATA_DIR + filename\n",
    "    df_cmp = read_finwire(file_path)\n",
    "\n",
    "    if len(df_cmp) == 0:\n",
    "        return df_cmp, None, None\n",
    "\n",
    "    # Define the column names and data types\n",
    "    column_names = [\n",
    "        \"SK_CompanyID\",\n",
    "        \"CompanyID\",\n",
    "        \"Status\",\n",
    "        \"Name\",\n",
    "        \"Industry\",\n",
    "        \"SPrating\",\n",
    "        \"isLowGrade\",\n",
    "        \"CEO\",\n",
    "        \"AddressLine1\",\n",
    "        \"AddressLine2\",\n",
    "        \"PostalCode\",\n",
    "        \"City\",\n",
    "        \"StateProv\",\n",
    "        \"Country\",\n",
    "        \"Description\",\n",
    "        \"FoundingDate\",\n",
    "        \"IsCurrent\",\n",
    "        \"BatchID\",\n",
    "        \"EffectiveDate\",\n",
    "        \"EndDate\",\n",
    "    ]\n",
    "    dtypes = {\n",
    "        \"SK_CompanyID\": \"uint32\",\n",
    "        \"CompanyID\": \"uint32\",\n",
    "        \"Status\": \"str\",\n",
    "        \"Name\": \"str\",\n",
    "        \"Industry\": \"str\",\n",
    "        \"SPrating\": \"str\",\n",
    "        \"isLowGrade\": \"boolean\",\n",
    "        \"CEO\": \"str\",\n",
    "        \"AddressLine1\": \"str\",\n",
    "        \"AddressLine2\": \"str\",\n",
    "        \"PostalCode\": \"str\",\n",
    "        \"City\": \"str\",\n",
    "        \"StateProv\": \"str\",\n",
    "        \"Country\": \"str\",\n",
    "        \"Description\": \"str\",\n",
    "        \"FoundingDate\": \"datetime64[ns]\",\n",
    "        \"IsCurrent\": \"bool\",\n",
    "        \"BatchID\": \"uint8\",\n",
    "        \"EffectiveDate\": \"datetime64[ns]\",\n",
    "        \"EndDate\": \"datetime64[ns]\",\n",
    "    }\n",
    "    # Create an empty DataFrame with the specified schema\n",
    "    dimCompany = pd.DataFrame(columns=column_names).astype(dtypes)\n",
    "\n",
    "    # Copy and map relevant columns\n",
    "    df_cmp[\"CIK\"] = pd.to_numeric(df_cmp[\"CIK\"], downcast=\"unsigned\")\n",
    "    dimCompany[\"CompanyID\"] = pd.to_numeric(df_cmp[\"CIK\"], downcast=\"unsigned\")\n",
    "    dimCompany[\"Name\"] = df_cmp[\"CompanyName\"]\n",
    "    dimCompany[\"SPrating\"] = df_cmp[\"SPrating\"]\n",
    "    dimCompany[\"CEO\"] = df_cmp[\"CEOname\"]\n",
    "    dimCompany[\"Description\"] = df_cmp[\"Description\"]\n",
    "    dimCompany[\"FoundingDate\"] = pd.to_datetime(\n",
    "        df_cmp[\"FoundingDate\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # For address fields\n",
    "    dimCompany[\"AddressLine1\"] = df_cmp[\"AddrLine1\"]\n",
    "    dimCompany[\"AddressLine2\"] = df_cmp[\"AddrLine2\"]\n",
    "    dimCompany[\"PostalCode\"] = df_cmp[\"PostalCode\"]\n",
    "    dimCompany[\"City\"] = df_cmp[\"City\"]\n",
    "    dimCompany[\"State_Prov\"] = df_cmp[\"StateProvince\"]\n",
    "    dimCompany[\"Country\"] = df_cmp[\"Country\"]\n",
    "\n",
    "    # Replace all-blank strings with None (NULL)\n",
    "    for col in [\n",
    "        \"Name\",\n",
    "        \"SPrating\",\n",
    "        \"CEO\",\n",
    "        \"Description\",\n",
    "        \"AddressLine1\",\n",
    "        \"AddressLine2\",\n",
    "        \"PostalCode\",\n",
    "        \"City\",\n",
    "        \"StateProv\",\n",
    "        \"Country\",\n",
    "    ]:\n",
    "        dimCompany[col] = dimCompany[col].replace(r\"^\\s*$\", None, regex=True)\n",
    "\n",
    "    # Update Status in dimCompany\n",
    "    dimCompany[\"Status\"] = df_cmp[\"Status\"].map(status_mapping)\n",
    "    # Update Industry in dimCompany\n",
    "    dimCompany[\"Industry\"] = df_cmp[\"IndustryID\"].map(industry_mapping)\n",
    "    # isLowGrade is set to False if SPrating begins with ‘A’ or ‘BBB’ otherwise set to True\n",
    "    dimCompany[\"isLowGrade\"] = ~df_cmp[\"SPrating\"].str.startswith((\"A\", \"BBB\"))\n",
    "\n",
    "    # Identify invalid SPratings\n",
    "    invalid_sprating_mask = ~dimCompany[\"SPrating\"].isin(valid_spratings)\n",
    "    # Filter dimCompany for invalid SPrating\n",
    "    invalid_sprating_data = dimCompany[invalid_sprating_mask]\n",
    "    if len(invalid_sprating_data) > 0:\n",
    "        message_data = (\n",
    "            \"CO_ID = \"\n",
    "            + invalid_sprating_data[\"CompanyID\"].astype(str)\n",
    "            + \", CO_SP_RATE = \"\n",
    "            + invalid_sprating_data[\"SPrating\"]\n",
    "        )\n",
    "        # Create DImessages DataFrame\n",
    "        dimessages = pd.DataFrame(\n",
    "            {\n",
    "                \"MessageDateAndTime\": [datetime.now()] * len(message_data),\n",
    "                \"BatchID\": [1] * len(message_data),\n",
    "                \"MessageSource\": [\"DimCompany\"] * len(message_data),\n",
    "                \"MessageText\": [\"Invalid SPRating\"] * len(message_data),\n",
    "                \"MessageType\": [\"Alert\"] * len(message_data),\n",
    "                \"MessageData\": message_data,\n",
    "            }\n",
    "        )\n",
    "        # Update dimCompany for invalid SPrating\n",
    "        dimCompany.loc[invalid_sprating_mask, [\"SPrating\", \"isLowGrade\"]] = pd.NA\n",
    "        # Insert DImessages into MySQL\n",
    "        dimessages.to_sql(\"dimessages\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "    dimCompany.loc[:, \"BatchID\"] = 1\n",
    "    dimCompany[\"EffectiveDate\"] = pd.to_datetime(df_cmp[\"PTS\"], format=\"%Y%m%d-%H%M%S\")\n",
    "    # Identify new and existing records based on CIK\n",
    "    if is_first_batch:\n",
    "        new_records = dimCompany\n",
    "        existing_records = pd.DataFrame(columns=column_names).astype(dtypes)\n",
    "        next_sk_id = 0\n",
    "    else:\n",
    "        existing_cik = pd.read_sql_query(\n",
    "            \"SELECT CompanyID FROM dimCompany WHERE IsCurrent = 1\", engine\n",
    "        )[\"CompanyID\"]\n",
    "        new_records = dimCompany[~df_cmp[\"CIK\"].isin(existing_cik)]\n",
    "        existing_records = dimCompany[df_cmp[\"CIK\"].isin(existing_cik)]\n",
    "        next_sk_id_query = \"SELECT MAX(SK_CompanyID) FROM dimCompany\"\n",
    "        next_sk_id = pd.read_sql_query(next_sk_id_query, engine).iloc[0, 0] or 0\n",
    "\n",
    "    new_records.loc[:, \"SK_CompanyID\"] = range(\n",
    "        next_sk_id + 1, next_sk_id + 1 + len(new_records)\n",
    "    )\n",
    "    new_records.loc[:, \"IsCurrent\"] = True\n",
    "    new_records.loc[:, \"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "    new_records.to_sql(\n",
    "        \"dimcompany\", engine, if_exists=\"append\", index=False, dtype=sql_dtypes\n",
    "    )\n",
    "    next_sk_id = new_records[\"SK_CompanyID\"].max()\n",
    "\n",
    "    # Process existing records\n",
    "    for cik in existing_records[\"CompanyID\"].unique():\n",
    "        effective_date = existing_records[existing_records[\"CompanyID\"] == cik][\n",
    "            \"EffectiveDate\"\n",
    "        ].max()\n",
    "        # Expire the current record in MySQL\n",
    "        update_query = f\"\"\"UPDATE dimcompany \n",
    "        SET IsCurrent = 0, EndDate = '{effective_date}' \n",
    "        WHERE CompanyID = '{cik}' AND IsCurrent = 1\n",
    "        \"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(update_query))\n",
    "            conn.commit()\n",
    "        # Insert updated record\n",
    "        updated_record = existing_records[\n",
    "            (existing_records[\"CompanyID\"] == cik)\n",
    "            & (existing_records[\"EffectiveDate\"] == effective_date)\n",
    "        ].copy()\n",
    "        updated_record[\"SK_CompanyID\"] = next_sk_id + 1\n",
    "        updated_record[\"IsCurrent\"] = True\n",
    "        updated_record[\"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "        updated_record.to_sql(\"dimcompany\", engine, if_exists=\"append\", index=False)\n",
    "        next_sk_id += 1\n",
    "\n",
    "    return df_cmp, new_records, existing_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(tqdm(finwire_files)):\n",
    "    load_dimcompany(file, i == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_finwire_fin(file_path):\n",
    "    # Define the column widths and names\n",
    "    col_widths = [15, 3, 4, 1, 8, 8, 17, 17, 12, 12, 12, 17, 17, 17, 13, 13, 60]\n",
    "    col_names = [\n",
    "        \"PTS\", \"RecType\", \"Year\", \"Quarter\", \"QtrStartDate\", \"PostingDate\",\n",
    "        \"Revenue\", \"Earnings\", \"EPS\", \"DilutedEPS\", \"Margin\", \"Inventory\",\n",
    "        \"Assets\", \"Liabilities\", \"ShOut\", \"DilutedShOut\", \"CoNameOrCIK\"\n",
    "    ]\n",
    "    # Read the fixed-width file\n",
    "    df_fin = pd.read_fwf(file_path, widths=col_widths, header=None, names=col_names)\n",
    "    # Filter the DataFrame for CMP records\n",
    "    df_fin = df_fin[df_fin['RecType'] == 'FIN']\n",
    "    # Convert PTS to datetime\n",
    "    df_fin['PTS'] = pd.to_datetime(df_fin['PTS'], format='%Y%m%d-%H%M%S')\n",
    "\n",
    "    return df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicate_indices_with_different_values(series):\n",
    "    duplicated_indices = series.index.duplicated(keep=False)\n",
    "    series_duplicates = series[duplicated_indices]\n",
    "\n",
    "    for index in series_duplicates.index.unique():\n",
    "        if series_duplicates.loc[index].drop_duplicates().shape[0] > 1:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_financial():\n",
    "    files = os.listdir(DATA_DIR)\n",
    "    finwire_files = [file for file in files if file.startswith(\"FINWIRE\") and 'audit' not in file]\n",
    "\n",
    "    for filename in tqdm(finwire_files):\n",
    "        file_path = DATA_DIR + filename\n",
    "        df_fin = read_finwire_fin(file_path)\n",
    "        if len(df_fin) == 0:\n",
    "            continue\n",
    "        # datatypes for the mysql table\n",
    "        sql_dtypes = {\n",
    "        \"SK_CompanyID\": sqlalchemy.types.BigInteger,\n",
    "        \"FI_YEAR\": sqlalchemy.types.Integer,\n",
    "        \"FI_QTR\": sqlalchemy.types.SmallInteger,\n",
    "        \"FI_QTR_START_DATE\": sqlalchemy.types.Date,\n",
    "        \"FI_REVENUE\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "        \"FI_NET_EARN\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "        \"FI_BASIC_EPS\": sqlalchemy.types.Numeric(precision=10, scale=2),\n",
    "        \"FI_DILUT_EPS\": sqlalchemy.types.Numeric(precision=10, scale=2),\n",
    "        \"FI_MARGIN\": sqlalchemy.types.Numeric(precision=10, scale=2),\n",
    "        \"FI_INVENTORY\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "        \"FI_ASSETS\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "        \"FI_LIABILITY\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "        \"FI_OUT_BASIC\": sqlalchemy.types.BigInteger,\n",
    "        \"FI_OUT_DILUT\": sqlalchemy.types.BigInteger\n",
    "    }\n",
    "\n",
    "        # data types for the DataFrame\n",
    "        dtypes = {\n",
    "            'SK_CompanyID': 'uint32',\n",
    "            'FI_YEAR': 'uint16',\n",
    "            'FI_QTR': 'uint8',\n",
    "            'FI_QTR_START_DATE': 'datetime64[ns]',\n",
    "            'FI_REVENUE': 'float64',\n",
    "            'FI_NET_EARN': 'float64',\n",
    "            'FI_BASIC_EPS': 'float64',\n",
    "            'FI_DILUT_EPS': 'float64',\n",
    "            'FI_MARGIN': 'float64',\n",
    "            'FI_INVENTORY': 'float64',\n",
    "            'FI_ASSETS': 'float64',\n",
    "            'FI_LIABILITY': 'float64',\n",
    "            'FI_OUT_BASIC': 'uint64',\n",
    "            'FI_OUT_DILUT': 'uint64'\n",
    "        }\n",
    "\n",
    "        # Create empty DataFrame\n",
    "        financial_df = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in dtypes.items()})\n",
    "\n",
    "        # copy directly\n",
    "        financial_df['FI_YEAR'] = pd.to_numeric(df_fin['Year'].str.strip(), downcast='unsigned')\n",
    "        financial_df['FI_QTR'] = pd.to_numeric(df_fin['Quarter'].str.strip(), downcast='unsigned')\n",
    "        financial_df['FI_QTR_START_DATE'] = pd.to_datetime(df_fin['QtrStartDate'], format='%Y%m%d')\n",
    "        financial_df['FI_REVENUE'] = pd.to_numeric(df_fin['Revenue'].str.strip(), downcast='float')\n",
    "        financial_df['FI_NET_EARN'] = pd.to_numeric(df_fin['Earnings'].str.strip(), downcast='float')\n",
    "        financial_df['FI_BASIC_EPS'] = pd.to_numeric(df_fin['EPS'].str.strip(), downcast='float')\n",
    "        financial_df['FI_DILUT_EPS'] = pd.to_numeric(df_fin['DilutedEPS'].str.strip(), downcast='float')\n",
    "        financial_df['FI_MARGIN'] = pd.to_numeric(df_fin['Margin'].str.strip(), downcast='float')\n",
    "        financial_df['FI_INVENTORY'] = pd.to_numeric(df_fin['Inventory'].str.strip(), downcast='float')\n",
    "        financial_df['FI_ASSETS'] = pd.to_numeric(df_fin['Assets'].str.strip(), downcast='float')\n",
    "        financial_df['FI_LIABILITY'] = pd.to_numeric(df_fin['Liabilities'].str.strip(), downcast='float')\n",
    "        financial_df['FI_OUT_BASIC'] = pd.to_numeric(df_fin['ShOut'].str.strip(), downcast='unsigned')\n",
    "        financial_df['FI_OUT_DILUT'] = pd.to_numeric(df_fin['DilutedShOut'].str.strip(), downcast='unsigned')\n",
    "\n",
    "        # Split df_fin based on the length of CoNameOrCIK\n",
    "        df_fin_id = df_fin[df_fin['CoNameOrCIK'].str.len() == 10][['PTS', 'CoNameOrCIK']]\n",
    "        df_fin_id['PTS'] = df_fin_id['PTS'].dt.strftime('%Y-%m-%d')\n",
    "        df_fin_id['CoNameOrCIK'] = pd.to_numeric(df_fin_id['CoNameOrCIK'], downcast='unsigned')\n",
    "        df_fin_name = df_fin[df_fin['CoNameOrCIK'].str.len() != 10][['PTS', 'CoNameOrCIK']]\n",
    "        df_fin_name['PTS'] = df_fin_name['PTS'].dt.strftime('%Y-%m-%d')\n",
    "        df_fin_name['CoNameOrCIK'] = df_fin_name['CoNameOrCIK'].str.strip()\n",
    "\n",
    "        def build_query(df, id_or_name_col):\n",
    "            '''Function to build SQL query for date range checks'''\n",
    "            query_parts = []\n",
    "            for _, row in df.iterrows():\n",
    "                pts = row['PTS']\n",
    "                if id_or_name_col == 'CompanyID':\n",
    "                    company_id = row['CoNameOrCIK']\n",
    "                    query_part = f\"(CompanyID = {company_id} AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "                else:  # Name\n",
    "                    company_name = row['CoNameOrCIK']\n",
    "                    query_part = f\"(Name = '{company_name}' AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "                query_parts.append(query_part)\n",
    "            return ' OR '.join(query_parts)\n",
    "\n",
    "        # Execute query and map results for ID-based records\n",
    "        if not df_fin_id.empty:\n",
    "            query_id = f\"SELECT CompanyID, SK_CompanyID FROM dimcompany WHERE \" + build_query(df_fin_id, 'CompanyID')\n",
    "            sk_id_map = pd.read_sql_query(query_id, engine).set_index('CompanyID')['SK_CompanyID']\n",
    "            # if has_duplicate_indices_with_different_values(sk_id_map):\n",
    "            #     print(\"WARNING: Different SK_CompanyID for Duplicate CompanyID\")\n",
    "            #     break\n",
    "            # drop duplicates from the index\n",
    "            sk_id_map = sk_id_map[~sk_id_map.index.duplicated(keep='last')]\n",
    "            \n",
    "            financial_df.loc[df_fin_id.index, 'SK_CompanyID'] = df_fin_id['CoNameOrCIK'].astype(int).map(sk_id_map)\n",
    "\n",
    "        # Execute query and map results for Name-based records\n",
    "        if not df_fin_name.empty:\n",
    "            query_name = f\"SELECT Name, SK_CompanyID FROM dimcompany WHERE \" + build_query(df_fin_name, 'Name')\n",
    "            sk_name_map = pd.read_sql_query(query_name, engine).set_index('Name')['SK_CompanyID']\n",
    "            # if has_duplicate_indices_with_different_values(sk_name_map):\n",
    "            #     print(\"WARNING: Different SK_CompanyID for Duplicate Company Name\")\n",
    "            #     break\n",
    "            # drop duplicates from the index\n",
    "            sk_name_map = sk_name_map[~sk_name_map.index.duplicated(keep='last')]\n",
    "            financial_df.loc[df_fin_name.index, 'SK_CompanyID'] = df_fin_name['CoNameOrCIK'].map(sk_name_map)\n",
    "\n",
    "        financial_df['SK_CompanyID'] = financial_df['SK_CompanyID'].astype('uint32')\n",
    "\n",
    "        # perform migration\n",
    "        financial_df.to_sql('financial', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_financial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimSecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_finwire_sec(file_path):\n",
    "    # Define the column widths and names\n",
    "    col_widths = [15, 3, 15, 6, 4, 70, 6, 13, 8, 8, 12, 60]\n",
    "    col_names = [\n",
    "        \"PTS\", \"RecType\", \"Symbol\", \"IssueType\", \"Status\", \"Name\", \"ExID\",\n",
    "        \"ShOut\", \"FirstTradeDate\", \"FirstTradeExchg\", \"Dividend\", \"CoNameOrCIK\"\n",
    "    ]\n",
    "    # Read the fixed-width file\n",
    "    df_sec = pd.read_fwf(file_path, widths=col_widths, header=None, names=col_names)\n",
    "    # Filter the DataFrame for CMP records\n",
    "    df_sec = df_sec[df_sec['RecType'] == 'SEC']\n",
    "    # Convert date cols to datetime\n",
    "    df_sec['PTS'] = pd.to_datetime(df_sec['PTS'], format='%Y%m%d-%H%M%S')\n",
    "    df_sec['FirstTradeDate'] = pd.to_datetime(df_sec['FirstTradeDate'], format='%Y%m%d')\n",
    "    df_sec['FirstTradeExchg'] = pd.to_datetime(df_sec['FirstTradeExchg'], format='%Y%m%d')\n",
    "\n",
    "    return df_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"Symbol\": sqlalchemy.types.String(15),\n",
    "    \"Issue\": sqlalchemy.types.String(6),\n",
    "    \"Status\": sqlalchemy.types.String(10),\n",
    "    \"Name\": sqlalchemy.types.String(70),\n",
    "    \"ExchangeID\": sqlalchemy.types.String(6),\n",
    "    \"SK_CompanyID\": sqlalchemy.types.Integer,\n",
    "    \"SharesOutstanding\": sqlalchemy.types.Integer,\n",
    "    \"FirstTrade\": sqlalchemy.types.Date,\n",
    "    \"FirstTradeOnExchange\": sqlalchemy.types.Date,\n",
    "    \"Dividend\": sqlalchemy.types.Numeric(10, 2),\n",
    "    \"IsCurrent\": sqlalchemy.types.Boolean,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger,\n",
    "    \"EffectiveDate\": sqlalchemy.types.Date,\n",
    "    \"EndDate\": sqlalchemy.types.Date,\n",
    "}\n",
    "\n",
    "dtypes = {\n",
    "    \"SK_SecurityID\": \"uint32\",\n",
    "    \"Symbol\": \"str\",\n",
    "    \"Issue\": \"str\",\n",
    "    \"Status\": \"str\",\n",
    "    \"Name\": \"str\",\n",
    "    \"ExchangeID\": \"str\",\n",
    "    \"SK_CompanyID\": \"uint32\",\n",
    "    \"SharesOutstanding\": \"uint32\",\n",
    "    \"FirstTrade\": \"datetime64[ns]\",\n",
    "    \"FirstTradeOnExchange\": \"datetime64[ns]\",\n",
    "    \"Dividend\": \"float64\",\n",
    "    \"IsCurrent\": \"bool\",\n",
    "    \"BatchID\": \"uint8\",\n",
    "    \"EffectiveDate\": \"datetime64[ns]\",\n",
    "    \"EndDate\": \"datetime64[ns]\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query StatusType table and create a mapping dictionary\n",
    "with engine.connect() as conn:\n",
    "    statustype_df = pd.read_sql(\"SELECT * FROM statustype\", conn)\n",
    "status_mapping = dict(statustype_df[['ST_ID', 'ST_NAME']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query(df, id_or_name_col):\n",
    "    '''Function to build SQL query for date range checks'''\n",
    "    query_parts = []\n",
    "    for _, row in df.iterrows():\n",
    "        pts = row['PTS']\n",
    "        if id_or_name_col == 'CompanyID':\n",
    "            company_id = row['CoNameOrCIK']\n",
    "            query_part = f\"(CompanyID = {company_id} AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "        else:  # Name\n",
    "            company_name = row['CoNameOrCIK']\n",
    "            query_part = f\"(Name = '{company_name}' AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "        query_parts.append(query_part)\n",
    "    return ' OR '.join(query_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dimsecurity():\n",
    "    finwire_files = os.listdir(DATA_DIR)\n",
    "    finwire_files = [\n",
    "        DATA_DIR + file\n",
    "        for file in finwire_files\n",
    "        if file.startswith(\"FINWIRE\") and \"audit\" not in file\n",
    "    ]\n",
    "    for i, file in enumerate(tqdm(finwire_files)):\n",
    "        # raw data from file\n",
    "        df_sec = read_finwire_sec(file)\n",
    "        # dimension table in data warehouse\n",
    "        security_df = pd.DataFrame(\n",
    "            {col: pd.Series(dtype=typ) for col, typ in dtypes.items()}\n",
    "        )\n",
    "        # copy directly\n",
    "        security_df[\"Symbol\"] = df_sec[\"Symbol\"]\n",
    "        security_df[\"Issue\"] = df_sec[\"IssueType\"]\n",
    "        security_df[\"Name\"] = df_sec[\"Name\"]\n",
    "        security_df[\"ExchangeID\"] = df_sec[\"ExID\"]\n",
    "        security_df[\"SharesOutstanding\"] = pd.to_numeric(\n",
    "            df_sec[\"ShOut\"], downcast=\"unsigned\"\n",
    "        )\n",
    "        security_df[\"FirstTrade\"] = df_sec[\"FirstTradeDate\"]\n",
    "        security_df[\"FirstTradeOnExchange\"] = df_sec[\"FirstTradeExchg\"]\n",
    "        security_df[\"Dividend\"] = pd.to_numeric(df_sec[\"Dividend\"], downcast=\"float\")\n",
    "        # Update Status in security_df\n",
    "        security_df[\"Status\"] = df_sec[\"Status\"].map(status_mapping)\n",
    "        # BatchID is set to 1\n",
    "        security_df[\"BatchID\"] = 1\n",
    "        # Split df_sec based on the length of CoNameOrCIK\n",
    "        df_sec_id = df_sec[df_sec[\"CoNameOrCIK\"].str.len() == 10][\n",
    "            [\"PTS\", \"CoNameOrCIK\"]\n",
    "        ]\n",
    "        df_sec_id[\"PTS\"] = df_sec_id[\"PTS\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df_sec_id[\"CoNameOrCIK\"] = pd.to_numeric(\n",
    "            df_sec_id[\"CoNameOrCIK\"], downcast=\"unsigned\"\n",
    "        )\n",
    "        df_sec_name = df_sec[df_sec[\"CoNameOrCIK\"].str.len() != 10][\n",
    "            [\"PTS\", \"CoNameOrCIK\"]\n",
    "        ]\n",
    "        df_sec_name[\"PTS\"] = df_sec_name[\"PTS\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df_sec_name[\"CoNameOrCIK\"] = df_sec_name[\"CoNameOrCIK\"].str.strip()\n",
    "        # Map results for ID-based records\n",
    "        if not df_sec_id.empty:\n",
    "            query_id = (\n",
    "                f\"SELECT CompanyID, SK_CompanyID FROM dimcompany WHERE \"\n",
    "                + build_query(df_sec_id, \"CompanyID\")\n",
    "            )\n",
    "            sk_id_map = pd.read_sql_query(query_id, engine).set_index(\"CompanyID\")[\n",
    "                \"SK_CompanyID\"\n",
    "            ]\n",
    "            # drop duplicates from the index\n",
    "            sk_id_map = sk_id_map[~sk_id_map.index.duplicated(keep=\"last\")]\n",
    "            security_df.loc[df_sec_id.index, \"SK_CompanyID\"] = (\n",
    "                df_sec_id[\"CoNameOrCIK\"].astype(int).map(sk_id_map)\n",
    "            )\n",
    "        # Map results for Name-based records\n",
    "        if not df_sec_name.empty:\n",
    "            query_name = (\n",
    "                f\"SELECT Name, SK_CompanyID FROM dimcompany WHERE \"\n",
    "                + build_query(df_sec_name, \"Name\")\n",
    "            )\n",
    "            sk_name_map = pd.read_sql_query(query_name, engine).set_index(\"Name\")[\n",
    "                \"SK_CompanyID\"\n",
    "            ]\n",
    "            # drop duplicates from the index\n",
    "            sk_name_map = sk_name_map[~sk_name_map.index.duplicated(keep=\"last\")]\n",
    "            security_df.loc[df_sec_name.index, \"SK_CompanyID\"] = df_sec_name[\n",
    "                \"CoNameOrCIK\"\n",
    "            ].map(sk_name_map)\n",
    "        # change the type back to uint32\n",
    "        security_df[\"SK_CompanyID\"] = security_df[\"SK_CompanyID\"].astype(\"uint32\")\n",
    "        # get effective date from posting date\n",
    "        security_df[\"EffectiveDate\"] = df_sec[\"PTS\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        # Identify new and existing records based on Symbol\n",
    "        is_first_batch = i == 0\n",
    "        if is_first_batch:\n",
    "            new_records = security_df\n",
    "            existing_records = pd.DataFrame(\n",
    "                {col: pd.Series(dtype=typ) for col, typ in dtypes.items()}\n",
    "            )\n",
    "            next_sk_id = 0\n",
    "        else:\n",
    "            existing_symbol = pd.read_sql_query(\n",
    "                \"SELECT Symbol FROM dimsecurity WHERE IsCurrent = 1\", engine\n",
    "            )[\"Symbol\"]\n",
    "            new_records = security_df[~security_df[\"Symbol\"].isin(existing_symbol)]\n",
    "            existing_records = security_df[security_df[\"Symbol\"].isin(existing_symbol)]\n",
    "            next_sk_id_query = \"SELECT MAX(SK_SecurityID) FROM dimsecurity\"\n",
    "            next_sk_id = pd.read_sql_query(next_sk_id_query, engine).iloc[0, 0] or 0\n",
    "        # update SK_SecurityID, IsCurrent, EndDate\n",
    "        new_records.loc[:, \"SK_SecurityID\"] = range(\n",
    "            next_sk_id + 1, next_sk_id + 1 + len(new_records)\n",
    "        )\n",
    "        new_records.loc[:, \"IsCurrent\"] = True\n",
    "        new_records.loc[:, \"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "        # Insert records with new SK_CompanyID\n",
    "        new_records.to_sql(\n",
    "            \"dimsecurity\", engine, if_exists=\"append\", index=False, dtype=sql_dtypes\n",
    "        )\n",
    "        next_sk_id_query = \"SELECT MAX(SK_SecurityID) FROM dimsecurity\"\n",
    "        next_sk_id = pd.read_sql_query(next_sk_id_query, engine).iloc[0, 0] or 0\n",
    "        # Process existing records\n",
    "        for index, row in existing_records.iterrows():\n",
    "            effective_date = row[\"EffectiveDate\"]\n",
    "            symbol = row[\"Symbol\"]\n",
    "            # Expire the current record in MySQL\n",
    "            update_query = f\"\"\"UPDATE dimsecurity \n",
    "            SET IsCurrent = 0, EndDate = '{effective_date}' \n",
    "            WHERE Symbol = '{symbol}' AND IsCurrent = 1\n",
    "            \"\"\"\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(update_query))\n",
    "                conn.commit()\n",
    "            row[\"SK_SecurityID\"] = next_sk_id + 1\n",
    "            row[\"SK_SecurityID\"] = int(row[\"SK_SecurityID\"])\n",
    "            row[\"IsCurrent\"] = True\n",
    "            row[\"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "            row_df = pd.DataFrame(row).T\n",
    "            # print(row_df)\n",
    "            # insert records with existing SK_CompanyID\n",
    "            row_df.to_sql(\n",
    "                \"dimsecurity\", engine, if_exists=\"append\", index=False, dtype=sql_dtypes\n",
    "            )\n",
    "            next_sk_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dimsecurity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prospect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prospect_file(filepath):\n",
    "    # Define the column names and their data types\n",
    "    columns = [\n",
    "        'AgencyID', 'LastName', 'FirstName', 'MiddleInitial', 'Gender', \n",
    "        'AddressLine1', 'AddressLine2', 'PostalCode', 'City', 'State', \n",
    "        'Country', 'Phone', 'Income', 'NumberCars', 'NumberChildren', \n",
    "        'MaritalStatus', 'Age', 'CreditRating', 'OwnOrRentFlag', \n",
    "        'Employer', 'NumberCreditCards', 'NetWorth'\n",
    "    ]\n",
    "\n",
    "    # Define the data types for reading the file\n",
    "    dtypes = {\n",
    "        'AgencyID': 'str', 'LastName': 'str', 'FirstName': 'str', \n",
    "        'MiddleInitial': 'str', 'Gender': 'str', 'AddressLine1': 'str', \n",
    "        'AddressLine2': 'str', 'PostalCode': 'str', 'City': 'str', \n",
    "        'State': 'str', 'Country': 'str', 'Phone': 'str', \n",
    "        'Income': 'Int64', 'NumberCars': 'Int8', 'NumberChildren': 'Int8', \n",
    "        'MaritalStatus': 'str', 'Age': 'Int8', 'CreditRating': 'Int16', \n",
    "        'OwnOrRentFlag': 'str', 'Employer': 'str', \n",
    "        'NumberCreditCards': 'Int8', 'NetWorth': 'Int64'\n",
    "    }\n",
    "\n",
    "    # Read the CSV file\n",
    "    raw_prospect_df = pd.read_csv(\n",
    "        filepath, \n",
    "        header=None, \n",
    "        names=columns, \n",
    "        dtype=dtypes\n",
    "    )\n",
    "\n",
    "    return raw_prospect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prospect_df = read_prospect_file(DATA_DIR + \"Prospect.csv\")\n",
    "print(raw_prospect_df.info())\n",
    "raw_prospect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'AgencyID': 'str',\n",
    "    'SK_RecordDateID': 'uint32',\n",
    "    'SK_UpdateDateID': 'uint32',\n",
    "    'BatchID': 'uint16',\n",
    "    'IsCustomer': 'boolean',\n",
    "    'LastName': 'str',\n",
    "    'FirstName': 'str',\n",
    "    'MiddleInitial': 'str',\n",
    "    'Gender': 'str',\n",
    "    'AddressLine1': 'str',\n",
    "    'AddressLine2': 'str',\n",
    "    'PostalCode': 'str',\n",
    "    'City': 'str',\n",
    "    'State': 'str',\n",
    "    'Country': 'str',\n",
    "    'Phone': 'str',\n",
    "    'Income': 'uint32',\n",
    "    'NumberCars': 'uint8',\n",
    "    'NumberChildren': 'uint8',\n",
    "    'MaritalStatus': 'str',\n",
    "    'Age': 'uint8',\n",
    "    'CreditRating': 'uint16',\n",
    "    'OwnOrRentFlag': 'str',\n",
    "    'Employer': 'str',\n",
    "    'NumberCreditCards': 'uint8',\n",
    "    'NetWorth': 'int64',\n",
    "    'MarketingNameplate': 'str'\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame with the specified schema\n",
    "prospect_df = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in dtypes.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df['AgencyID'] = raw_prospect_df['AgencyID']\n",
    "prospect_df['LastName'] = raw_prospect_df['LastName']\n",
    "prospect_df['FirstName'] = raw_prospect_df['FirstName']\n",
    "prospect_df['MiddleInitial'] = raw_prospect_df['MiddleInitial']\n",
    "prospect_df['Gender'] = raw_prospect_df['Gender']\n",
    "prospect_df['AddressLine1'] = raw_prospect_df['AddressLine1']\n",
    "prospect_df['AddressLine2'] = raw_prospect_df['AddressLine2']\n",
    "prospect_df['PostalCode'] = raw_prospect_df['PostalCode']\n",
    "prospect_df['City'] = raw_prospect_df['City']\n",
    "prospect_df['State'] = raw_prospect_df['State']\n",
    "prospect_df['Country'] = raw_prospect_df['Country']\n",
    "prospect_df['Phone'] = raw_prospect_df['Phone']\n",
    "prospect_df['Income'] = raw_prospect_df['Income']\n",
    "prospect_df['NumberCars'] = raw_prospect_df['NumberCars']\n",
    "prospect_df['NumberChildren'] = raw_prospect_df['NumberChildren']\n",
    "prospect_df['MaritalStatus'] = raw_prospect_df['MaritalStatus']\n",
    "prospect_df['Age'] = raw_prospect_df['Age']\n",
    "prospect_df['CreditRating'] = raw_prospect_df['CreditRating']\n",
    "prospect_df['OwnOrRentFlag'] = raw_prospect_df['OwnOrRentFlag']\n",
    "prospect_df['Employer'] = raw_prospect_df['Employer']\n",
    "prospect_df['NumberCreditCards'] = raw_prospect_df['NumberCreditCards']\n",
    "prospect_df['NetWorth'] = raw_prospect_df['NetWorth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + \"BatchDate.txt\", \"r\") as f:\n",
    "    batch_date = f.read().strip()\n",
    "sk_dateid = pd.read_sql_query(f\"select SK_DateID from dimdate where DateValue = '{batch_date}'\", engine).iloc[0, 0]\n",
    "# SK_RecordDateID is set to the DimDate SK_DateID field that corresponds to the Batch Date.\n",
    "prospect_df['SK_RecordDateID'] = sk_dateid\n",
    "# SK_UpdateDateID is set to the DimDate SK_DateID field that corresponds to the Batch Date\n",
    "prospect_df['SK_UpdateDateID'] = sk_dateid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions for each tag with null checks\n",
    "conditions = {\n",
    "    \"HighValue\": (prospect_df[\"NetWorth\"].notnull() & prospect_df[\"Income\"].notnull())\n",
    "    & ((prospect_df[\"NetWorth\"] > 1_000_000) | (prospect_df[\"Income\"] > 200_000)),\n",
    "    \"Expenses\": (\n",
    "        prospect_df[\"NumberChildren\"].notnull()\n",
    "        & prospect_df[\"NumberCreditCards\"].notnull()\n",
    "    )\n",
    "    & ((prospect_df[\"NumberChildren\"] > 3) | (prospect_df[\"NumberCreditCards\"] > 5)),\n",
    "    \"Boomer\": prospect_df[\"Age\"].notnull() & (prospect_df[\"Age\"] > 45),\n",
    "    \"MoneyAlert\": (\n",
    "        prospect_df[\"Income\"].notnull()\n",
    "        & prospect_df[\"CreditRating\"].notnull()\n",
    "        & prospect_df[\"NetWorth\"].notnull()\n",
    "    )\n",
    "    & (\n",
    "        (prospect_df[\"Income\"] < 50_000)\n",
    "        | (prospect_df[\"CreditRating\"] < 600)\n",
    "        | (prospect_df[\"NetWorth\"] < 100_000)\n",
    "    ),\n",
    "    \"Spender\": (\n",
    "        prospect_df[\"NumberCars\"].notnull() & prospect_df[\"NumberCreditCards\"].notnull()\n",
    "    )\n",
    "    & ((prospect_df[\"NumberCars\"] > 3) | (prospect_df[\"NumberCreditCards\"] > 7)),\n",
    "    \"Inherited\": (prospect_df[\"Age\"].notnull() & prospect_df[\"NetWorth\"].notnull())\n",
    "    & ((prospect_df[\"Age\"] < 25) & (prospect_df[\"NetWorth\"] > 1_000_000)),\n",
    "}\n",
    "\n",
    "# Apply conditions to assign tags\n",
    "prospect_df[\"MarketingNameplate\"] = \"\"\n",
    "for tag, condition in conditions.items():\n",
    "    prospect_df[\"MarketingNameplate\"] += np.where(condition, tag + \"+\", \"\")\n",
    "\n",
    "# Remove trailing '+' and replace empty strings with None\n",
    "prospect_df[\"MarketingNameplate\"] = (\n",
    "    prospect_df[\"MarketingNameplate\"].str.rstrip(\"+\").replace(\"\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IsCurrent and BatchID are set after processing dimCustomer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimCustomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_DIR + \"CustomerMgmt.xml\"\n",
    "tree = etree.parse(data_file)\n",
    "namespace = {'tpcdi': 'http://www.tpc.org/tpc-di'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'SK_CustomerID': 'int32',\n",
    "    'CustomerID': 'int32',\n",
    "    'TaxID': 'str',\n",
    "    'Status': 'str',\n",
    "    'LastName': 'str',\n",
    "    'FirstName': 'str',\n",
    "    'MiddleInitial': 'str',\n",
    "    'Gender': 'str',\n",
    "    'Tier': 'UInt8',\n",
    "    'DOB': 'datetime64[ns]',\n",
    "    'AddressLine1': 'str',\n",
    "    'AddressLine2': 'str',\n",
    "    'PostalCode': 'str',\n",
    "    'City': 'str',\n",
    "    'StateProv': 'str',\n",
    "    'Country': 'str',\n",
    "    'Phone1': 'str',\n",
    "    'Phone2': 'str',\n",
    "    'Phone3': 'str',\n",
    "    'Email1': 'str',\n",
    "    'Email2': 'str',\n",
    "    'NationalTaxRateDesc': 'str',\n",
    "    'NationalTaxRate': 'Float64',\n",
    "    'LocalTaxRateDesc': 'str',\n",
    "    'LocalTaxRate': 'Float64',\n",
    "    'AgencyID': 'str',\n",
    "    'CreditRating': 'UInt16',\n",
    "    'NetWorth': 'Float64',\n",
    "    'MarketingNameplate': 'str',\n",
    "    'IsCurrent': 'boolean',\n",
    "    'BatchID': 'uint8',\n",
    "    'EffectiveDate': 'datetime64[ns]',\n",
    "    'EndDate': 'datetime64[ns]'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_phone_number(phone_element):\n",
    "    # Extract components of the phone number\n",
    "    ctry_code = phone_element.findtext('C_CTRY_CODE', default=None, namespaces=namespace)\n",
    "    area_code = phone_element.findtext('C_AREA_CODE', default=None, namespaces=namespace)\n",
    "    local = phone_element.findtext('C_LOCAL', default=None, namespaces=namespace)\n",
    "    ext = phone_element.findtext('C_EXT', default=None, namespaces=namespace)\n",
    "\n",
    "    # Apply transformation rules\n",
    "    if ctry_code and area_code and local:\n",
    "        phone = f\"+{ctry_code} ({area_code}) {local}\"\n",
    "    elif area_code and local:\n",
    "        phone = f\"({area_code}) {local}\"\n",
    "    elif local:\n",
    "        phone = local\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Add extension if present\n",
    "    if ext:\n",
    "        phone += ext\n",
    "\n",
    "    return phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tax_info(tax_ids):\n",
    "    tax_ids_str = \"','\".join(tax_ids)\n",
    "    query = f\"SELECT TX_ID, TX_NAME, TX_RATE FROM taxrate WHERE TX_ID IN ('{tax_ids_str}')\"\n",
    "    result = pd.read_sql_query(query, engine)\n",
    "    return result.set_index('TX_ID').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + \"BatchDate.txt\", \"r\") as f:\n",
    "    batch_date = f.read().strip()\n",
    "batch_date = pd.to_datetime(batch_date, format=\"%Y-%m-%d\")\n",
    "batch_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the latest index of 'UPDCUST' or 'INACT' for each CustomerID\n",
    "latest_updates = {}\n",
    "\n",
    "# Get all actions\n",
    "all_actions = tree.xpath(\".//tpcdi:Action\", namespaces=namespace)\n",
    "# NEW actions\n",
    "new_actions = [action for action in all_actions if action.get('ActionType') == 'NEW']\n",
    "# UPD actions\n",
    "upd_actions = [action for action in all_actions if action.get('ActionType') == 'UPDCUST']\n",
    "# INACT actions\n",
    "inact_actions = [action for action in all_actions if action.get('ActionType') == 'INACT']\n",
    "\n",
    "# Preprocess to fill the dictionary\n",
    "for i, action in enumerate(all_actions):\n",
    "    if action.get('ActionType') in ['UPDCUST', 'INACT']:\n",
    "        customer = action.find('Customer', namespaces=namespace)\n",
    "        customer_id = customer.get('C_ID', None)\n",
    "        if customer_id:\n",
    "            latest_updates[int(customer_id)] = i\n",
    "\n",
    "# Modified has_later_update function\n",
    "def has_later_update(customer_id, current_index):\n",
    "    \"\"\"Check for subsequent 'UPDCUST' or 'INACT' actions for a given CustomerID at current_index\"\"\"\n",
    "    return latest_updates.get(customer_id, -1) > current_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified schema\n",
    "dimCustomer_df = pd.DataFrame(\n",
    "    {col: pd.Series(dtype=typ) for col, typ in dtypes.items()}\n",
    ")\n",
    "\n",
    "# Initialize lists to store tax IDs for each record\n",
    "national_tax_ids = []\n",
    "local_tax_ids = []\n",
    "\n",
    "# temporary prospect_df for matching\n",
    "prospect_df_temp = prospect_df[\n",
    "    [\n",
    "        \"AgencyID\",\n",
    "        \"CreditRating\",\n",
    "        \"NetWorth\",\n",
    "        \"MarketingNameplate\",\n",
    "        \"LastName\",\n",
    "        \"FirstName\",\n",
    "        \"AddressLine1\",\n",
    "        \"AddressLine2\",\n",
    "        \"PostalCode\",\n",
    "    ]\n",
    "].copy()\n",
    "prospect_df_temp[\"LastName\"] = prospect_df_temp[\"LastName\"].str.upper()\n",
    "prospect_df_temp[\"FirstName\"] = prospect_df_temp[\"FirstName\"].str.upper()\n",
    "prospect_df_temp[\"AddressLine1\"] = prospect_df_temp[\"AddressLine1\"].str.upper()\n",
    "prospect_df_temp[\"AddressLine2\"] = prospect_df_temp[\"AddressLine2\"].str.upper()\n",
    "prospect_df_temp[\"PostalCode\"] = prospect_df_temp[\"PostalCode\"].str.upper()\n",
    "\n",
    "\n",
    "# Initialize lists to store data for NEW actions\n",
    "data = {\n",
    "    \"CustomerID\": [],\n",
    "    \"TaxID\": [],\n",
    "    \"LastName\": [],\n",
    "    \"FirstName\": [],\n",
    "    \"MiddleInitial\": [],\n",
    "    \"Tier\": [],\n",
    "    \"DOB\": [],\n",
    "    \"Gender\": [],\n",
    "    \"Email1\": [],\n",
    "    \"Email2\": [],\n",
    "    \"AddressLine1\": [],\n",
    "    \"AddressLine2\": [],\n",
    "    \"PostalCode\": [],\n",
    "    \"City\": [],\n",
    "    \"StateProv\": [],\n",
    "    \"Country\": [],\n",
    "    \"Phone1\": [],\n",
    "    \"Phone2\": [],\n",
    "    \"Phone3\": [],\n",
    "    \"NationalTaxRateDesc\": [],\n",
    "    \"NationalTaxRate\": [],\n",
    "    \"LocalTaxRateDesc\": [],\n",
    "    \"LocalTaxRate\": [],\n",
    "    \"AgencyID\": [],\n",
    "    \"CreditRating\": [],\n",
    "    \"NetWorth\": [],\n",
    "    \"MarketingNameplate\": [],\n",
    "    \"EffectiveDate\": [],\n",
    "}\n",
    "\n",
    "# Iterate through each 'Action' element with ActionType=\"NEW\"\n",
    "for index, action in enumerate(tqdm(new_actions)):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    name = customer.find(\"Name\", namespaces=namespace)\n",
    "    contact_info = customer.find(\"ContactInfo\", namespaces=namespace)\n",
    "    address = customer.find(\"Address\", namespaces=namespace)\n",
    "    tax_info = customer.find(\"TaxInfo\", namespaces=namespace)\n",
    "\n",
    "    customer_id = customer.get(\"C_ID\", None)\n",
    "    customer_id = int(customer_id) if customer_id else None\n",
    "    data[\"CustomerID\"].append(customer_id)\n",
    "    data[\"TaxID\"].append(customer.get(\"C_TAX_ID\", None))\n",
    "    tier = customer.get(\"C_TIER\", None)\n",
    "    tier = int(tier) if tier else None\n",
    "    if tier is not None and tier not in (1,2,3):\n",
    "        \"\"\"\n",
    "        A record will be inserted in the DImessages table if a customer's Tier is not one of the valid\n",
    "        values (1,2,3). The MessageSource is “DimCustomer”, the MessageType is “Alert” and the\n",
    "        MessageText is “Invalid customer tier”. The MessageData field is “C_ID = ” followed by the\n",
    "        natural key value of the record, then “, C_TIER = ” and the C_TIER value.\n",
    "        \"\"\"\n",
    "        MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "        batch_id = 1\n",
    "        sk_customer_id = len(data[\"CustomerID\"])\n",
    "        message = f\"C_ID = {sk_customer_id}, C_TIER = {tier}\"\n",
    "        message_source = \"DimCustomer\"\n",
    "        message_type = \"Alert\"\n",
    "        message_text = \"Invalid customer tier\"\n",
    "        query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "        VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(query))\n",
    "            conn.commit()\n",
    "\n",
    "    data[\"Tier\"].append(tier)\n",
    "    dob = customer.get(\"C_DOB\", None)\n",
    "    dob = pd.to_datetime(dob, format=\"%Y-%m-%d\") if dob else None\n",
    "    \"\"\"A record will be reported in the DImessages table if a customer's DOB is invalid. A customer's\n",
    "    DOB is invalid if DOB < Batch Date - 100 years or DOB > Batch Date (customer is over 100\n",
    "    years old or born in the future). The MessageSource is “DimCustomer”, the MessageType is\n",
    "    “Alert” and the MessageText is “DOB out of range”. The MessageData field is “C_ID = ”\n",
    "    followed by the natural key value of the record, then “, C_DOB = ” and the C_DOB value.\"\"\"\n",
    "    if dob and (dob < batch_date - pd.Timedelta(days=100*365) or dob > batch_date):\n",
    "        MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "        batch_id = 1\n",
    "        sk_customer_id = len(data[\"CustomerID\"])\n",
    "        message = f\"C_ID = {sk_customer_id}, C_DOB = {dob}\"\n",
    "        message_source = \"DimCustomer\"\n",
    "        message_type = \"Alert\"\n",
    "        message_text = \"DOB out of range\"\n",
    "        query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "        VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(query))\n",
    "            conn.commit()\n",
    "    data[\"DOB\"].append(dob)\n",
    "    gender = customer.get(\"C_GNDR\", \"U\")\n",
    "    gender = \"U\" if gender not in (\"M\", \"F\") else gender\n",
    "    data[\"Gender\"].append(gender)\n",
    "    \n",
    "    first_name = name.findtext(\"C_F_NAME\", default=None, namespaces=namespace)\n",
    "    data[\"FirstName\"].append(first_name if first_name else None)\n",
    "    middle_initial = name.findtext(\"C_M_NAME\", default=None, namespaces=namespace)\n",
    "    data[\"MiddleInitial\"].append(middle_initial if middle_initial else None)\n",
    "    last_name = name.findtext(\"C_L_NAME\", default=None, namespaces=namespace)\n",
    "    data[\"LastName\"].append(last_name if last_name else None)\n",
    "\n",
    "    \n",
    "    prim_email = contact_info.findtext(\n",
    "        \"C_PRIM_EMAIL\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"Email1\"].append(prim_email if prim_email else None)\n",
    "    alt_email = contact_info.findtext(\n",
    "        \"C_ALT_EMAIL\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"Email2\"].append(alt_email if alt_email else None)\n",
    "    data[\"Phone1\"].append(\n",
    "        format_phone_number(contact_info.find(\"C_PHONE_1\", namespaces=namespace))\n",
    "    )\n",
    "    data[\"Phone2\"].append(\n",
    "        format_phone_number(contact_info.find(\"C_PHONE_2\", namespaces=namespace))\n",
    "    )\n",
    "    data[\"Phone3\"].append(\n",
    "        format_phone_number(contact_info.find(\"C_PHONE_3\", namespaces=namespace))\n",
    "    )\n",
    "\n",
    "    # Extracting address information\n",
    "    address_line1 = address.findtext(\n",
    "        \"C_ADLINE1\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"AddressLine1\"].append(address_line1 if address_line1 else None)\n",
    "    address_line2 = address.findtext(\n",
    "        \"C_ADLINE2\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"AddressLine2\"].append(address_line2 if address_line2 else None)\n",
    "    postalcode = address.findtext(\"C_ZIPCODE\", default=None, namespaces=namespace)\n",
    "    data[\"PostalCode\"].append(postalcode if postalcode else None)\n",
    "    city = address.findtext(\"C_CITY\", default=None, namespaces=namespace)\n",
    "    data[\"City\"].append(city if city else None)\n",
    "    state_prov = address.findtext(\n",
    "        \"C_STATE_PROV\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"StateProv\"].append(state_prov if state_prov else None)\n",
    "    country = address.findtext(\"C_CTRY\", default=None, namespaces=namespace)\n",
    "    data[\"Country\"].append(country if country else None)\n",
    "    \n",
    "    # Store TX_ID as placeholders\n",
    "    national_tax_id = tax_info.findtext(\n",
    "        \"C_NAT_TX_ID\", default=None, namespaces=namespace\n",
    "    )\n",
    "    national_tax_id = national_tax_id if national_tax_id else None\n",
    "    national_tax_ids.append(national_tax_id)\n",
    "    local_tax_id = tax_info.findtext(\n",
    "        \"C_LCL_TX_ID\", default=None, namespaces=namespace\n",
    "    )\n",
    "    local_tax_id = local_tax_id if local_tax_id else None\n",
    "    local_tax_ids.append(local_tax_id)\n",
    "\n",
    "    if not has_later_update(customer_id, index):\n",
    "        # Find matching prospect record\n",
    "        match = prospect_df[\n",
    "            (prospect_df_temp[\"LastName\"] == last_name.upper())\n",
    "            & (prospect_df_temp[\"FirstName\"] == first_name.upper())\n",
    "            & (prospect_df_temp[\"AddressLine1\"] == address_line1.upper())\n",
    "            & (prospect_df_temp[\"AddressLine2\"] == address_line2.upper())\n",
    "            & (prospect_df_temp[\"PostalCode\"] == postalcode.upper())\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            # Set values from the matching prospect record\n",
    "            data[\"AgencyID\"].append(match[\"AgencyID\"].iloc[0])\n",
    "            data[\"CreditRating\"].append(match[\"CreditRating\"].iloc[0])\n",
    "            data[\"NetWorth\"].append(match[\"NetWorth\"].iloc[0])\n",
    "            data[\"MarketingNameplate\"].append(match[\"MarketingNameplate\"].iloc[0])\n",
    "        else:\n",
    "            # Set values to NULL\n",
    "            data[\"AgencyID\"].append(None)\n",
    "            data[\"CreditRating\"].append(None)\n",
    "            data[\"NetWorth\"].append(None)\n",
    "            data[\"MarketingNameplate\"].append(None)\n",
    "    else:\n",
    "        # Set values to NULL due to later 'UPDCUST' or 'INACT'\n",
    "        data[\"AgencyID\"].append(None)\n",
    "        data[\"CreditRating\"].append(None)\n",
    "        data[\"NetWorth\"].append(None)\n",
    "        data[\"MarketingNameplate\"].append(None)\n",
    "    # history tracking\n",
    "    data[\"EffectiveDate\"].append(pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "# Get unique TX_IDs and remove None values \n",
    "unique_tax_ids = set(national_tax_ids + local_tax_ids) - {None}\n",
    "all_tax_info = get_all_tax_info(unique_tax_ids)\n",
    "\n",
    "# map each tax ID to its description and rate\n",
    "for i in range(len(national_tax_ids)):\n",
    "    national_info = all_tax_info.get(\n",
    "        national_tax_ids[i], {\"TX_NAME\": None, \"TX_RATE\": None}\n",
    "    )\n",
    "    data[\"NationalTaxRateDesc\"].append(national_info[\"TX_NAME\"])\n",
    "    data[\"NationalTaxRate\"].append(national_info[\"TX_RATE\"])\n",
    "\n",
    "    local_info = all_tax_info.get(local_tax_ids[i], {\"TX_NAME\": None, \"TX_RATE\": None})\n",
    "    data[\"LocalTaxRateDesc\"].append(local_info[\"TX_NAME\"])\n",
    "    data[\"LocalTaxRate\"].append(local_info[\"TX_RATE\"])\n",
    "\n",
    "# Creating DataFrame\n",
    "dimCustomer_df = pd.concat([dimCustomer_df, pd.DataFrame(data)])\n",
    "dimCustomer_df[\"Status\"] = \"ACTIVE\"\n",
    "print(dimCustomer_df.info())\n",
    "dimCustomer_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2dict(df, exclude_columns):\n",
    "    # Remove the specified columns from the DataFrame\n",
    "    df_filtered = df.drop(columns=exclude_columns)\n",
    "    # Convert the filtered DataFrame to a dictionary\n",
    "    df_dict = df_filtered.to_dict(orient='index')\n",
    "    # Create a new dictionary that maps CustomerID to a dictionary of column values\n",
    "    customer_dict = {row['CustomerID']: {col: val for col, val in row.items() if col != 'CustomerID'} for _, row in df_dict.items()}\n",
    "    return customer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['SK_CustomerID', 'IsCurrent', 'BatchID', 'EffectiveDate', 'EndDate', 'Status']\n",
    "# dictionary to track latest values for each customer\n",
    "customer_data = df2dict(dimCustomer_df, exclude_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for NEW actions\n",
    "data = {col: [] for col in data.keys()}\n",
    "\n",
    "# Iterate through each 'Action' element with ActionType=\"UPDCUST\"\n",
    "for index, action in enumerate(upd_actions):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    name = customer.find(\"Name\", namespaces=namespace)\n",
    "    contact_info = customer.find(\"ContactInfo\", namespaces=namespace)\n",
    "    address = customer.find(\"Address\", namespaces=namespace)\n",
    "    tax_info = customer.find(\"TaxInfo\", namespaces=namespace)\n",
    "\n",
    "    customer_id = int(customer.get(\"C_ID\", None))\n",
    "    data[\"CustomerID\"].append(customer_id)\n",
    "\n",
    "    # Update tax_id\n",
    "    tax_id = customer.get(\"C_TAX_ID\", None)\n",
    "    if tax_id is None:\n",
    "        tax_id = customer_data[customer_id][\"TaxID\"]\n",
    "    else:\n",
    "        customer_data[customer_id][\"TaxID\"] = tax_id\n",
    "    data[\"TaxID\"].append(tax_id)\n",
    "    # Update tier\n",
    "    tier = customer.get(\"C_TIER\", None)\n",
    "    tier = int(tier) if tier else None\n",
    "    if tier is None:\n",
    "        tier = customer_data[customer_id][\"Tier\"]\n",
    "    else:\n",
    "        customer_data[customer_id][\"Tier\"] = tier\n",
    "        if tier is not None and tier not in (1,2,3):\n",
    "            \"\"\"\n",
    "            A record will be inserted in the DImessages table if a customer's Tier is not one of the valid\n",
    "            values (1,2,3). The MessageSource is “DimCustomer”, the MessageType is “Alert” and the\n",
    "            MessageText is “Invalid customer tier”. The MessageData field is “C_ID = ” followed by the\n",
    "            natural key value of the record, then “, C_TIER = ” and the C_TIER value.\n",
    "            \"\"\"\n",
    "            MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "            batch_id = 1\n",
    "            sk_customer_id = len(data[\"CustomerID\"]) + dimCustomer_df.shape[0]\n",
    "            message = f\"C_ID = {sk_customer_id}, C_TIER = {tier}\"\n",
    "            message_source = \"DimCustomer\"\n",
    "            message_type = \"Alert\"\n",
    "            message_text = \"Invalid customer tier\"\n",
    "            query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(query))\n",
    "                conn.commit()\n",
    "    data[\"Tier\"].append(tier)\n",
    "    # Update DOB\n",
    "    dob = customer.get(\"C_DOB\", None)\n",
    "    dob = pd.to_datetime(dob, format=\"%Y-%m-%d\") if dob else None\n",
    "    if dob is None:\n",
    "        dob = customer_data[customer_id][\"DOB\"]\n",
    "    else:\n",
    "        customer_data[customer_id][\"DOB\"] = dob\n",
    "        \"\"\"A record will be reported in the DImessages table if a customer's DOB is invalid. A customer's\n",
    "        DOB is invalid if DOB < Batch Date - 100 years or DOB > Batch Date (customer is over 100\n",
    "        years old or born in the future). The MessageSource is “DimCustomer”, the MessageType is\n",
    "        “Alert” and the MessageText is “DOB out of range”. The MessageData field is “C_ID = ”\n",
    "        followed by the natural key value of the record, then “, C_DOB = ” and the C_DOB value.\"\"\"\n",
    "        if dob and (dob < batch_date - pd.Timedelta(days=100*365) or dob > batch_date):\n",
    "            MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "            batch_id = 1\n",
    "            sk_customer_id = len(data[\"CustomerID\"])\n",
    "            message = f\"C_ID = {sk_customer_id}, C_DOB = {dob}\"\n",
    "            message_source = \"DimCustomer\"\n",
    "            message_type = \"Alert\"\n",
    "            message_text = \"DOB out of range\"\n",
    "            query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(query))\n",
    "                conn.commit()\n",
    "    data[\"DOB\"].append(dob)\n",
    "    # Update gender\n",
    "    gender = customer.get(\"C_GNDR\", None)\n",
    "    if gender is None:\n",
    "        gender = customer_data[customer_id][\"Gender\"]\n",
    "    else:\n",
    "        gender = \"U\" if gender not in (\"M\", \"F\") else gender\n",
    "        customer_data[customer_id][\"Gender\"] = gender\n",
    "    data[\"Gender\"].append(gender)\n",
    "\n",
    "    # Update first name\n",
    "    if name is None:\n",
    "        first_name = customer_data[customer_id][\"FirstName\"]\n",
    "        data[\"FirstName\"].append(first_name)\n",
    "        middle_initial = customer_data[customer_id][\"MiddleInitial\"]\n",
    "        data[\"MiddleInitial\"].append(middle_initial)\n",
    "        last_name = customer_data[customer_id][\"LastName\"]\n",
    "        data[\"LastName\"].append(last_name)\n",
    "    else:\n",
    "        first_name = name.findtext(\"C_F_NAME\", default=None, namespaces=namespace)\n",
    "        if first_name is None:\n",
    "            first_name = customer_data[customer_id][\"FirstName\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"FirstName\"] = first_name\n",
    "        data[\"FirstName\"].append(first_name)\n",
    "        # Update middle initial\n",
    "        middle_initial = name.findtext(\"C_M_NAME\", default=None, namespaces=namespace)\n",
    "        if middle_initial is None:\n",
    "            middle_initial = customer_data[customer_id][\"MiddleInitial\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"MiddleInitial\"] = middle_initial\n",
    "        data[\"MiddleInitial\"].append(middle_initial)\n",
    "        # Update last name\n",
    "        last_name = name.findtext(\"C_L_NAME\", default=None, namespaces=namespace)\n",
    "        if last_name is None:\n",
    "            last_name = customer_data[customer_id][\"LastName\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"LastName\"] = last_name\n",
    "        data[\"LastName\"].append(last_name if last_name else None)\n",
    "\n",
    "    if contact_info is None:\n",
    "        prim_email = customer_data[customer_id][\"Email1\"]\n",
    "        data[\"Email1\"].append(prim_email)\n",
    "        alt_email = customer_data[customer_id][\"Email2\"]\n",
    "        data[\"Email2\"].append(alt_email)\n",
    "        phone1 = customer_data[customer_id][\"Phone1\"]\n",
    "        data[\"Phone1\"].append(phone1)\n",
    "        phone2 = customer_data[customer_id][\"Phone2\"]\n",
    "        data[\"Phone2\"].append(phone2)\n",
    "        phone3 = customer_data[customer_id][\"Phone3\"]\n",
    "        data[\"Phone3\"].append(phone3)\n",
    "    else:\n",
    "        # update primary email\n",
    "        prim_email = contact_info.findtext(\n",
    "            \"C_PRIM_EMAIL\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if prim_email is None:\n",
    "            prim_email = customer_data[customer_id][\"Email1\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"Email1\"] = prim_email\n",
    "        data[\"Email1\"].append(prim_email if prim_email else None)\n",
    "        # update alternate email\n",
    "        alt_email = contact_info.findtext(\n",
    "            \"C_ALT_EMAIL\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if alt_email is None:\n",
    "            alt_email = customer_data[customer_id][\"Email2\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"Email2\"] = alt_email\n",
    "        data[\"Email2\"].append(alt_email if alt_email else None)\n",
    "        # update phone numbers\n",
    "        phone1 = contact_info.find(\"C_PHONE_1\", namespaces=namespace)\n",
    "        if phone1 is None:\n",
    "            phone1 = customer_data[customer_id][\"Phone1\"]\n",
    "        else:\n",
    "            phone1 = format_phone_number(phone1)\n",
    "            customer_data[customer_id][\"Phone1\"] = phone1\n",
    "        data[\"Phone1\"].append(phone1)\n",
    "        phone2 = contact_info.find(\"C_PHONE_2\", namespaces=namespace)\n",
    "        if phone2 is None:\n",
    "            phone2 = customer_data[customer_id][\"Phone2\"]\n",
    "        else:\n",
    "            phone2 = format_phone_number(phone2)\n",
    "            customer_data[customer_id][\"Phone2\"] = phone2\n",
    "        data[\"Phone2\"].append(phone2)\n",
    "        phone3 = contact_info.find(\"C_PHONE_3\", namespaces=namespace)\n",
    "        if phone3 is None:\n",
    "            phone3 = customer_data[customer_id][\"Phone3\"]\n",
    "        else:\n",
    "            phone3 = format_phone_number(phone3)\n",
    "            customer_data[customer_id][\"Phone3\"] = phone3\n",
    "        data[\"Phone3\"].append(phone3)\n",
    "\n",
    "    if address is None:\n",
    "        address_line1 = customer_data[customer_id][\"AddressLine1\"]\n",
    "        data[\"AddressLine1\"].append(address_line1)\n",
    "        address_line2 = customer_data[customer_id][\"AddressLine2\"]\n",
    "        data[\"AddressLine2\"].append(address_line2)\n",
    "        postalcode = customer_data[customer_id][\"PostalCode\"]\n",
    "        data[\"PostalCode\"].append(postalcode)\n",
    "        city = customer_data[customer_id][\"City\"]\n",
    "        data[\"City\"].append(city)\n",
    "        state_prov = customer_data[customer_id][\"StateProv\"]\n",
    "        data[\"StateProv\"].append(state_prov)\n",
    "        country = customer_data[customer_id][\"Country\"]\n",
    "        data[\"Country\"].append(country)\n",
    "    else:\n",
    "        # Extracting address information\n",
    "        address_line1 = address.findtext(\n",
    "            \"C_ADLINE1\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if address_line1 is None:\n",
    "            address_line1 = customer_data[customer_id][\"AddressLine1\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"AddressLine1\"] = address_line1\n",
    "        data[\"AddressLine1\"].append(address_line1 if address_line1 else None)\n",
    "        address_line2 = address.findtext(\n",
    "            \"C_ADLINE2\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if address_line2 is None:\n",
    "            address_line2 = customer_data[customer_id][\"AddressLine2\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"AddressLine2\"] = address_line2\n",
    "        data[\"AddressLine2\"].append(address_line2 if address_line2 else None)\n",
    "        postalcode = address.findtext(\"C_ZIPCODE\", default=None, namespaces=namespace)\n",
    "        if postalcode is None:\n",
    "            postalcode = customer_data[customer_id][\"PostalCode\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"PostalCode\"] = postalcode\n",
    "        data[\"PostalCode\"].append(postalcode if postalcode else None)\n",
    "        city = address.findtext(\"C_CITY\", default=None, namespaces=namespace)\n",
    "        if city is None:\n",
    "            city = customer_data[customer_id][\"City\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"City\"] = city\n",
    "        data[\"City\"].append(city if city else None)\n",
    "        state_prov = address.findtext(\n",
    "            \"C_STATE_PROV\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if state_prov is None:\n",
    "            state_prov = customer_data[customer_id][\"StateProv\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"StateProv\"] = state_prov\n",
    "        data[\"StateProv\"].append(state_prov if state_prov else None)\n",
    "        country = address.findtext(\"C_CTRY\", default=None, namespaces=namespace)\n",
    "        if country is None:\n",
    "            country = customer_data[customer_id][\"Country\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"Country\"] = country\n",
    "        data[\"Country\"].append(country if country else None)\n",
    "    \n",
    "    # Store TX_ID as placeholders\n",
    "    if tax_info is None:\n",
    "        national_tax_rate_desc = customer_data[customer_id][\"NationalTaxRateDesc\"]\n",
    "        data[\"NationalTaxRateDesc\"].append(national_tax_rate_desc)\n",
    "        national_tax_rate = customer_data[customer_id][\"NationalTaxRate\"]\n",
    "        data[\"NationalTaxRate\"].append(national_tax_rate)\n",
    "        local_tax_rate_desc = customer_data[customer_id][\"LocalTaxRateDesc\"]\n",
    "        data[\"LocalTaxRateDesc\"].append(local_tax_rate_desc)\n",
    "        local_tax_rate = customer_data[customer_id][\"LocalTaxRate\"]\n",
    "        data[\"LocalTaxRate\"].append(local_tax_rate)\n",
    "        national_tax_ids.append(None)\n",
    "        local_tax_ids.append(None)        \n",
    "    else:\n",
    "        national_tax_id = tax_info.findtext(\n",
    "            \"C_NAT_TX_ID\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if national_tax_id is None:\n",
    "            national_tax_rate_desc = customer_data[customer_id][\"NationalTaxRateDesc\"]\n",
    "            data[\"NationalTaxRateDesc\"].append(national_tax_rate_desc)\n",
    "            national_tax_rate = customer_data[customer_id][\"NationalTaxRate\"]\n",
    "            data[\"NationalTaxRate\"].append(national_tax_rate)\n",
    "            national_tax_ids.append(None)\n",
    "        else:\n",
    "            result = pd.read_sql(f\"SELECT TX_NAME, TX_RATE FROM taxrate WHERE TX_ID = '{national_tax_id}'\", engine)\n",
    "            national_tax_rate_desc = result.iloc[0, 0]\n",
    "            national_tax_rate = result.iloc[0, 1]\n",
    "            customer_data[customer_id][\"NationalTaxRateDesc\"] = national_tax_rate_desc\n",
    "            customer_data[customer_id][\"NationalTaxRate\"] = national_tax_rate\n",
    "            data[\"NationalTaxRateDesc\"].append(national_tax_rate_desc)\n",
    "            data[\"NationalTaxRate\"].append(national_tax_rate)\n",
    "        local_tax_id = tax_info.findtext(\n",
    "            \"C_LCL_TX_ID\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if local_tax_id is None:\n",
    "            local_tax_rate_desc = customer_data[customer_id][\"LocalTaxRateDesc\"]\n",
    "            data[\"LocalTaxRateDesc\"].append(local_tax_rate_desc)\n",
    "            local_tax_rate = customer_data[customer_id][\"LocalTaxRate\"]\n",
    "            data[\"LocalTaxRate\"].append(local_tax_rate)\n",
    "        else:\n",
    "            result = pd.read_sql(f\"SELECT TX_NAME, TX_RATE FROM taxrate WHERE TX_ID = '{local_tax_id}'\", engine)\n",
    "            local_tax_rate_desc = result.iloc[0, 0]\n",
    "            local_tax_rate = result.iloc[0, 1]\n",
    "            customer_data[customer_id][\"LocalTaxRateDesc\"] = local_tax_rate_desc\n",
    "            customer_data[customer_id][\"LocalTaxRate\"] = local_tax_rate\n",
    "            data[\"LocalTaxRateDesc\"].append(local_tax_rate_desc)\n",
    "            data[\"LocalTaxRate\"].append(local_tax_rate)\n",
    "\n",
    "\n",
    "    if not has_later_update(customer_id, index):\n",
    "        # Find matching prospect record\n",
    "        match = prospect_df[\n",
    "            (prospect_df_temp[\"LastName\"] == last_name.upper())\n",
    "            & (prospect_df_temp[\"FirstName\"] == first_name.upper())\n",
    "            & (prospect_df_temp[\"AddressLine1\"] == address_line1.upper())\n",
    "            & (prospect_df_temp[\"AddressLine2\"] == address_line2.upper())\n",
    "            & (prospect_df_temp[\"PostalCode\"] == postalcode.upper())\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            # Set values from the matching prospect record\n",
    "            data[\"AgencyID\"].append(match[\"AgencyID\"].iloc[0])\n",
    "            data[\"CreditRating\"].append(match[\"CreditRating\"].iloc[0])\n",
    "            data[\"NetWorth\"].append(match[\"NetWorth\"].iloc[0])\n",
    "            data[\"MarketingNameplate\"].append(match[\"MarketingNameplate\"].iloc[0])\n",
    "        else:\n",
    "            # Set values to those in customer_data\n",
    "            data[\"AgencyID\"].append(customer_data[customer_id][\"AgencyID\"])\n",
    "            data[\"CreditRating\"].append(customer_data[customer_id][\"CreditRating\"])\n",
    "            data[\"NetWorth\"].append(customer_data[customer_id][\"NetWorth\"])\n",
    "            data[\"MarketingNameplate\"].append(customer_data[customer_id][\"MarketingNameplate\"])\n",
    "    else:\n",
    "        # Set values to those in customer_data due to later 'UPDCUST' or 'INACT'\n",
    "        data[\"AgencyID\"].append(customer_data[customer_id][\"AgencyID\"])\n",
    "        data[\"CreditRating\"].append(customer_data[customer_id][\"CreditRating\"])\n",
    "        data[\"NetWorth\"].append(customer_data[customer_id][\"NetWorth\"])\n",
    "        data[\"MarketingNameplate\"].append(customer_data[customer_id][\"MarketingNameplate\"])\n",
    "    # history tracking\n",
    "    data[\"EffectiveDate\"].append(pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "# Creating DataFrame\n",
    "dimCustomer_df = pd.concat([dimCustomer_df, pd.DataFrame(data)])\n",
    "dimCustomer_df[\"Status\"] = \"ACTIVE\"\n",
    "print(dimCustomer_df.info())\n",
    "dimCustomer_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for NEW actions\n",
    "data = {col: [] for col in data.keys()}\n",
    "\n",
    "# Iterate through each 'Action' element with ActionType=\"INACT\"\n",
    "for index, action in enumerate(inact_actions):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    customer_id = int(customer.get(\"C_ID\", None))\n",
    "    data[\"CustomerID\"].append(customer_id)\n",
    "    # Copy all fields from customer_data\n",
    "    for col in data.keys():\n",
    "        if col in (\"CustomerID\", \"EffectiveDate\"):\n",
    "            continue\n",
    "        else:\n",
    "            data[col].append(customer_data[customer_id][col])\n",
    "    # history tracking\n",
    "    data[\"EffectiveDate\"].append(pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "# Creating DataFrame\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df[\"Status\"] = \"INACTIVE\"\n",
    "dimCustomer_df = pd.concat([dimCustomer_df, data_df])\n",
    "dimCustomer_df[\"BatchID\"] = 1\n",
    "print(dimCustomer_df.info())\n",
    "dimCustomer_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimCustomer_df['SK_CustomerID'] = range(1, len(dimCustomer_df) + 1)\n",
    "dimCustomer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by CustomerID and EffectiveDate\n",
    "dimCustomer_df.sort_values(by=['CustomerID', 'EffectiveDate'], inplace=True)\n",
    "# Create a shifted DataFrame\n",
    "shifted_df = dimCustomer_df.shift(-1)\n",
    "# Update EndDate: If next row has same CustomerID, use its EffectiveDate; otherwise, use default date\n",
    "dimCustomer_df['EndDate'] = pd.Timestamp('9999-12-31')\n",
    "mask = dimCustomer_df['CustomerID'] == shifted_df['CustomerID']\n",
    "dimCustomer_df.loc[mask, 'EndDate'] = shifted_df.loc[mask, 'EffectiveDate']\n",
    "\n",
    "# Update IsCurrent: True if next row has different CustomerID or is the last row\n",
    "dimCustomer_df['IsCurrent'] = ~mask\n",
    "dimCustomer_df.sort_values(by=['SK_CustomerID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'CustomerID': sqlalchemy.types.Integer,\n",
    "    'TaxID': sqlalchemy.types.String(20),\n",
    "    'Status': sqlalchemy.types.String(10),\n",
    "    'LastName': sqlalchemy.types.String(30),\n",
    "    'FirstName': sqlalchemy.types.String(30),\n",
    "    'MiddleInitial': sqlalchemy.types.String(1),\n",
    "    'Gender': sqlalchemy.types.String(1),\n",
    "    'Tier': sqlalchemy.types.SmallInteger,\n",
    "    'DOB': sqlalchemy.types.Date,\n",
    "    'AddressLine1': sqlalchemy.types.String(80),\n",
    "    'AddressLine2': sqlalchemy.types.String(80),\n",
    "    'PostalCode': sqlalchemy.types.String(12),\n",
    "    'City': sqlalchemy.types.String(25),\n",
    "    'StateProv': sqlalchemy.types.String(20),\n",
    "    'Country': sqlalchemy.types.String(24),\n",
    "    'Phone1': sqlalchemy.types.String(30),\n",
    "    'Phone2': sqlalchemy.types.String(30),\n",
    "    'Phone3': sqlalchemy.types.String(30),\n",
    "    'Email1': sqlalchemy.types.String(50),\n",
    "    'Email2': sqlalchemy.types.String(50),\n",
    "    'NationalTaxRateDesc': sqlalchemy.types.String(50),\n",
    "    'NationalTaxRate': sqlalchemy.types.Numeric(6, 5),\n",
    "    'LocalTaxRateDesc': sqlalchemy.types.String(50),\n",
    "    'LocalTaxRate': sqlalchemy.types.Numeric(6, 5),\n",
    "    'AgencyID': sqlalchemy.types.String(30),\n",
    "    'CreditRating': sqlalchemy.types.SmallInteger,\n",
    "    'NetWorth': sqlalchemy.types.Numeric(10),\n",
    "    'MarketingNameplate': sqlalchemy.types.String(100),\n",
    "    'IsCurrent': sqlalchemy.types.Boolean,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'EffectiveDate': sqlalchemy.types.Date,\n",
    "    'EndDate': sqlalchemy.types.Date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int 32\n",
    "cols = ['SK_CustomerID', 'CustomerID', 'BatchID']\n",
    "for col in cols:\n",
    "    dimCustomer_df[col] = dimCustomer_df[col].astype('int')\n",
    "dimCustomer_df['Tier'] = dimCustomer_df['Tier'].astype('UInt8')\n",
    "dimCustomer_df['NationalTaxRate'] = dimCustomer_df['NationalTaxRate'].astype('float32')\n",
    "dimCustomer_df['LocalTaxRate'] = dimCustomer_df['LocalTaxRate'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimCustomer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame columns to the appropriate types\n",
    "dimCustomer_df['SK_CustomerID'] = dimCustomer_df['SK_CustomerID'].astype(np.int32)\n",
    "dimCustomer_df['CustomerID'] = dimCustomer_df['CustomerID'].astype(np.int32)\n",
    "dimCustomer_df['Tier'] = dimCustomer_df['Tier'].astype(pd.Int8Dtype())\n",
    "dimCustomer_df['NationalTaxRate'] = dimCustomer_df['NationalTaxRate'].astype(np.float64)\n",
    "dimCustomer_df['LocalTaxRate'] = dimCustomer_df['LocalTaxRate'].astype(np.float64)\n",
    "dimCustomer_df['CreditRating'] = dimCustomer_df['CreditRating'].astype(pd.Int16Dtype())\n",
    "dimCustomer_df['NetWorth'] = dimCustomer_df['NetWorth'].astype(pd.Float64Dtype())\n",
    "dimCustomer_df['BatchID'] = dimCustomer_df['BatchID'].astype(np.int16)\n",
    "\n",
    "# Convert date columns to datetime.date\n",
    "dimCustomer_df['DOB'] = pd.to_datetime(dimCustomer_df['DOB']).dt.date\n",
    "dimCustomer_df['EffectiveDate'] = pd.to_datetime(dimCustomer_df['EffectiveDate']).dt.date\n",
    "dimCustomer_df['EndDate'] = pd.to_datetime(dimCustomer_df['EndDate']).dt.date\n",
    "\n",
    "# Now try writing to SQL\n",
    "dimCustomer_df.to_sql('dimcustomer', engine, if_exists='replace', index=False, dtype=sql_dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary uppercase columns for merging in both DataFrames\n",
    "merge_fields = [\"FirstName\", \"LastName\", \"AddressLine1\", \"AddressLine2\", \"PostalCode\"]\n",
    "for field in merge_fields:\n",
    "    prospect_df[f\"temp_{field}\"] = prospect_df[field].str.upper()\n",
    "    dimCustomer_df[f\"temp_{field}\"] = dimCustomer_df[field].str.upper()\n",
    "\n",
    "# Filter dimCustomer_df for active and current customers\n",
    "active_customers = dimCustomer_df[(dimCustomer_df['IsCurrent'] == True) & (dimCustomer_df['Status'] == 'ACTIVE')]\n",
    "\n",
    "# Perform an outer merge on the temporary uppercase fields\n",
    "temp_merge_fields = [f\"temp_{field}\" for field in merge_fields]\n",
    "merged_df = prospect_df.merge(active_customers, how='left', \n",
    "                              left_on=temp_merge_fields, right_on=temp_merge_fields,\n",
    "                              indicator=True)\n",
    "\n",
    "# Update IsCustomer based on whether a match was found\n",
    "prospect_df['IsCustomer'] = merged_df['_merge'] == 'both'\n",
    "\n",
    "# Clean up by dropping the temporary columns\n",
    "prospect_df.drop(columns=temp_merge_fields, inplace=True)\n",
    "dimCustomer_df.drop(columns=temp_merge_fields, inplace=True)\n",
    "\n",
    "prospect_df['IsCustomer'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df['BatchID'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'AgencyID': sqlalchemy.types.CHAR(30),\n",
    "    'SK_RecordDateID': sqlalchemy.types.Integer,\n",
    "    'SK_UpdateDateID': sqlalchemy.types.Integer,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'IsCustomer': sqlalchemy.types.Boolean,\n",
    "    'LastName': sqlalchemy.types.CHAR(30),\n",
    "    'FirstName': sqlalchemy.types.CHAR(30),\n",
    "    'MiddleInitial': sqlalchemy.types.CHAR(1),\n",
    "    'Gender': sqlalchemy.types.CHAR(1),\n",
    "    'AddressLine1': sqlalchemy.types.CHAR(80),\n",
    "    'AddressLine2': sqlalchemy.types.CHAR(80),\n",
    "    'PostalCode': sqlalchemy.types.CHAR(12),\n",
    "    'City': sqlalchemy.types.CHAR(25),\n",
    "    'State': sqlalchemy.types.CHAR(20),\n",
    "    'Country': sqlalchemy.types.CHAR(24),\n",
    "    'Phone': sqlalchemy.types.CHAR(30),\n",
    "    'Income': sqlalchemy.types.Integer,\n",
    "    'NumberCars': sqlalchemy.types.SmallInteger,\n",
    "    'NumberChildren': sqlalchemy.types.SmallInteger,\n",
    "    'MaritalStatus': sqlalchemy.types.CHAR(1),\n",
    "    'Age': sqlalchemy.types.SmallInteger,\n",
    "    'CreditRating': sqlalchemy.types.SmallInteger,\n",
    "    'OwnOrRentFlag': sqlalchemy.types.CHAR(1),\n",
    "    'Employer': sqlalchemy.types.CHAR(30),\n",
    "    'NumberCreditCards': sqlalchemy.types.SmallInteger,\n",
    "    'NetWorth': sqlalchemy.types.BigInteger,\n",
    "    'MarketingNameplate': sqlalchemy.types.CHAR(100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df.to_sql('prospect', engine, if_exists='replace', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Prospect file is processed, a count is kept of new Prospect table rows created. After\n",
    "the last row, a “Status” message is written to the DImessages table, with the MessageSource\n",
    "“Prospect”, MessageText “Inserted rows” and the MessageData field containing the number\n",
    "of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = prospect_df.shape[0]\n",
    "message_type = \"Status\"\n",
    "message_source = \"Prospect\"\n",
    "message_text = f\"Inserted rows\"\n",
    "MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "batch_id = 1\n",
    "\n",
    "query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{num_rows}')\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimAccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema as a dictionary\n",
    "schema = {\n",
    "    'SK_AccountID': 'uint32',\n",
    "    'AccountID': 'uint32',\n",
    "    'SK_BrokerID': 'uint32',\n",
    "    'SK_CustomerID': 'uint32',\n",
    "    'Status': 'str',\n",
    "    'AccountDesc': 'str',\n",
    "    'TaxStatus': 'UInt8',\n",
    "    'IsCurrent': 'bool',\n",
    "    'BatchID': 'uint8',\n",
    "    'EffectiveDate': 'datetime64[ns]',\n",
    "    'EndDate': 'datetime64[ns]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_DIR + \"CustomerMgmt.xml\"\n",
    "tree = etree.parse(data_file)\n",
    "namespace = {'tpcdi': 'http://www.tpc.org/tpc-di'}\n",
    "\n",
    "# Get all actions\n",
    "all_actions = tree.xpath(\".//tpcdi:Action\", namespaces=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified schema\n",
    "dimAccount_df = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in schema.items()})\n",
    "\n",
    "# initialize lists to store data\n",
    "relevant_cols = ['AccountID', 'SK_BrokerID', 'SK_CustomerID', 'Status', 'AccountDesc', 'TaxStatus', 'EffectiveDate']\n",
    "data = {col: [] for col in relevant_cols}\n",
    "\n",
    "# initialize dict to store most recent values for each account of a customer\n",
    "customer_accounts = dict()\n",
    "\n",
    "for index, action in enumerate(tqdm(all_actions)):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    customer_id = int(customer.get(\"C_ID\", None))\n",
    "    if customer_id not in customer_accounts:\n",
    "        customer_accounts[customer_id] = dict()\n",
    "    if action.get(\"ActionType\") in (\"NEW\", \"ADDACCT\"):\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            # print(\"NEW/ADDACT:\", customer_id, account_id)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            # Customer/Account/CA_NAME\n",
    "            account_desc = account.findtext(\"CA_NAME\", default=None, namespaces=namespace)\n",
    "            data[\"AccountDesc\"].append(account_desc)\n",
    "            # Customer/Account/@CA_TAX_ST\n",
    "            tax_status = account.get(\"CA_TAX_ST\", None)\n",
    "            tax_status = int(tax_status) if tax_status else None\n",
    "            data[\"TaxStatus\"].append(tax_status)\n",
    "            #  Customer/Account/CA_B_ID\n",
    "            broker_id = account.findtext(\"CA_B_ID\", default=None, namespaces=namespace)\n",
    "            broker_id = int(broker_id) if broker_id else None\n",
    "            data[\"SK_BrokerID\"].append((broker_id, action_ts))\n",
    "            data[\"SK_CustomerID\"].append((customer_id, action_ts))\n",
    "            status = \"ACTIVE\"\n",
    "            data[\"Status\"].append(status)\n",
    "            # update customer_accounts\n",
    "            customer_accounts[customer_id][account_id] = {\n",
    "                \"AccountDesc\": account_desc,\n",
    "                \"TaxStatus\": tax_status,\n",
    "                \"SK_BrokerID\": (broker_id, action_ts),\n",
    "                \"SK_CustomerID\": (customer_id, action_ts),\n",
    "                \"Status\": status,\n",
    "            }\n",
    "    elif action.get(\"ActionType\") == \"UPDACCT\":\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            # Customer/Account/CA_NAME\n",
    "            account_desc = account.findtext(\"CA_NAME\", default=None, namespaces=namespace)\n",
    "            if account_desc is None:\n",
    "                account_desc = customer_accounts[customer_id][account_id][\"AccountDesc\"]\n",
    "            else:\n",
    "                customer_accounts[customer_id][account_id][\"AccountDesc\"] = account_desc\n",
    "            data[\"AccountDesc\"].append(account_desc)\n",
    "            # Customer/Account/@CA_TAX_ST\n",
    "            tax_status = account.get(\"CA_TAX_ST\", None)\n",
    "            tax_status = int(tax_status) if tax_status else None\n",
    "            if tax_status is None:\n",
    "                tax_status = customer_accounts[customer_id][account_id][\"TaxStatus\"]\n",
    "            else:\n",
    "                customer_accounts[customer_id][account_id][\"TaxStatus\"] = tax_status\n",
    "            data[\"TaxStatus\"].append(tax_status)\n",
    "            #  Customer/Account/CA_B_ID\n",
    "            broker_id = account.findtext(\"CA_B_ID\", default=None, namespaces=namespace)\n",
    "            broker_id = int(broker_id) if broker_id else None\n",
    "            if broker_id is None:\n",
    "                broker_id = customer_accounts[customer_id][account_id][\"SK_BrokerID\"][0]\n",
    "            else:\n",
    "                customer_accounts[customer_id][account_id][\"SK_BrokerID\"] = (broker_id, action_ts)\n",
    "            sk_brokerid = (broker_id, action_ts)\n",
    "            data[\"SK_BrokerID\"].append(sk_brokerid)\n",
    "            sk_customer_id = (customer_id, action_ts)\n",
    "            customer_accounts[customer_id][account_id][\"SK_CustomerID\"] = sk_customer_id\n",
    "            data[\"SK_CustomerID\"].append(sk_customer_id)\n",
    "            status = \"ACTIVE\"\n",
    "            data[\"Status\"].append(status)\n",
    "    elif action.get(\"ActionType\") == \"UPDCUST\":\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            print(\"UPDCUST:\", customer_id, account_id)\n",
    "            # set all other fields as is\n",
    "            for col in customer_accounts[customer_id][account_id]:\n",
    "                if not col.startswith(\"SK_\"):\n",
    "                    data[col].append(customer_accounts[customer_id][account_id][col])\n",
    "            broker_id = customer_accounts[customer_id][account_id][\"SK_BrokerID\"][0]\n",
    "            customer_accounts[customer_id][account_id][\"SK_BrokerID\"] = (broker_id, action_ts)\n",
    "            sk_brokerid = (broker_id, action_ts)\n",
    "            data[\"SK_BrokerID\"].append(sk_brokerid)\n",
    "            sk_customer_id = (customer_id, action_ts)\n",
    "            customer_accounts[customer_id][account_id][\"SK_CustomerID\"] = sk_customer_id\n",
    "            data[\"SK_CustomerID\"].append(sk_customer_id)\n",
    "    elif action.get(\"ActionType\") in (\"INACT\", \"CLOSEACCT\"):\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            # set all other fields as is\n",
    "            for col in customer_accounts[customer_id][account_id]:\n",
    "                if col.startswith(\"SK_\"):\n",
    "                    continue\n",
    "                elif col != \"Status\":\n",
    "                    data[col].append(customer_accounts[customer_id][account_id][col])\n",
    "                else:\n",
    "                    data[col].append(\"INACTIVE\")\n",
    "                    customer_accounts[customer_id][account_id][col] = \"INACTIVE\"\n",
    "            broker_id = customer_accounts[customer_id][account_id][\"SK_BrokerID\"][0]\n",
    "            customer_accounts[customer_id][account_id][\"SK_BrokerID\"] = (broker_id, action_ts)\n",
    "            sk_brokerid = (broker_id, action_ts)\n",
    "            data[\"SK_BrokerID\"].append(sk_brokerid)\n",
    "            sk_customer_id = (customer_id, action_ts)\n",
    "            customer_accounts[customer_id][account_id][\"SK_CustomerID\"] = sk_customer_id\n",
    "            data[\"SK_CustomerID\"].append(sk_customer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the database to get all SK_BrokerID\n",
    "query_parts = [\n",
    "    f\"(BrokerID = {broker_id} AND EffectiveDate <= '{action_ts}' <= EndDate)\"\n",
    "    for broker_id, action_ts in data[\"SK_BrokerID\"]\n",
    "]\n",
    "# Joining all conditions with 'OR'\n",
    "conditions = \" OR \".join(query_parts)\n",
    "query = f\"\"\"SELECT BrokerID, EffectiveDate, EndDate, SK_BrokerID \n",
    "FROM dimbroker \n",
    "WHERE {conditions}\"\"\"\n",
    "result = pd.read_sql_query(query, engine)\n",
    "for index, pair in enumerate(data[\"SK_BrokerID\"]):\n",
    "    broker_id, action_ts = pair    \n",
    "    sk_brokerid = result[\n",
    "        (result[\"BrokerID\"] == broker_id)\n",
    "        & (result[\"EffectiveDate\"] <= action_ts.date())\n",
    "        & (action_ts.date() <= result[\"EndDate\"])\n",
    "    ].iloc[0, 3]\n",
    "    data[\"SK_BrokerID\"][index] = sk_brokerid\n",
    "\n",
    "# query the database to get all SK_CustomerID\n",
    "query_parts = [\n",
    "    f\"(CustomerID = {customer_id} AND EffectiveDate <= '{action_ts}' <= EndDate)\"\n",
    "    for customer_id, action_ts in data[\"SK_CustomerID\"]\n",
    "]\n",
    "# Joining all conditions with 'OR'\n",
    "conditions = \" OR \".join(query_parts)\n",
    "query = f\"\"\"SELECT CustomerID, EffectiveDate, EndDate, SK_CustomerID \n",
    "FROM dimcustomer \n",
    "WHERE {conditions}\"\"\"\n",
    "result = pd.read_sql_query(query, engine)\n",
    "for index, pair in enumerate(data[\"SK_CustomerID\"]):\n",
    "    customer_id, action_ts = pair\n",
    "    sk_brokerid = result[\n",
    "        (result[\"CustomerID\"] == customer_id)\n",
    "        & (result[\"EffectiveDate\"] <= action_ts.date())\n",
    "        & (action_ts.date() <= result[\"EndDate\"])\n",
    "    ].iloc[0, 3]\n",
    "    data[\"SK_CustomerID\"][index] = sk_brokerid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data:\n",
    "    dimAccount_df[col] = data[col]\n",
    "dimAccount_df['SK_AccountID'] = range(1, len(dimAccount_df) + 1)\n",
    "dimAccount_df['BatchID'] = 1\n",
    "\n",
    "# Sort the DataFrame by CustomerID and EffectiveDate\n",
    "dimAccount_df.sort_values(by=['AccountID', 'EffectiveDate'], inplace=True)\n",
    "# Create a shifted DataFrame\n",
    "shifted_df = dimAccount_df.shift(-1)\n",
    "# Update EndDate: If next row has same CustomerID, use its EffectiveDate; otherwise, use default date\n",
    "dimAccount_df['EndDate'] = pd.Timestamp('9999-12-31')\n",
    "mask = dimAccount_df['AccountID'] == shifted_df['AccountID']\n",
    "dimAccount_df.loc[mask, 'EndDate'] = shifted_df.loc[mask, 'EffectiveDate']\n",
    "\n",
    "# Update IsCurrent: True if next row has different CustomerID or is the last row\n",
    "dimAccount_df['IsCurrent'] = ~mask\n",
    "dimAccount_df.sort_values(by=['SK_AccountID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'SK_AccountID': sqlalchemy.types.Integer,\n",
    "    'AccountID': sqlalchemy.types.Integer,\n",
    "    'SK_BrokerID': sqlalchemy.types.Integer,\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'Status': sqlalchemy.types.String(10),\n",
    "    'AccountDesc': sqlalchemy.types.String(50),\n",
    "    'TaxStatus': sqlalchemy.types.SmallInteger,\n",
    "    'IsCurrent': sqlalchemy.types.Boolean,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'EffectiveDate': sqlalchemy.types.Date,\n",
    "    'EndDate': sqlalchemy.types.Date\n",
    "}\n",
    "dimAccount_df.to_sql('dimaccount', engine, if_exists='replace', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'T_ID', 'T_DTS', 'T_ST_ID', 'T_TT_ID', 'T_IS_CASH', \n",
    "    'T_S_SYMB', 'T_QTY', 'T_BID_PRICE', 'T_CA_ID', 'T_EXEC_NAME', \n",
    "    'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX'\n",
    "]\n",
    "dtypes = {\n",
    "    'T_ID': 'uint64',\n",
    "    'T_DTS': 'str',\n",
    "    'T_ST_ID': 'str',\n",
    "    'T_TT_ID': 'str',\n",
    "    'T_IS_CASH': 'bool',\n",
    "    'T_S_SYMB': 'str',\n",
    "    'T_QTY': 'uint32',\n",
    "    'T_BID_PRICE': 'float64',\n",
    "    'T_CA_ID': 'uint32',\n",
    "    'T_EXEC_NAME': 'str',\n",
    "    'T_TRADE_PRICE': 'float64',\n",
    "    'T_CHRG': 'float64',\n",
    "    'T_COMM': 'float64',\n",
    "    'T_TAX': 'float64'\n",
    "}\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "trade_df = pd.read_csv(\n",
    "    DATA_DIR + \"Trade.txt\", \n",
    "    sep='|', \n",
    "    header=None, \n",
    "    names=columns, \n",
    "    dtype=dtypes,\n",
    "    parse_dates=['T_DTS']\n",
    ")\n",
    "trade_df['T_DTS'] = pd.to_datetime(trade_df['T_DTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"TH_T_ID\", \"TH_DTS\", \"TH_ST_ID\"]\n",
    "dtypes = {\n",
    "    \"TH_T_ID\": \"uint64\",\n",
    "    \"TH_DTS\": \"str\",\n",
    "    \"TH_ST_ID\": \"str\"\n",
    "}\n",
    "tradehistory_df = pd.read_csv(DATA_DIR + \"TradeHistory.txt\", sep=\"|\", header=None, \n",
    "                              names=columns, dtype=dtypes, parse_dates=['TH_DTS'])\n",
    "tradehistory_df['TH_DTS'] = pd.to_datetime(tradehistory_df['TH_DTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged = tradehistory_df.merge(trade_df, left_on='TH_T_ID', right_on='T_ID')\n",
    "del tradehistory_df\n",
    "del trade_df\n",
    "print(trade_merged.shape)\n",
    "trade_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-fetch data from related tables\n",
    "date_mapping = pd.read_sql(\"SELECT DateValue, SK_DateID FROM dimdate\", engine).set_index(\"DateValue\")[\"SK_DateID\"].to_dict()\n",
    "time_mapping = pd.read_sql(\"SELECT TimeValue, SK_TimeID FROM dimtime\", engine).set_index(\"TimeValue\")[\"SK_TimeID\"].to_dict()\n",
    "status_mapping = pd.read_sql(\"SELECT ST_ID, ST_NAME FROM statustype\", engine).set_index(\"ST_ID\")[\"ST_NAME\"].to_dict()\n",
    "trade_type_mapping = pd.read_sql(\"SELECT TT_ID, TT_NAME FROM tradetype\", engine).set_index(\"TT_ID\")[\"TT_NAME\"].to_dict()\n",
    "\n",
    "# Fetching security and account info in one go\n",
    "security_info = pd.read_sql(\"SELECT Symbol, SK_SecurityID, SK_CompanyID, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])\n",
    "account_info = pd.read_sql(\"SELECT AccountID, SK_AccountID, SK_CustomerID, SK_BrokerID, EffectiveDate, EndDate FROM dimaccount\", engine)\n",
    "account_info['EffectiveDate'] = pd.to_datetime(account_info['EffectiveDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct copy\n",
    "trade_merged[\"TradeID\"] = trade_merged[\"T_ID\"]\n",
    "trade_merged[\"CashFlag\"] = trade_merged[\"T_IS_CASH\"]\n",
    "trade_merged[\"Quantity\"] = trade_merged[\"T_QTY\"]\n",
    "trade_merged[\"BidPrice\"] = trade_merged[\"T_BID_PRICE\"]\n",
    "trade_merged[\"ExecutedBy\"] = trade_merged[\"T_EXEC_NAME\"]\n",
    "trade_merged[\"TradePrice\"] = trade_merged[\"T_TRADE_PRICE\"]\n",
    "trade_merged[\"Fee\"] = trade_merged[\"T_CHRG\"]\n",
    "trade_merged[\"Commission\"] = trade_merged[\"T_COMM\"]\n",
    "trade_merged[\"Tax\"] = trade_merged[\"T_TAX\"]\n",
    "trade_merged[\"Status\"] = trade_merged[\"TH_ST_ID\"].map(status_mapping)\n",
    "trade_merged[\"Type\"] = trade_merged[\"T_TT_ID\"].map(trade_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially set null\n",
    "trade_merged[\"SK_CreateDateID\"] = None\n",
    "trade_merged[\"SK_CreateTimeID\"] = None\n",
    "trade_merged[\"SK_CloseDateID\"] = None\n",
    "trade_merged[\"SK_CloseTimeID\"] = None\n",
    "\n",
    "# now populate\n",
    "create_mask = (trade_merged[\"TH_ST_ID\"] == \"PNDG\") | (trade_merged[\"TH_ST_ID\"] == \"SBMT\")\n",
    "trade_merged.loc[create_mask, \"SK_CreateDateID\"] = trade_merged.loc[create_mask, \"TH_DTS\"].dt.date.map(date_mapping)\n",
    "trade_merged.loc[create_mask, \"SK_CreateTimeID\"] = pd.to_timedelta(trade_merged.loc[create_mask, \"TH_DTS\"].dt.time.astype(str)).map(time_mapping)\n",
    "close_mask = (trade_merged[\"TH_ST_ID\"] == \"CMPT\") | (trade_merged[\"TH_ST_ID\"] == \"CNCL\")\n",
    "trade_merged.loc[close_mask, \"SK_CloseDateID\"] = trade_merged.loc[close_mask, \"TH_DTS\"].dt.date.map(date_mapping)\n",
    "trade_merged.loc[close_mask, \"SK_CloseTimeID\"] = pd.to_timedelta(trade_merged.loc[close_mask, \"TH_DTS\"].dt.time.astype(str)).map(time_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_merged[\n",
    "#     [\n",
    "#         \"TradeID\",\n",
    "#         \"SK_CreateDateID\",\n",
    "#         \"SK_CreateTimeID\",\n",
    "#         \"SK_CloseDateID\",\n",
    "#         \"SK_CloseTimeID\",\n",
    "#     ]\n",
    "# ].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged.rename({\"T_S_SYMB\": \"Symbol\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trade_merged.shape)\n",
    "# trade_merged1 = pd.merge_asof(\n",
    "#     trade_merged.sort_values(\"TH_DTS\"),\n",
    "#     security_info.sort_values(\"EffectiveDate\"),\n",
    "#     left_on=\"TH_DTS\",\n",
    "#     right_on=\"EffectiveDate\",\n",
    "#     by=\"Symbol\",\n",
    "#     direction=\"forward\",\n",
    "# )\n",
    "# print(trade_merged1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trade_merged.shape)\n",
    "trade_merged = pd.merge(\n",
    "    trade_merged,\n",
    "    security_info,\n",
    "    how=\"left\",\n",
    "    on=\"Symbol\",\n",
    ")\n",
    "# print(trade_merged.shape)\n",
    "# trade_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_merged[\n",
    "#     [\n",
    "#         \"TradeID\",\n",
    "#         \"Symbol\",\n",
    "#         \"SK_CreateDateID\",\n",
    "#         \"SK_CreateTimeID\",\n",
    "#         \"SK_CloseDateID\",\n",
    "#         \"SK_CloseTimeID\",\n",
    "#     ]\n",
    "# ].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows where TH_DTS < EffectiveDate  or TH_DTS > EndDate\n",
    "# print(trade_merged.shape)\n",
    "trade_merged = trade_merged[\n",
    "    (trade_merged[\"TH_DTS\"] >= trade_merged[\"EffectiveDate\"])\n",
    "    & (trade_merged[\"TH_DTS\"].dt.date < trade_merged[\"EndDate\"])\n",
    "]\n",
    "# print(trade_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_merged[[\"TradeID\", \"T_CA_ID\", \"Symbol\", \"TH_DTS\", \"SK_SecurityID\", \"SK_CompanyID\", \"EffectiveDate\", \"EndDate\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged = pd.merge(\n",
    "    trade_merged,\n",
    "    account_info,\n",
    "    how=\"left\",\n",
    "    left_on=\"T_CA_ID\",\n",
    "    right_on=\"AccountID\",\n",
    ")\n",
    "trade_merged = trade_merged[\n",
    "    (trade_merged[\"TH_DTS\"] >= trade_merged[\"EffectiveDate_y\"])\n",
    "    & (trade_merged[\"TH_DTS\"].dt.date < trade_merged[\"EndDate_y\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_merged[[\"TradeID\", \"T_CA_ID\", \"TH_DTS\", 'SK_AccountID', 'SK_CustomerID', 'SK_BrokerID', \"EffectiveDate_x\", \"EndDate_x\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [\n",
    "    \"TradeID\",\n",
    "    \"SK_BrokerID\",\n",
    "    \"SK_CreateDateID\",\n",
    "    \"SK_CreateTimeID\",\n",
    "    \"SK_CloseDateID\",\n",
    "    \"SK_CloseTimeID\",\n",
    "    \"Status\",\n",
    "    \"Type\",\n",
    "    \"CashFlag\",\n",
    "    \"SK_SecurityID\",\n",
    "    \"SK_CompanyID\",\n",
    "    \"Quantity\",\n",
    "    \"BidPrice\",\n",
    "    \"SK_CustomerID\",\n",
    "    \"SK_AccountID\",\n",
    "    \"ExecutedBy\",\n",
    "    \"TradePrice\",\n",
    "    \"Fee\",\n",
    "    \"Commission\",\n",
    "    \"Tax\",\n",
    "]\n",
    "trade_merged = trade_merged[use_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged = trade_merged.groupby(\"TradeID\").last().reset_index()\n",
    "trade_merged['BatchID'] = 1\n",
    "# trade_merged = trade_merged.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'TradeID': 'uint32',\n",
    "    'SK_BrokerID': 'UInt32',\n",
    "    'SK_CreateDateID': 'uint32',\n",
    "    'SK_CreateTimeID': 'uint32',\n",
    "    'SK_CloseDateID': 'UInt32',\n",
    "    'SK_CloseTimeID': 'UInt32',\n",
    "    'Status': 'str',\n",
    "    'Type': 'str',\n",
    "    'CashFlag': 'bool',\n",
    "    'SK_SecurityID': 'uint32',\n",
    "    'SK_CompanyID': 'uint32',\n",
    "    'Quantity': 'uint32',\n",
    "    'BidPrice': 'float64',\n",
    "    'SK_CustomerID': 'uint32',\n",
    "    'SK_AccountID': 'uint32',\n",
    "    'ExecutedBy': 'str',\n",
    "    'TradePrice': 'float64',\n",
    "    'Fee': 'float64',\n",
    "    'Commission': 'float64',\n",
    "    'Tax': 'float64',\n",
    "    'BatchID': 'uint8'\n",
    "}\n",
    "trade_merged = trade_merged.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'TradeID': sqlalchemy.types.Integer,\n",
    "    'SK_BrokerID': sqlalchemy.types.Integer,\n",
    "    'SK_CreateDateID': sqlalchemy.types.Integer,\n",
    "    'SK_CreateTimeID': sqlalchemy.types.Integer,\n",
    "    'SK_CloseDateID': sqlalchemy.types.Integer,\n",
    "    'SK_CloseTimeID': sqlalchemy.types.Integer,\n",
    "    'Status': sqlalchemy.types.CHAR(10),\n",
    "    'Type': sqlalchemy.types.CHAR(12),\n",
    "    'CashFlag': sqlalchemy.types.Boolean,\n",
    "    'SK_SecurityID': sqlalchemy.types.Integer,\n",
    "    'SK_CompanyID': sqlalchemy.types.Integer,\n",
    "    'Quantity': sqlalchemy.types.Integer,\n",
    "    'BidPrice': sqlalchemy.types.Numeric(8, 2),\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'SK_AccountID': sqlalchemy.types.Integer,\n",
    "    'ExecutedBy': sqlalchemy.types.CHAR(64),\n",
    "    'TradePrice': sqlalchemy.types.Numeric(8, 2),\n",
    "    'Fee': sqlalchemy.types.Numeric(10, 2),\n",
    "    'Commission': sqlalchemy.types.Numeric(10, 2),\n",
    "    'Tax': sqlalchemy.types.Numeric(10, 2),\n",
    "    'BatchID': sqlalchemy.types.SmallInteger\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "try:\n",
    "    for i in range(0, trade_merged.shape[0], 100000):\n",
    "        trade_merged.iloc[i:i+100000].to_sql('dimtrade', engine, if_exists='append', index=False, dtype=sql_dtypes)\n",
    "    session.commit()\n",
    "except:\n",
    "    session.rollback()\n",
    "    raise\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "invalid_trades = trade_merged[\n",
    "    (trade_merged[\"Commission\"].notnull())\n",
    "    & (trade_merged[\"Commission\"] > (trade_merged[\"TradePrice\"] * trade_merged[\"Quantity\"]))\n",
    "]\n",
    "print(invalid_trades.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists without using iterrows\n",
    "MessageSource = [\"DimTrade\"] * len(invalid_trades)\n",
    "MessageType = [\"Alert\"] * len(invalid_trades)\n",
    "MessageText = [\"Invalid trade commission\"] * len(invalid_trades)\n",
    "MessageData = [\n",
    "    \"T_ID = \"\n",
    "    + invalid_trades[\"TradeID\"].astype(str)\n",
    "    + \", T_COMM = \"\n",
    "    + invalid_trades[\"Commission\"].astype(str)\n",
    "]\n",
    "# Convert MessageData from a list of Series to a list of strings\n",
    "MessageData = MessageData[0].tolist()\n",
    "\n",
    "print(len(MessageSource))\n",
    "print(len(MessageData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"INSERT INTO Dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "VALUES \"\"\"\n",
    "for i in range(len(MessageSource)):\n",
    "    query += f\"\"\"('{pd.Timestamp(\"now\")}', 1, '{MessageSource[i]}', '{MessageText[i]}', '{MessageType[i]}', '{MessageData[i]}'),\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query[:-1]))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for invalid trade fees\n",
    "invalid_fee_trades = trade_merged[\n",
    "    (trade_merged[\"Fee\"].notnull())\n",
    "    & (trade_merged[\"Fee\"] > (trade_merged[\"TradePrice\"] * trade_merged[\"Quantity\"]))\n",
    "]\n",
    "\n",
    "# Create the required lists\n",
    "MessageSource = [\"DimTrade\"] * len(invalid_fee_trades)\n",
    "MessageType = [\"Alert\"] * len(invalid_fee_trades)\n",
    "MessageText = [\"Invalid trade fee\"] * len(invalid_fee_trades)\n",
    "\n",
    "# Vectorized operation for MessageData\n",
    "MessageData = (\n",
    "    \"T_ID = \"\n",
    "    + invalid_fee_trades[\"TradeID\"].astype(str)\n",
    "    + \", T_CHRG = \"\n",
    "    + invalid_fee_trades[\"Fee\"].astype(str)\n",
    ")\n",
    "MessageData = MessageData.tolist()\n",
    "\n",
    "query = \"\"\"INSERT INTO Dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData) VALUES \"\"\"\n",
    "for i in range(len(MessageSource)):\n",
    "    query += f\"\"\"('{pd.Timestamp(\"now\")}', 1, '{MessageSource[i]}', '{MessageText[i]}', '{MessageType[i]}', '{MessageData[i]}'),\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query[:-1]))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactCashBalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_CustomerID\": sqlalchemy.types.Integer,\n",
    "    \"SK_AccountID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID\": sqlalchemy.types.Integer,\n",
    "    \"Cash\": sqlalchemy.types.DECIMAL(precision=15, scale=2),\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df = pd.read_csv(\n",
    "    DATA_DIR + \"CashTransaction.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"CT_CA_ID\",\n",
    "        \"CT_DTS\",\n",
    "        \"CT_AMT\",\n",
    "        \"CT_NAME\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"CT_CA_ID\": \"uint32\",\n",
    "        \"CT_DTS\": \"str\",\n",
    "        \"CT_AMT\": \"float64\",\n",
    "        \"CT_NAME\": \"str\"\n",
    "    },\n",
    "    parse_dates=[\"CT_DTS\"],\n",
    ")\n",
    "cash_txn_df[\"CT_DTS\"] = pd.to_datetime(cash_txn_df[\"CT_DTS\"])\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SK_CustomerID and SK_AccountID are obtained from DimAccount by matching CT_CA_ID\n",
    "with AccountID, where CT_DTS is in the range given by EffectiveDate and EndDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_info = pd.read_sql(\n",
    "    \"SELECT AccountID, SK_AccountID, SK_CustomerID, EffectiveDate, EndDate FROM dimaccount\",\n",
    "    engine,\n",
    ")\n",
    "account_info[\"EffectiveDate\"] = pd.to_datetime(account_info[\"EffectiveDate\"])\n",
    "account_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df = cash_txn_df.merge(\n",
    "    account_info,\n",
    "    how=\"left\",\n",
    "    left_on=\"CT_CA_ID\",\n",
    "    right_on=\"AccountID\",\n",
    ")\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df = cash_txn_df[\n",
    "    (cash_txn_df[\"CT_DTS\"] >= cash_txn_df[\"EffectiveDate\"])\n",
    "    & (cash_txn_df[\"CT_DTS\"].dt.date < cash_txn_df[\"EndDate\"])\n",
    "]\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SK_DateID is obtained from DimDate by matching just the date portion of CT_DTS with\n",
    "DateValue to return the SK_DateID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info = pd.read_sql(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info[\"DateValue\"] = pd.to_datetime(date_info[\"DateValue\"])\n",
    "date_info.info()\n",
    "date_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df['SK_DateID'] = cash_txn_df['CT_DTS'].dt.date.map(date_info.set_index('DateValue')['SK_DateID'])\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cash is calculated as the sum of the prior Cash amount for this account plus the sum of all\n",
    "CT_AMT values from all transactions in this account on this day. If there is no previous\n",
    "FactCashBalances record for the associated account, zero is used. Remember that the net effect of all cash transactions for a given account on a given day is totaled, and only a single record is generated per account that had changes per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by account ID and transaction date\n",
    "cash_txn_df.sort_values(by=['SK_AccountID', 'CT_DTS'], inplace=True)\n",
    "\n",
    "# Create a new column to store the prior cash amount\n",
    "cash_txn_df['PriorCash'] = cash_txn_df.groupby('SK_AccountID')['CT_AMT'].cumsum() - cash_txn_df['CT_AMT']\n",
    "cash_txn_df['PriorCash'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the cash balance\n",
    "cash_txn_df['Cash'] = cash_txn_df['PriorCash'] + cash_txn_df['CT_AMT']\n",
    "\n",
    "# Keep only the last record for each account on each day\n",
    "cash_txn_df = cash_txn_df.groupby(['AccountID', 'SK_DateID']).last().reset_index()\n",
    "\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    \"SK_CustomerID\",\n",
    "    \"SK_AccountID\",\n",
    "    \"SK_DateID\",\n",
    "    \"Cash\",\n",
    "]\n",
    "cash_txn_df = cash_txn_df[keep_cols]\n",
    "cash_txn_df['BatchID'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactHoldings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL queries\n",
    "sql_commands = [\n",
    "    \"DROP TABLE IF EXISTS TempHoldingHistory\",\n",
    "    \"\"\"\n",
    "    CREATE TEMPORARY TABLE TempHoldingHistory (\n",
    "        HH_H_T_ID INT UNSIGNED NOT NULL,\n",
    "        HH_T_ID INT UNSIGNED NOT NULL,\n",
    "        HH_BEFORE_QTY INT NOT NULL,\n",
    "        HH_AFTER_QTY INT NOT NULL\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    LOAD DATA LOCAL INFILE 'E:\\\\\\\\Documents\\\\\\\\BDMA\\\\\\\\ULB\\\\\\\\Data Warehouses\\\\\\\\tpc-di\\\\\\\\TPC-DI\\\\\\\\data\\\\\\\\sf5\\\\\\\\Batch1\\\\\\\\HoldingHistory.txt'\n",
    "    INTO TABLE TempHoldingHistory\n",
    "    FIELDS TERMINATED BY '|'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    (HH_H_T_ID, HH_T_ID, HH_BEFORE_QTY, HH_AFTER_QTY)\n",
    "    \"\"\",\n",
    "    \"DROP TABLE IF EXISTS FactHoldings\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE FactHoldings (\n",
    "        TradeID INT UNSIGNED NOT NULL,\n",
    "        CurrentTradeID INT UNSIGNED NOT NULL,\n",
    "        SK_CustomerID INT UNSIGNED NOT NULL,\n",
    "        SK_AccountID INT UNSIGNED NOT NULL,\n",
    "        SK_SecurityID INT UNSIGNED NOT NULL,\n",
    "        SK_CompanyID INT UNSIGNED NOT NULL,\n",
    "        SK_DateID INT UNSIGNED NOT NULL,\n",
    "        SK_TimeID INT UNSIGNED NOT NULL,\n",
    "        CurrentPrice DECIMAL(8, 2) NOT NULL CHECK (CurrentPrice > 0),\n",
    "        CurrentHolding INT NOT NULL,\n",
    "        BatchID SMALLINT UNSIGNED NOT NULL\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    INSERT INTO FactHoldings (TradeID, CurrentTradeID, SK_CustomerID, SK_AccountID, SK_SecurityID, SK_CompanyID, SK_DateID, SK_TimeID, CurrentPrice, CurrentHolding, BatchID)\n",
    "    SELECT \n",
    "        thh.HH_H_T_ID AS TradeID,\n",
    "        thh.HH_T_ID AS CurrentTradeID,\n",
    "        dt.SK_CustomerID,\n",
    "        dt.SK_AccountID,\n",
    "        dt.SK_SecurityID,\n",
    "        dt.SK_CompanyID,\n",
    "        dt.SK_CloseDateID AS SK_DateID,\n",
    "        dt.SK_CloseTimeID AS SK_TimeID,\n",
    "        dt.TradePrice AS CurrentPrice,\n",
    "        thh.HH_AFTER_QTY AS CurrentHolding,\n",
    "        1 AS BatchID\n",
    "    FROM \n",
    "        TempHoldingHistory thh\n",
    "    JOIN \n",
    "        DimTrade dt ON thh.HH_T_ID = dt.TradeID\n",
    "    \"\"\",\n",
    "    \"DROP TABLE IF EXISTS TempHoldingHistory\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the queries\n",
    "with engine.connect() as connection:\n",
    "    # connection.execute(text(\"SET GLOBAL local_infile = 1;\"))\n",
    "    for sql in sql_commands:\n",
    "        connection.execute(text(sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactMarketHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.read_csv(\n",
    "    DATA_DIR + \"DailyMarket.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"DM_DATE\",\n",
    "        \"DM_S_SYMB\",\n",
    "        \"DM_CLOSE\",\n",
    "        \"DM_HIGH\",\n",
    "        \"DM_LOW\",\n",
    "        \"DM_VOL\",\n",
    "    ],\n",
    "    dtype={\n",
    "        \"DM_DATE\": \"str\",\n",
    "        \"DM_S_SYMB\": \"str\",\n",
    "        \"DM_CLOSE\": \"float32\",\n",
    "        \"DM_HIGH\": \"float32\",\n",
    "        \"DM_LOW\": \"float32\",\n",
    "        \"DM_VOL\": \"int64\",\n",
    "    },\n",
    "    parse_dates=[\"DM_DATE\"],\n",
    ")\n",
    "dailymarket_df[\"DM_DATE\"] = pd.to_datetime(dailymarket_df[\"DM_DATE\"])\n",
    "dailymarket_df.info()\n",
    "dailymarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClosePrice, DayHigh, DayLow, and Volume are copied from DM_CLOSE, DM_HIGH,\n",
    "# DM_LOW, and DM_VOL respectively.\n",
    "dailymarket_df[\"ClosePrice\"] = dailymarket_df[\"DM_CLOSE\"]\n",
    "dailymarket_df[\"DayHigh\"] = dailymarket_df[\"DM_HIGH\"]\n",
    "dailymarket_df[\"DayLow\"] = dailymarket_df[\"DM_LOW\"]\n",
    "dailymarket_df[\"Volume\"] = dailymarket_df[\"DM_VOL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_info = pd.read_sql(\"SELECT Symbol AS DM_S_SYMB, SK_SecurityID, SK_CompanyID, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])\n",
    "security_info.info()\n",
    "security_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.merge(\n",
    "    dailymarket_df,\n",
    "    security_info,\n",
    "    how=\"left\",\n",
    "    on=\"DM_S_SYMB\",\n",
    ")\n",
    "dailymarket_df = dailymarket_df[\n",
    "    (dailymarket_df[\"DM_DATE\"] >= dailymarket_df[\"EffectiveDate\"])\n",
    "    & (dailymarket_df[\"DM_DATE\"].dt.date < dailymarket_df[\"EndDate\"])\n",
    "]\n",
    "# drop temp columns\n",
    "dailymarket_df.drop(columns=[\"EffectiveDate\", \"EndDate\"], inplace=True)\n",
    "dailymarket_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SK_DateID is obtained from DimDate by matching DM_DATE with DateValue to return the\n",
    "SK_DateID. The match is guaranteed to succeed because DimDate has been populated\n",
    "with date information for all dates relevant to the benchmark.\"\"\"\n",
    "date_info = pd.read_sql(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info[\"DateValue\"] = pd.to_datetime(date_info[\"DateValue\"])\n",
    "date_info = date_info.set_index(\"DateValue\")[\"SK_DateID\"].to_dict()\n",
    "dailymarket_df[\"SK_DateID\"] = dailymarket_df[\"DM_DATE\"].dt.date.map(date_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sort the DataFrame\n",
    "dailymarket_df.sort_values(by='DM_DATE', inplace=True)\n",
    "\n",
    "# Step 3 & 4: Group by 'DM_S_SYMB' and apply rolling max\n",
    "rolling_max = dailymarket_df.groupby('DM_S_SYMB').rolling('365D', on='DM_DATE')['DM_HIGH'].max()\n",
    "\n",
    "# Reset index to make merging easier\n",
    "rolling_max = rolling_max.reset_index()\n",
    "\n",
    "# Step 5: Merge with the original DataFrame\n",
    "dailymarket_df = dailymarket_df.merge(rolling_max, on=['DM_S_SYMB', 'DM_DATE'], suffixes=('', '_52WeekHigh'))\n",
    "\n",
    "# Rename the column for clarity\n",
    "dailymarket_df.rename(columns={'DM_HIGH_52WeekHigh': 'FiftyTwoWeekHigh'}, inplace=True)\n",
    "dailymarket_df[['DM_DATE', 'DM_S_SYMB', 'DM_HIGH', 'FiftyTwoWeekHigh']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly wrote this myself but dont ask me to explain...........\n",
    "rolling_rank = (\n",
    "    dailymarket_df.groupby(\"DM_S_SYMB\")\n",
    "    .rolling(\"365D\", on=\"DM_DATE\")[\"DM_HIGH\"]\n",
    "    .rank(method=\"average\", ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"DM_HIGH\": \"Rank\"})\n",
    ")\n",
    "rolling_rank[\"Rank\"] = rolling_rank[\"Rank\"].astype(\"uint32\")\n",
    "# Apply the mask to select DM_DATE only for those rows, then forward fill\n",
    "mask = rolling_rank['Rank'] == 1\n",
    "rolling_rank['SK_FiftyTwoWeekHighDate'] = rolling_rank['DM_DATE'].where(mask).ffill()\n",
    "rolling_rank['SK_FiftyTwoWeekHighDate'] = rolling_rank['SK_FiftyTwoWeekHighDate'].dt.date.map(date_info)\n",
    "rolling_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.concat([dailymarket_df, rolling_rank['SK_FiftyTwoWeekHighDate']], axis=1)\n",
    "dailymarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df.sort_values(by='DM_DATE', inplace=True)\n",
    "# Step 3 & 4: Group by 'DM_S_SYMB' and apply rolling min\n",
    "rolling_min = dailymarket_df.groupby('DM_S_SYMB').rolling('365D', on='DM_DATE')['DM_LOW'].min()\n",
    "# Reset index to make merging easier\n",
    "rolling_min = rolling_min.reset_index()\n",
    "# Step 5: Merge with the original DataFrame\n",
    "dailymarket_df = dailymarket_df.merge(rolling_min, on=['DM_S_SYMB', 'DM_DATE'], suffixes=('', '_52WeekLow'))\n",
    "# Rename the column for clarity\n",
    "dailymarket_df.rename(columns={'DM_LOW_52WeekLow': 'FiftyTwoWeekLow'}, inplace=True)\n",
    "dailymarket_df[['DM_DATE', 'DM_S_SYMB', 'DM_LOW', 'FiftyTwoWeekLow']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same...\n",
    "rolling_rank = (\n",
    "    dailymarket_df.groupby(\"DM_S_SYMB\")\n",
    "    .rolling(\"365D\", on=\"DM_DATE\")[\"DM_LOW\"]\n",
    "    .rank(method=\"average\", ascending=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"DM_LOW\": \"Rank\"})\n",
    ")\n",
    "rolling_rank[\"Rank\"] = rolling_rank[\"Rank\"].astype(\"uint32\")\n",
    "# Apply the mask to select DM_DATE only for those rows, then forward fill\n",
    "mask = rolling_rank['Rank'] == 1\n",
    "rolling_rank['SK_FiftyTwoWeekLowDate'] = rolling_rank['DM_DATE'].where(mask).ffill()\n",
    "rolling_rank['SK_FiftyTwoWeekLowDate'] = rolling_rank['SK_FiftyTwoWeekLowDate'].dt.date.map(date_info)\n",
    "rolling_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.concat([dailymarket_df, rolling_rank['SK_FiftyTwoWeekLowDate']], axis=1)\n",
    "dailymarket_df.info()\n",
    "dailymarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df['SK_SecurityID'] = dailymarket_df['SK_SecurityID'].astype('uint32')\n",
    "dailymarket_df['SK_CompanyID'] = dailymarket_df['SK_CompanyID'].astype('uint32')\n",
    "dailymarket_df['SK_DateID'] = dailymarket_df['SK_DateID'].astype('uint32')\n",
    "dailymarket_df['FiftyTwoWeekHigh'] = dailymarket_df['FiftyTwoWeekHigh'].astype('float32')\n",
    "dailymarket_df['SK_FiftyTwoWeekHighDate'] = dailymarket_df['SK_FiftyTwoWeekHighDate'].astype('uint32')\n",
    "dailymarket_df['FiftyTwoWeekLow'] = dailymarket_df['FiftyTwoWeekLow'].astype('float32')\n",
    "dailymarket_df['SK_FiftyTwoWeekLowDate'] = dailymarket_df['SK_FiftyTwoWeekLowDate'].astype('uint32')\n",
    "dailymarket_df['DM_S_SYMB'] = dailymarket_df['DM_S_SYMB'].astype('category')\n",
    "\n",
    "dailymarket_df.drop(columns=['DM_HIGH', 'DM_LOW', 'DM_VOL', 'DM_CLOSE'], inplace=True)\n",
    "dailymarket_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_info = pd.read_sql(\"SELECT Symbol, Dividend, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = dailymarket_df.merge(\n",
    "    security_info,\n",
    "    how=\"left\",\n",
    "    left_on=\"DM_S_SYMB\",\n",
    "    right_on=\"Symbol\",\n",
    ")\n",
    "dailymarket_df = dailymarket_df[\n",
    "    (dailymarket_df[\"DM_DATE\"] >= dailymarket_df[\"EffectiveDate\"])\n",
    "    & (dailymarket_df[\"DM_DATE\"].dt.date < dailymarket_df[\"EndDate\"])\n",
    "]\n",
    "dailymarket_df.drop(columns=[\"Symbol\", \"EffectiveDate\", \"EndDate\"], inplace=True)\n",
    "dailymarket_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df['Yield'] = dailymarket_df['Dividend'] / dailymarket_df['ClosePrice'] * 100\n",
    "dailymarket_df.drop(columns=[\"Dividend\"], inplace=True)\n",
    "dailymarket_df['BatchID'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"SK_CompanyID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID\": sqlalchemy.types.Integer,\n",
    "    # PERatio to be done in MySQL\n",
    "    # \"PERatio\": sqlalchemy.types.DECIMAL(precision=10, scale=2),\n",
    "    \"Yield\": sqlalchemy.types.DECIMAL(precision=5, scale=2),\n",
    "    \"FiftyTwoWeekHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekHighDate\": sqlalchemy.types.Integer,\n",
    "    \"FiftyTwoWeekLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekLowDate\": sqlalchemy.types.Integer,\n",
    "    \"ClosePrice\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"Volume\": sqlalchemy.types.BigInteger,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger,\n",
    "    \"DM_DATE\": sqlalchemy.types.Date,\n",
    "    \"DM_S_SYMB\": sqlalchemy.types.CHAR(16),\n",
    "}\n",
    "len(sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(0, dailymarket_df.shape[0], 100000):\n",
    "    dailymarket_df.iloc[i:i+100000].to_sql('tempfactmarketprice', engine, if_exists='append', index=False,\n",
    "                                           dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_commands = [\n",
    "    \"CREATE INDEX idx_sk_companyid ON tempfactmarketprice(SK_CompanyID);\",\n",
    "    \"ALTER TABLE factmarkethistory ADD COLUMN DM_S_SYMB TEXT;\",\n",
    "    \"\"\"INSERT INTO factmarkethistory\n",
    "        SELECT SK_SecurityID, fmp.SK_CompanyID, SK_DateID, fmp.ClosePrice / T.Sum_EPS AS PERatio, Yield, FiftyTwoWeekHigh,\n",
    "        SK_FiftyTwoWeekHighDate, FiftyTwoWeekLow, SK_FiftyTwoWeekLowDate, ClosePrice, DayHigh, DayLow, Volume, BatchID, DM_S_SYMB\n",
    "        FROM tempfactmarketprice fmp\n",
    "        JOIN (SELECT \n",
    "            c.CompanyID, \n",
    "            c.SK_CompanyID AS SKCID, \n",
    "            f.FI_QTR_START_DATE,\n",
    "            SUM(f.FI_BASIC_EPS) OVER (\n",
    "                PARTITION BY c.CompanyID \n",
    "                ORDER BY f.FI_QTR_START_DATE \n",
    "                ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "            ) AS Sum_EPS\n",
    "        FROM financial f RIGHT JOIN dimCompany c ON f.SK_CompanyID = c.SK_CompanyID\n",
    "        ORDER BY c.CompanyID, f.FI_QTR_START_DATE) T\n",
    "        ON T.SKCID = fmp.SK_CompanyID\n",
    "        AND T.FI_QTR_START_DATE < fmp.DM_DATE \n",
    "        AND T.FI_QTR_START_DATE >= DATE_SUB(fmp.DM_DATE, INTERVAL 4 MONTH);\"\"\",\n",
    "    \"\"\"INSERT INTO dimessages\n",
    "        SELECT NOW() AS MessageDateAndTime, 1 AS BATCHID, 'FactMarketHistory' AS MessageSource, 'No earnings for company' AS MessageText,\n",
    "        'Alert' AS MessageType, CONCAT('DM_S_SYMB = ', DM_S_SYMB)\n",
    "        FROM factmarkethistory \n",
    "        WHERE PERatio IS NULL;\"\"\",\n",
    "    \"ALTER TABLE factmarkethistory DROP COLUMN DM_S_SYMB;\",\n",
    "    \"DROP TABLE tempfactmarketprice;\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the queries\n",
    "with engine.connect() as connection:\n",
    "    for sql in sql_commands:\n",
    "        connection.execute(text(sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactWatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500311 entries, 0 to 1500310\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count    Dtype         \n",
      "---  ------    --------------    -----         \n",
      " 0   W_C_ID    1500311 non-null  uint32        \n",
      " 1   W_S_SYMB  1500311 non-null  object        \n",
      " 2   W_DTS     1500311 non-null  datetime64[ns]\n",
      " 3   W_ACTION  1500311 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(2), uint32(1)\n",
      "memory usage: 40.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_C_ID</th>\n",
       "      <th>W_S_SYMB</th>\n",
       "      <th>W_DTS</th>\n",
       "      <th>W_ACTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>AAAAAAAAAAAAEGI</td>\n",
       "      <td>2012-07-07 00:02:15</td>\n",
       "      <td>ACTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>AAAAAAAAAAAADXJ</td>\n",
       "      <td>2012-07-07 00:02:39</td>\n",
       "      <td>ACTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>AAAAAAAAAAAADOK</td>\n",
       "      <td>2012-07-07 00:05:30</td>\n",
       "      <td>ACTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>AAAAAAAAAAAADFL</td>\n",
       "      <td>2012-07-07 00:06:37</td>\n",
       "      <td>ACTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>AAAAAAAAAAAACWM</td>\n",
       "      <td>2012-07-07 00:06:57</td>\n",
       "      <td>ACTV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   W_C_ID         W_S_SYMB               W_DTS W_ACTION\n",
       "0      19  AAAAAAAAAAAAEGI 2012-07-07 00:02:15     ACTV\n",
       "1      58  AAAAAAAAAAAADXJ 2012-07-07 00:02:39     ACTV\n",
       "2      37  AAAAAAAAAAAADOK 2012-07-07 00:05:30     ACTV\n",
       "3      16  AAAAAAAAAAAADFL 2012-07-07 00:06:37     ACTV\n",
       "4      55  AAAAAAAAAAAACWM 2012-07-07 00:06:57     ACTV"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_DIR + \"WatchHistory.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"W_C_ID\",\n",
    "        \"W_S_SYMB\",\n",
    "        \"W_DTS\",\n",
    "        \"W_ACTION\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"W_C_ID\": \"uint32\",\n",
    "        \"W_S_SYMB\": \"str\",\n",
    "        \"W_DTS\": \"str\",\n",
    "        \"W_ACTION\": \"str\"\n",
    "    },\n",
    "    parse_dates=[\"W_DTS\"]\n",
    ")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_info = pd.read_sql_query(\"SELECT CustomerID, SK_CustomerID, EffectiveDate, EndDate FROM dimcustomer\", engine)\n",
    "customer_info['EffectiveDate'] = pd.to_datetime(customer_info['EffectiveDate'])\n",
    "security_info = pd.read_sql_query(\"SELECT Symbol, SK_SecurityID, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])\n",
    "date_info = pd.read_sql_query(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info['DateValue'] = pd.to_datetime(date_info['DateValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1500311 entries, 0 to 1694090\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count    Dtype         \n",
      "---  ------                --------------    -----         \n",
      " 0   W_C_ID                1500311 non-null  uint32        \n",
      " 1   W_S_SYMB              1500311 non-null  object        \n",
      " 2   W_DTS                 1500311 non-null  datetime64[ns]\n",
      " 3   W_ACTION              1500311 non-null  object        \n",
      " 4   SK_CustomerID         1500311 non-null  int64         \n",
      " 5   SK_SecurityID         1500311 non-null  int64         \n",
      " 6   SK_DateID_DatePlaced  1500311 non-null  int64         \n",
      " 7   BatchID               1500311 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(4), object(2), uint32(1)\n",
      "memory usage: 97.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_C_ID</th>\n",
       "      <th>W_S_SYMB</th>\n",
       "      <th>W_DTS</th>\n",
       "      <th>W_ACTION</th>\n",
       "      <th>SK_CustomerID</th>\n",
       "      <th>SK_SecurityID</th>\n",
       "      <th>SK_DateID_DatePlaced</th>\n",
       "      <th>BatchID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>AAAAAAAAAAAAEGI</td>\n",
       "      <td>2012-07-07 00:02:15</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>9960</td>\n",
       "      <td>3191</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>AAAAAAAAAAAADXJ</td>\n",
       "      <td>2012-07-07 00:02:39</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>8737</td>\n",
       "      <td>2923</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>AAAAAAAAAAAADOK</td>\n",
       "      <td>2012-07-07 00:05:30</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>38</td>\n",
       "      <td>2668</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>AAAAAAAAAAAADFL</td>\n",
       "      <td>2012-07-07 00:06:37</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>10179</td>\n",
       "      <td>2393</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>AAAAAAAAAAAACWM</td>\n",
       "      <td>2012-07-07 00:06:57</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>9971</td>\n",
       "      <td>2152</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   W_C_ID         W_S_SYMB               W_DTS W_ACTION  SK_CustomerID  \\\n",
       "0      19  AAAAAAAAAAAAEGI 2012-07-07 00:02:15     ACTV           9960   \n",
       "1      58  AAAAAAAAAAAADXJ 2012-07-07 00:02:39     ACTV           8737   \n",
       "2      37  AAAAAAAAAAAADOK 2012-07-07 00:05:30     ACTV             38   \n",
       "3      16  AAAAAAAAAAAADFL 2012-07-07 00:06:37     ACTV          10179   \n",
       "4      55  AAAAAAAAAAAACWM 2012-07-07 00:06:57     ACTV           9971   \n",
       "\n",
       "   SK_SecurityID  SK_DateID_DatePlaced  BatchID  \n",
       "0           3191              20120707        1  \n",
       "1           2923              20120707        1  \n",
       "2           2668              20120707        1  \n",
       "3           2393              20120707        1  \n",
       "4           2152              20120707        1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get SK_CustomerID\n",
    "df = df.merge(customer_info, how=\"left\", left_on=\"W_C_ID\", right_on=\"CustomerID\")\n",
    "# filter based on date\n",
    "df = df[\n",
    "    (df[\"W_DTS\"] >= df[\"EffectiveDate\"])\n",
    "    & (df[\"W_DTS\"].dt.date < df[\"EndDate\"])\n",
    "]\n",
    "# drop cols\n",
    "df.drop(columns=[\"CustomerID\", \"EffectiveDate\", \"EndDate\"], inplace=True)\n",
    "# get SK_SecurityID\n",
    "df = df.merge(security_info, how=\"left\", left_on=\"W_S_SYMB\", right_on=\"Symbol\")\n",
    "# filter based on date\n",
    "df = df[\n",
    "    (df[\"W_DTS\"] >= df[\"EffectiveDate\"])\n",
    "    & (df[\"W_DTS\"].dt.date < df[\"EndDate\"])\n",
    "]\n",
    "# drop cols\n",
    "df.drop(columns=[\"Symbol\", \"EffectiveDate\", \"EndDate\"], inplace=True)\n",
    "# SK_DateID_DatePlaced - set based on W_DTS.\n",
    "df['SK_DateID_DatePlaced'] = df['W_DTS'].dt.date.map(date_info.set_index('DateValue')['SK_DateID'])\n",
    "# BatchID - set to 1.\n",
    "df['BatchID'] = 1\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1500311 entries, 0 to 1694090\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   W_C_ID                 1500311 non-null  uint32        \n",
      " 1   W_S_SYMB               1500311 non-null  object        \n",
      " 2   W_DTS                  1500311 non-null  datetime64[ns]\n",
      " 3   W_ACTION               1500311 non-null  object        \n",
      " 4   SK_CustomerID          1500311 non-null  int64         \n",
      " 5   SK_SecurityID          1500311 non-null  int64         \n",
      " 6   SK_DateID_DatePlaced   1500311 non-null  int64         \n",
      " 7   BatchID                1500311 non-null  int64         \n",
      " 8   SK_DateID_DateRemoved  299280 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(2), uint32(1)\n",
      "memory usage: 108.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_C_ID</th>\n",
       "      <th>W_S_SYMB</th>\n",
       "      <th>W_DTS</th>\n",
       "      <th>W_ACTION</th>\n",
       "      <th>SK_CustomerID</th>\n",
       "      <th>SK_SecurityID</th>\n",
       "      <th>SK_DateID_DatePlaced</th>\n",
       "      <th>BatchID</th>\n",
       "      <th>SK_DateID_DateRemoved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>AAAAAAAAAAAAEGI</td>\n",
       "      <td>2012-07-07 00:02:15</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>9960</td>\n",
       "      <td>3191</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>AAAAAAAAAAAADXJ</td>\n",
       "      <td>2012-07-07 00:02:39</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>8737</td>\n",
       "      <td>2923</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>AAAAAAAAAAAADOK</td>\n",
       "      <td>2012-07-07 00:05:30</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>38</td>\n",
       "      <td>2668</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>AAAAAAAAAAAADFL</td>\n",
       "      <td>2012-07-07 00:06:37</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>10179</td>\n",
       "      <td>2393</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>AAAAAAAAAAAACWM</td>\n",
       "      <td>2012-07-07 00:06:57</td>\n",
       "      <td>ACTV</td>\n",
       "      <td>9971</td>\n",
       "      <td>2152</td>\n",
       "      <td>20120707</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   W_C_ID         W_S_SYMB               W_DTS W_ACTION  SK_CustomerID  \\\n",
       "0      19  AAAAAAAAAAAAEGI 2012-07-07 00:02:15     ACTV           9960   \n",
       "1      58  AAAAAAAAAAAADXJ 2012-07-07 00:02:39     ACTV           8737   \n",
       "2      37  AAAAAAAAAAAADOK 2012-07-07 00:05:30     ACTV             38   \n",
       "3      16  AAAAAAAAAAAADFL 2012-07-07 00:06:37     ACTV          10179   \n",
       "4      55  AAAAAAAAAAAACWM 2012-07-07 00:06:57     ACTV           9971   \n",
       "\n",
       "   SK_SecurityID  SK_DateID_DatePlaced  BatchID  SK_DateID_DateRemoved  \n",
       "0           3191              20120707        1                    NaN  \n",
       "1           2923              20120707        1                    NaN  \n",
       "2           2668              20120707        1                    NaN  \n",
       "3           2393              20120707        1                    NaN  \n",
       "4           2152              20120707        1                    NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mask for rows where W_ACTION is 'CNCL'\n",
    "mask_cncl = df['W_ACTION'] == 'CNCL'\n",
    "df.loc[mask_cncl, 'SK_DateID_DateRemoved'] = df.loc[mask_cncl, 'W_DTS'].dt.date.map(date_info.set_index('DateValue')['SK_DateID'])\n",
    "df.loc[~mask_cncl, 'SK_DateID_DateRemoved'] = None\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1500311 entries, 0 to 1694090\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   SK_CustomerID          1500311 non-null  int64  \n",
      " 1   SK_SecurityID          1500311 non-null  int64  \n",
      " 2   SK_DateID_DatePlaced   1500311 non-null  int64  \n",
      " 3   SK_DateID_DateRemoved  299280 non-null   float64\n",
      " 4   BatchID                1500311 non-null  int64  \n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 68.7 MB\n"
     ]
    }
   ],
   "source": [
    "keep_cols = [\"SK_CustomerID\", \"SK_SecurityID\", \"SK_DateID_DatePlaced\", \"SK_DateID_DateRemoved\", \"BatchID\"]\n",
    "df = df[keep_cols]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_CustomerID\": sqlalchemy.types.Integer,\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID_DatePlaced\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID_DateRemoved\": sqlalchemy.types.Integer,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger\n",
    "}\n",
    "for i in range(0, df.shape[0], 100000):\n",
    "    df.iloc[i:i+100000].to_sql('factwatches', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"E:\\Documents\\BDMA\\ULB\\Data Warehouses\\tpc-di\\TPC-DI\\validation\\tpcdi_validation.sql\"\n",
    "# Read the SQL file\n",
    "with open(filepath, 'r') as file:\n",
    "    sql_file = file.read()\n",
    "\n",
    "# Execute the SQL commands\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(sql_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpcdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
