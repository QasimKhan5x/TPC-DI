{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPC-DI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection details\n",
    "host = \"localhost\"\n",
    "user = \"root\"\n",
    "password = \"password\"\n",
    "sf = 5\n",
    "database = f\"tpcdi_sf{sf}\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{user}:{password}@{host}/{database}?allow_local_infile=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_ID = 1\n",
    "DATA_DIR = f\"..\\\\data\\\\sf5\\\\Batch{BATCH_ID}\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + \"BatchDate.txt\", \"r\") as f:\n",
    "    BATCH_DATE = f.read().strip()\n",
    "BATCH_DATE = pd.to_datetime(BATCH_DATE, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.read_csv(\n",
    "    DATA_DIR + \"Date.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"SK_DateID\", \"DateValue\", \"DateDesc\", \"CalendarYearID\", \"CalendarYearDesc\", \n",
    "        \"CalendarQtrID\", \"CalendarQtrDesc\", \"CalendarMonthID\", \"CalendarMonthDesc\", \n",
    "        \"CalendarWeekID\", \"CalendarWeekDesc\", \"DayOfWeekNum\", \"DayOfWeekDesc\", \n",
    "        \"FiscalYearID\", \"FiscalYearDesc\", \"FiscalQtrID\", \"FiscalQtrDesc\", \"HolidayFlag\"\n",
    "    ],\n",
    "    parse_dates=[\"DateValue\"],\n",
    "    dtype={\n",
    "        \"SK_DateID\": \"uint32\", \"DateDesc\": \"str\", \"CalendarYearID\": \"uint16\", \"CalendarYearDesc\": \"str\",\n",
    "        \"CalendarQtrID\": \"uint16\", \"CalendarQtrDesc\": \"str\", \"CalendarMonthID\": \"uint32\", \"CalendarMonthDesc\": \"str\",\n",
    "        \"CalendarWeekID\": \"uint32\", \"CalendarWeekDesc\": \"str\", \"DayOfWeekNum\": \"uint8\", \"DayOfWeekDesc\": \"str\",\n",
    "        \"FiscalYearID\": \"uint16\", \"FiscalYearDesc\": \"str\", \"FiscalQtrID\": \"uint16\", \"FiscalQtrDesc\": \"str\",\n",
    "        \"HolidayFlag\": \"bool\"\n",
    "    }\n",
    ")\n",
    "date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"SK_DateID\": sqlalchemy.types.BigInteger,\n",
    "    \"DateValue\": sqlalchemy.types.Date,\n",
    "    \"DateDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarYearID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarYearDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarQtrID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarQtrDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarMonthID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarMonthDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"CalendarWeekID\": sqlalchemy.types.Integer,\n",
    "    \"CalendarWeekDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"DayOfWeekNum\": sqlalchemy.types.SmallInteger,\n",
    "    \"DayOfWeekDesc\": sqlalchemy.types.CHAR(length=10),\n",
    "    \"FiscalYearID\": sqlalchemy.types.Integer,\n",
    "    \"FiscalYearDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"FiscalQtrID\": sqlalchemy.types.Integer,\n",
    "    \"FiscalQtrDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"HolidayFlag\": sqlalchemy.types.Boolean\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimDate (\n",
    "    SK_DateID INT UNSIGNED NOT NULL,\n",
    "    DateValue DATE NOT NULL,\n",
    "    DateDesc CHAR(20) NOT NULL,\n",
    "    CalendarYearID SMALLINT UNSIGNED NOT NULL,\n",
    "    CalendarYearDesc CHAR(20) NOT NULL,\n",
    "    CalendarQtrID SMALLINT UNSIGNED NOT NULL,\n",
    "    CalendarQtrDesc CHAR(20) NOT NULL,\n",
    "    CalendarMonthID MEDIUMINT UNSIGNED NOT NULL,\n",
    "    CalendarMonthDesc CHAR(20) NOT NULL,\n",
    "    CalendarWeekID MEDIUMINT UNSIGNED NOT NULL,\n",
    "    CalendarWeekDesc CHAR(20) NOT NULL,\n",
    "    DayOfWeekNum TINYINT UNSIGNED NOT NULL,\n",
    "    DayOfWeekDesc CHAR(10) NOT NULL,\n",
    "    FiscalYearID SMALLINT UNSIGNED NOT NULL,\n",
    "    FiscalYearDesc CHAR(20) NOT NULL,\n",
    "    FiscalQtrID SMALLINT UNSIGNED NOT NULL,\n",
    "    FiscalQtrDesc CHAR(20) NOT NULL,\n",
    "    HolidayFlag BOOLEAN,\n",
    "    PRIMARY KEY (SK_DateID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df.to_sql(name='dimdate', con=engine, if_exists='append', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "file_path = DATA_DIR + \"Time.txt\"\n",
    "dim_time_df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"SK_TimeID\", \"TimeValue\", \"HourID\", \"HourDesc\", \n",
    "        \"MinuteID\", \"MinuteDesc\", \"SecondID\", \"SecondDesc\",\n",
    "        \"MarketHoursFlag\", \"OfficeHoursFlag\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"SK_TimeID\": \"uint32\", \"HourID\": \"uint8\", \n",
    "        \"HourDesc\": \"str\", \"MinuteID\": \"uint8\", \"MinuteDesc\": \"str\", \n",
    "        \"SecondID\": \"uint8\", \"SecondDesc\": \"str\", \"MarketHoursFlag\": \"bool\", \n",
    "        \"OfficeHoursFlag\": \"bool\"\n",
    "    },\n",
    "    parse_dates=[\"TimeValue\"],\n",
    "    date_format=\"%H:%M:%S\"\n",
    ")\n",
    "dim_time_df['TimeValue'] = dim_time_df['TimeValue'].dt.time\n",
    "\n",
    "dim_time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"SK_TimeID\": sqlalchemy.types.BigInteger,\n",
    "    \"TimeValue\": sqlalchemy.types.Time,\n",
    "    \"HourID\": sqlalchemy.types.SmallInteger,\n",
    "    \"HourDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"MinuteID\": sqlalchemy.types.SmallInteger,\n",
    "    \"MinuteDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"SecondID\": sqlalchemy.types.SmallInteger,\n",
    "    \"SecondDesc\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"MarketHoursFlag\": sqlalchemy.types.Boolean,\n",
    "    \"OfficeHoursFlag\": sqlalchemy.types.Boolean\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimTime (\n",
    "    SK_TimeID INT UNSIGNED NOT NULL,\n",
    "    TimeValue TIME(3) NOT NULL,\n",
    "    HourID TINYINT UNSIGNED NOT NULL,\n",
    "    HourDesc CHAR(20) NOT NULL,\n",
    "    MinuteID TINYINT UNSIGNED NOT NULL,\n",
    "    MinuteDesc CHAR(20) NOT NULL,\n",
    "    SecondID TINYINT UNSIGNED NOT NULL,\n",
    "    SecondDesc CHAR(20) NOT NULL,\n",
    "    MarketHoursFlag BOOLEAN,\n",
    "    OfficeHoursFlag BOOLEAN,\n",
    "    PRIMARY KEY (SK_TimeID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))\n",
    "dim_time_df.to_sql(name='dimtime', con=engine, if_exists='append', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "file_path = r\"..\\data\\sf5\\Batch1\\Industry.txt\"\n",
    "industry_df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"IN_ID\", \"IN_NAME\", \"IN_SC_ID\"],\n",
    "    dtype={\n",
    "        \"IN_ID\": \"str\",\n",
    "        \"IN_NAME\": \"str\",\n",
    "        \"IN_SC_ID\": \"str\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"IN_ID\": sqlalchemy.types.CHAR(length=2),\n",
    "    \"IN_NAME\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"IN_SC_ID\": sqlalchemy.types.CHAR(length=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE Industry (\n",
    "    IN_ID CHAR(2) NOT NULL,\n",
    "    IN_NAME CHAR(50) NOT NULL,\n",
    "    IN_SC_ID CHAR(4) NOT NULL,\n",
    "    PRIMARY KEY (IN_ID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_df.to_sql(name=\"industry\", con=engine, if_exists='append', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### StatusType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read StatusType data\n",
    "filepath = r\"..\\data\\sf5\\Batch1\\StatusType.txt\"\n",
    "status_type_df = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ST_ID\", \"ST_NAME\"],\n",
    "    dtype={\"ST_ID\": \"str\", \"ST_NAME\": \"str\"}\n",
    ")\n",
    "status_type_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"ST_ID\": sqlalchemy.types.CHAR(length=4),\n",
    "    \"ST_NAME\": sqlalchemy.types.CHAR(length=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"\"\"CREATE TABLE StatusType (\n",
    "    ST_ID CHAR(4) NOT NULL,\n",
    "    ST_NAME CHAR(10) NOT NULL,\n",
    "    PRIMARY KEY (ST_ID)\n",
    ");\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_type_df.to_sql(name=\"statustype\", con=engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TradeType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TradeType data\n",
    "filepath = r\"..\\data\\sf5\\Batch1\\TradeType.txt\"\n",
    "trade_type_df = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"TT_ID\", \"TT_NAME\", \"TT_IS_SELL\", \"TT_IS_MRKT\"],\n",
    "    dtype={\"TT_ID\": \"str\", \"TT_NAME\": \"str\", \"TT_IS_SELL\": \"uint8\", \"TT_IS_MRKT\": \"uint8\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"TT_ID\": sqlalchemy.types.CHAR(length=3),\n",
    "    \"TT_NAME\": sqlalchemy.types.CHAR(length=12),\n",
    "    \"TT_IS_SELL\": sqlalchemy.types.SmallInteger,\n",
    "    \"TT_IS_MRKT\": sqlalchemy.types.SmallInteger\n",
    "}\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"\"\"CREATE TABLE TradeType (\n",
    "    TT_ID CHAR(3) NOT NULL,\n",
    "    TT_NAME CHAR(12) NOT NULL,\n",
    "    TT_IS_SELL TINYINT UNSIGNED NOT NULL CHECK (TT_IS_SELL IN (0, 1)),\n",
    "    TT_IS_MRKT TINYINT UNSIGNED NOT NULL CHECK (TT_IS_MRKT IN (0, 1)),\n",
    "    PRIMARY KEY (TT_ID)\n",
    ");\"\"\"))\n",
    "trade_type_df.to_sql(name=\"tradetype\", con=engine, if_exists='append', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TaxRate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TaxRate data\n",
    "filepath = r\"..\\data\\sf5\\Batch1\\TaxRate.txt\"\n",
    "tax_rate_df = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"TX_ID\", \"TX_NAME\", \"TX_RATE\"],\n",
    "    dtype={\"TX_ID\": \"str\", \"TX_NAME\": \"str\", \"TX_RATE\": \"float64\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"TX_ID\": sqlalchemy.types.CHAR(length=4),\n",
    "    \"TX_NAME\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"TX_RATE\": sqlalchemy.types.Numeric(precision=6, scale=5)\n",
    "}\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"\"\"CREATE TABLE TaxRate (\n",
    "    TX_ID CHAR(4) NOT NULL,\n",
    "    TX_NAME CHAR(50) NOT NULL,\n",
    "    TX_RATE DECIMAL(6, 5) NOT NULL,\n",
    "    PRIMARY KEY (TX_ID)\n",
    ");\"\"\"))\n",
    "\n",
    "tax_rate_df.to_sql(name=\"taxrate\", con=engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimBroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_df = pd.read_csv(\n",
    "    r\"..\\data\\sf5\\Batch1\\HR.csv\",\n",
    "    sep=\",\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"EmployeeID\", \"ManagerID\", \"EmployeeFirstName\", \"EmployeeLastName\",\n",
    "        \"EmployeeMI\", \"EmployeeJobCode\", \"EmployeeBranch\",\n",
    "        \"EmployeeOffice\", \"EmployeePhone\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"EmployeeID\": \"uint32\",\n",
    "        \"ManagerID\": \"uint32\",\n",
    "        \"EmployeeFirstName\": \"str\",\n",
    "        \"EmployeeLastName\": \"str\",\n",
    "        \"EmployeeMI\": \"str\",\n",
    "        \"EmployeeJobCode\": \"UInt16\",\n",
    "        \"EmployeeBranch\": \"str\",\n",
    "        \"EmployeeOffice\": \"str\",\n",
    "        \"EmployeePhone\": \"str\"\n",
    "    }\n",
    ")\n",
    "hr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter Records\n",
    "filtered_df = hr_df[hr_df['EmployeeJobCode'] == 314]\n",
    "filtered_df.drop(columns=['EmployeeJobCode'], inplace=True)\n",
    "# 2. Map Columns\n",
    "DimBroker = filtered_df.rename(columns={\n",
    "    'EmployeeID': 'BrokerID',\n",
    "    'ManagerID': 'ManagerID',\n",
    "    'EmployeeFirstName': 'FirstName',\n",
    "    'EmployeeLastName': 'LastName',\n",
    "    'EmployeeMI': 'MiddleInitial',\n",
    "    'EmployeeBranch': 'Branch',\n",
    "    'EmployeeOffice': 'Office',\n",
    "    'EmployeePhone': 'Phone'\n",
    "})\n",
    "# 3. Handle Surrogate Key (SK_BrokerID)\n",
    "# Using cumcount to generate a unique ID for each row\n",
    "DimBroker.loc[:, 'SK_BrokerID'] = range(1, len(DimBroker) + 1)\n",
    "\n",
    "# 4. Set Default Values for New Fields\n",
    "DimBroker.loc[:, 'IsCurrent'] = True\n",
    "DimBroker.loc[:, 'BatchID'] = BATCH_ID\n",
    "# EffectiveDate is set to the earliest date in the DimDate table and EndDate is set to 9999- 12-31\n",
    "DimBroker.loc[:, 'EffectiveDate'] = pd.read_sql_query(\"SELECT MIN(datevalue) FROM dimdate\", engine).iloc[0, 0]\n",
    "DimBroker.loc[:, 'EndDate'] = pd.Timestamp('9999-12-31')\n",
    "\n",
    "# Display the first few rows of the newly created DimBroker DataFrame\n",
    "DimBroker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"SK_BrokerID\": sqlalchemy.types.BigInteger,\n",
    "    \"BrokerID\": sqlalchemy.types.BigInteger,\n",
    "    \"ManagerID\": sqlalchemy.types.BigInteger,\n",
    "    \"FirstName\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"LastName\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"MiddleInitial\": sqlalchemy.types.CHAR(length=1),\n",
    "    \"Branch\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"Office\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"Phone\": sqlalchemy.types.CHAR(length=14),\n",
    "    \"IsCurrent\": sqlalchemy.types.Boolean,\n",
    "    \"BatchID\": sqlalchemy.types.Integer,\n",
    "    \"EffectiveDate\": sqlalchemy.types.Date,\n",
    "    \"EndDate\": sqlalchemy.types.Date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimBroker (\n",
    "    SK_BrokerID INT UNSIGNED NOT NULL,\n",
    "    BrokerID INT UNSIGNED NOT NULL,\n",
    "    ManagerID INT UNSIGNED,\n",
    "    FirstName CHAR(50) NOT NULL,\n",
    "    LastName CHAR(50) NOT NULL,\n",
    "    MiddleInitial CHAR(1),\n",
    "    Branch CHAR(50),\n",
    "    Office CHAR(50),\n",
    "    Phone CHAR(14),\n",
    "    IsCurrent BOOLEAN NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL,\n",
    "    EffectiveDate DATE NOT NULL,\n",
    "    EndDate DATE NOT NULL,\n",
    "    PRIMARY KEY (SK_BrokerID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimBroker.to_sql(name='dimbroker', con=engine, if_exists='append', index=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimCompany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_finwire(file_path):\n",
    "    # Define the column widths and names\n",
    "    col_widths = [15, 3, 60, 10, 4, 2, 4, 8, 80, 80, 12, 25, 20, 24, 46, 150]\n",
    "    col_names = [\n",
    "        \"PTS\", \"RecType\", \"CompanyName\", \"CIK\", \"Status\", \"IndustryID\",\n",
    "        \"SPrating\", \"FoundingDate\", \"AddrLine1\", \"AddrLine2\", \"PostalCode\",\n",
    "        \"City\", \"StateProvince\", \"Country\", \"CEOname\", \"Description\"\n",
    "    ]\n",
    "    # Read the fixed-width file\n",
    "    df = pd.read_fwf(file_path, widths=col_widths, header=None, names=col_names)\n",
    "\n",
    "    # Filter the DataFrame for CMP records\n",
    "    df_cmp = df[df['RecType'] == 'CMP']\n",
    "    return df_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query StatusType table and create a mapping dictionary\n",
    "with engine.connect() as conn:\n",
    "    statustype_df = pd.read_sql(\"SELECT * FROM statustype\", conn)\n",
    "status_mapping = dict(statustype_df[[\"ST_ID\", \"ST_NAME\"]].values)\n",
    "\n",
    "# Query Industry table and create a mapping dictionary\n",
    "with engine.connect() as conn:\n",
    "    industry_df = pd.read_sql(\"SELECT * FROM industry\", conn)\n",
    "industry_mapping = dict(industry_df[[\"IN_ID\", \"IN_NAME\"]].values)\n",
    "\n",
    "# Valid SPrating values\n",
    "valid_spratings = [\n",
    "    \"AAA\",\n",
    "    \"AA+\",\n",
    "    \"AA\",\n",
    "    \"AA-\",\n",
    "    \"A+\",\n",
    "    \"A\",\n",
    "    \"A-\",\n",
    "    \"BBB+\",\n",
    "    \"BBB\",\n",
    "    \"BBB-\",\n",
    "    \"BB+\",\n",
    "    \"BB\",\n",
    "    \"BB-\",\n",
    "    \"B+\",\n",
    "    \"B\",\n",
    "    \"B-\",\n",
    "    \"CCC+\",\n",
    "    \"CCC\",\n",
    "    \"CCC-\",\n",
    "    \"CC\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(DATA_DIR)\n",
    "finwire_files = [file for file in files if file.startswith(\"FINWIRE\") and 'audit' not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_CompanyID\": sqlalchemy.types.BigInteger,\n",
    "    \"CompanyID\": sqlalchemy.types.BigInteger,\n",
    "    \"Status\": sqlalchemy.types.CHAR(length=10),\n",
    "    \"Name\": sqlalchemy.types.CHAR(length=60),\n",
    "    \"Industry\": sqlalchemy.types.CHAR(length=50),\n",
    "    \"SPrating\": sqlalchemy.types.CHAR(length=4),\n",
    "    \"isLowGrade\": sqlalchemy.types.Boolean,\n",
    "    \"CEO\": sqlalchemy.types.CHAR(length=100),\n",
    "    \"AddressLine1\": sqlalchemy.types.CHAR(length=80),\n",
    "    \"AddressLine2\": sqlalchemy.types.CHAR(length=80),\n",
    "    \"PostalCode\": sqlalchemy.types.CHAR(length=12),\n",
    "    \"City\": sqlalchemy.types.CHAR(length=25),\n",
    "    \"StateProv\": sqlalchemy.types.CHAR(length=20),\n",
    "    \"Country\": sqlalchemy.types.CHAR(length=24),\n",
    "    \"Description\": sqlalchemy.types.CHAR(length=150),\n",
    "    \"FoundingDate\": sqlalchemy.types.Date,\n",
    "    \"IsCurrent\": sqlalchemy.types.Boolean,\n",
    "    \"BatchID\": sqlalchemy.types.Integer,\n",
    "    \"EffectiveDate\": sqlalchemy.types.Date,\n",
    "    \"EndDate\": sqlalchemy.types.Date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dimcompany(filename, is_first_batch=False):\n",
    "    file_path = DATA_DIR + filename\n",
    "    df_cmp = read_finwire(file_path)\n",
    "\n",
    "    if len(df_cmp) == 0:\n",
    "        return\n",
    "    \n",
    "    # Define the column names and data types\n",
    "    column_names = [\n",
    "        \"SK_CompanyID\",\n",
    "        \"CompanyID\",\n",
    "        \"Status\",\n",
    "        \"Name\",\n",
    "        \"Industry\",\n",
    "        \"SPrating\",\n",
    "        \"isLowGrade\",\n",
    "        \"CEO\",\n",
    "        \"AddressLine1\",\n",
    "        \"AddressLine2\",\n",
    "        \"PostalCode\",\n",
    "        \"City\",\n",
    "        \"StateProv\",\n",
    "        \"Country\",\n",
    "        \"Description\",\n",
    "        \"FoundingDate\",\n",
    "        \"IsCurrent\",\n",
    "        \"BatchID\",\n",
    "        \"EffectiveDate\",\n",
    "        \"EndDate\",\n",
    "    ]\n",
    "    dtypes = {\n",
    "        \"SK_CompanyID\": \"uint32\",\n",
    "        \"CompanyID\": \"uint32\",\n",
    "        \"Status\": \"str\",\n",
    "        \"Name\": \"str\",\n",
    "        \"Industry\": \"str\",\n",
    "        \"SPrating\": \"str\",\n",
    "        \"isLowGrade\": \"boolean\",\n",
    "        \"CEO\": \"str\",\n",
    "        \"AddressLine1\": \"str\",\n",
    "        \"AddressLine2\": \"str\",\n",
    "        \"PostalCode\": \"str\",\n",
    "        \"City\": \"str\",\n",
    "        \"StateProv\": \"str\",\n",
    "        \"Country\": \"str\",\n",
    "        \"Description\": \"str\",\n",
    "        \"FoundingDate\": \"datetime64[ns]\",\n",
    "        \"IsCurrent\": \"bool\",\n",
    "        \"BatchID\": \"uint8\",\n",
    "        \"EffectiveDate\": \"datetime64[ns]\",\n",
    "        \"EndDate\": \"datetime64[ns]\",\n",
    "    }\n",
    "    # Create an empty DataFrame with the specified schema\n",
    "    dimCompany = pd.DataFrame(columns=column_names).astype(dtypes)\n",
    "\n",
    "    # Copy and map relevant columns\n",
    "    df_cmp[\"CIK\"] = pd.to_numeric(df_cmp[\"CIK\"], downcast=\"unsigned\")\n",
    "    dimCompany[\"CompanyID\"] = pd.to_numeric(df_cmp[\"CIK\"], downcast=\"unsigned\")\n",
    "    dimCompany[\"Name\"] = df_cmp[\"CompanyName\"].str.strip()\n",
    "    dimCompany[\"SPrating\"] = df_cmp[\"SPrating\"].str.upper()\n",
    "    dimCompany[\"CEO\"] = df_cmp[\"CEOname\"].str.strip()\n",
    "    dimCompany[\"Description\"] = df_cmp[\"Description\"].str.strip()\n",
    "    dimCompany[\"FoundingDate\"] = pd.to_datetime(\n",
    "        df_cmp[\"FoundingDate\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # For address fields\n",
    "    dimCompany[\"AddressLine1\"] = df_cmp[\"AddrLine1\"].str.strip()\n",
    "    dimCompany[\"AddressLine2\"] = df_cmp[\"AddrLine2\"].str.strip()\n",
    "    dimCompany[\"PostalCode\"] = df_cmp[\"PostalCode\"].str.strip()\n",
    "    dimCompany[\"City\"] = df_cmp[\"City\"].str.strip()\n",
    "    dimCompany[\"StateProv\"] = df_cmp[\"StateProvince\"].str.strip()\n",
    "    dimCompany[\"Country\"] = df_cmp[\"Country\"].str.strip()\n",
    "\n",
    "    # Replace all-blank strings with None (NULL)\n",
    "    for col in [\n",
    "        \"Name\",\n",
    "        \"SPrating\",\n",
    "        \"CEO\",\n",
    "        \"Description\",\n",
    "        \"AddressLine1\",\n",
    "        \"AddressLine2\",\n",
    "        \"PostalCode\",\n",
    "        \"City\",\n",
    "        \"StateProv\",\n",
    "        \"Country\",\n",
    "    ]:\n",
    "        dimCompany[col] = dimCompany[col].replace(r\"^\\s*$\", None, regex=True)\n",
    "\n",
    "    # Update Status in dimCompany\n",
    "    dimCompany[\"Status\"] = df_cmp[\"Status\"].map(status_mapping)\n",
    "    # Update Industry in dimCompany\n",
    "    dimCompany[\"Industry\"] = df_cmp[\"IndustryID\"].map(industry_mapping)\n",
    "    # isLowGrade is set to False if SPrating begins with ‘A’ or ‘BBB’ otherwise set to True\n",
    "    dimCompany[\"isLowGrade\"] = ~df_cmp[\"SPrating\"].str.startswith((\"A\", \"BBB\"))\n",
    "\n",
    "    # Identify invalid SPratings\n",
    "    invalid_sprating_mask = ~dimCompany[\"SPrating\"].isin(valid_spratings)\n",
    "    # Filter dimCompany for invalid SPrating\n",
    "    invalid_sprating_data = dimCompany[invalid_sprating_mask]\n",
    "    if len(invalid_sprating_data) > 0:\n",
    "        message_data = (\n",
    "            \"CO_ID = \"\n",
    "            + invalid_sprating_data[\"CompanyID\"].astype(str)\n",
    "            + \", CO_SP_RATE = \"\n",
    "            + invalid_sprating_data[\"SPrating\"]\n",
    "        )\n",
    "        # Create DImessages DataFrame\n",
    "        dimessages = pd.DataFrame(\n",
    "            {\n",
    "                \"MessageDateAndTime\": [datetime.now()] * len(message_data),\n",
    "                \"BatchID\": [1] * len(message_data),\n",
    "                \"MessageSource\": [\"DimCompany\"] * len(message_data),\n",
    "                \"MessageText\": [\"Invalid SPRating\"] * len(message_data),\n",
    "                \"MessageType\": [\"Alert\"] * len(message_data),\n",
    "                \"MessageData\": message_data,\n",
    "            }\n",
    "        )\n",
    "        # Update dimCompany for invalid SPrating\n",
    "        dimCompany.loc[invalid_sprating_mask, [\"SPrating\", \"isLowGrade\"]] = pd.NA\n",
    "        # Insert DImessages into MySQL\n",
    "        dimessages.to_sql(\"dimessages\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "    dimCompany.loc[:, \"BatchID\"] = 1\n",
    "    dimCompany[\"EffectiveDate\"] = pd.to_datetime(df_cmp[\"PTS\"], format=\"%Y%m%d-%H%M%S\")\n",
    "    # Identify new and existing records based on CIK\n",
    "    if is_first_batch:\n",
    "        new_records = dimCompany\n",
    "        existing_records = pd.DataFrame(columns=column_names).astype(dtypes)\n",
    "        next_sk_id = 0\n",
    "    else:\n",
    "        existing_cik = pd.read_sql_query(\n",
    "            \"SELECT CompanyID FROM dimCompany WHERE IsCurrent = 1\", engine\n",
    "        )[\"CompanyID\"]\n",
    "        new_records = dimCompany[~df_cmp[\"CIK\"].isin(existing_cik)]\n",
    "        existing_records = dimCompany[df_cmp[\"CIK\"].isin(existing_cik)]\n",
    "        next_sk_id_query = \"SELECT MAX(SK_CompanyID) FROM dimCompany\"\n",
    "        next_sk_id = pd.read_sql_query(next_sk_id_query, engine).iloc[0, 0] or 0\n",
    "\n",
    "    new_records.loc[:, \"SK_CompanyID\"] = range(\n",
    "        next_sk_id + 1, next_sk_id + 1 + len(new_records)\n",
    "    )\n",
    "    new_records.loc[:, \"IsCurrent\"] = True\n",
    "    new_records.loc[:, \"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "    new_records.to_sql(\n",
    "        \"dimcompany\", engine, if_exists=\"append\", index=False, dtype=sql_dtypes\n",
    "    )\n",
    "    next_sk_id = new_records[\"SK_CompanyID\"].max()\n",
    "\n",
    "    # Process existing records\n",
    "    for _, row in existing_records.iterrows():\n",
    "        effective_date = row[\"EffectiveDate\"]\n",
    "        company_id = row[\"CompanyID\"]\n",
    "        # get the current record\n",
    "        select_query = f\"\"\"SELECT * FROM dimCompany\n",
    "        WHERE CompanyID = '{company_id}' AND IsCurrent = 1\"\"\"\n",
    "        existing_record = pd.read_sql(select_query, engine).iloc[0]\n",
    "        compare_cols = [\n",
    "            \"Status\",\n",
    "            \"Name\",\n",
    "            \"Industry\",\n",
    "            \"SPrating\",\n",
    "            \"isLowGrade\",\n",
    "            \"CEO\",\n",
    "            \"AddressLine1\",\n",
    "            \"AddressLine2\",\n",
    "            \"PostalCode\",\n",
    "            \"City\",\n",
    "            \"StateProv\",\n",
    "            \"Country\",\n",
    "            \"Description\",\n",
    "            \"FoundingDate\",\n",
    "        ]\n",
    "        is_same = True\n",
    "        for col in compare_cols:\n",
    "            if row[col] != existing_record[col]:\n",
    "                is_same = False\n",
    "                break\n",
    "        if not is_same:\n",
    "            # Expire the current record in MySQL\n",
    "            update_query = f\"\"\"UPDATE dimcompany \n",
    "            SET IsCurrent = 0, EndDate = '{effective_date}' \n",
    "            WHERE CompanyID = '{company_id}' AND IsCurrent = 1\n",
    "            \"\"\"\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(update_query))\n",
    "                conn.commit()\n",
    "            # Insert updated record\n",
    "            row[\"SK_CompanyID\"] = next_sk_id + 1\n",
    "            row[\"IsCurrent\"] = True\n",
    "            row[\"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "            row_df = pd.DataFrame(row).T\n",
    "            # insert records with existing SK_CompanyID\n",
    "            row_df.to_sql(\n",
    "                \"dimcompany\", engine, if_exists=\"append\", index=False, dtype=sql_dtypes\n",
    "            )        \n",
    "            next_sk_id += 1\n",
    "\n",
    "    return df_cmp, new_records, existing_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimCompany (\n",
    "    SK_CompanyID INT UNSIGNED NOT NULL,\n",
    "    CompanyID INT UNSIGNED NOT NULL,\n",
    "    Status CHAR(10) NOT NULL,\n",
    "    Name CHAR(60) NOT NULL,\n",
    "    Industry CHAR(50) NOT NULL,\n",
    "    SPrating CHAR(4),\n",
    "    isLowGrade BOOLEAN,\n",
    "    CEO CHAR(100) NOT NULL,\n",
    "    AddressLine1 CHAR(80),\n",
    "    AddressLine2 CHAR(80),\n",
    "    PostalCode CHAR(12) NOT NULL,\n",
    "    City CHAR(25) NOT NULL,\n",
    "    StateProv CHAR(20) NOT NULL,\n",
    "    Country CHAR(24),\n",
    "    Description CHAR(150) NOT NULL,\n",
    "    FoundingDate DATE,\n",
    "    IsCurrent BOOLEAN NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL,\n",
    "    EffectiveDate DATE NOT NULL,\n",
    "    EndDate DATE NOT NULL,\n",
    "    PRIMARY KEY (SK_CompanyID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(tqdm(finwire_files)):\n",
    "    load_dimcompany(file, i == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_finwire_fin(file_path):\n",
    "    # Define the column widths and names\n",
    "    col_widths = [15, 3, 4, 1, 8, 8, 17, 17, 12, 12, 12, 17, 17, 17, 13, 13, 60]\n",
    "    col_names = [\n",
    "        \"PTS\", \"RecType\", \"Year\", \"Quarter\", \"QtrStartDate\", \"PostingDate\",\n",
    "        \"Revenue\", \"Earnings\", \"EPS\", \"DilutedEPS\", \"Margin\", \"Inventory\",\n",
    "        \"Assets\", \"Liabilities\", \"ShOut\", \"DilutedShOut\", \"CoNameOrCIK\"\n",
    "    ]\n",
    "    # Read the fixed-width file\n",
    "    df_fin = pd.read_fwf(file_path, widths=col_widths, header=None, names=col_names)\n",
    "    # Filter the DataFrame for CMP records\n",
    "    df_fin = df_fin[df_fin['RecType'] == 'FIN']\n",
    "    # Convert PTS to datetime\n",
    "    df_fin['PTS'] = pd.to_datetime(df_fin['PTS'], format='%Y%m%d-%H%M%S')\n",
    "\n",
    "    return df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_financial():\n",
    "    files = os.listdir(DATA_DIR)\n",
    "    finwire_files = [file for file in files if file.startswith(\"FINWIRE\") and 'audit' not in file]\n",
    "    \n",
    "\n",
    "    for filename in tqdm(finwire_files):\n",
    "        file_path = DATA_DIR + filename\n",
    "        df_fin = read_finwire_fin(file_path)\n",
    "        if len(df_fin) == 0:\n",
    "            continue\n",
    "        # datatypes for the mysql table\n",
    "        sql_dtypes = {\n",
    "            \"SK_CompanyID\": sqlalchemy.types.BigInteger,\n",
    "            \"FI_YEAR\": sqlalchemy.types.Integer,\n",
    "            \"FI_QTR\": sqlalchemy.types.SmallInteger,\n",
    "            \"FI_QTR_START_DATE\": sqlalchemy.types.Date,\n",
    "            \"FI_REVENUE\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "            \"FI_NET_EARN\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "            \"FI_BASIC_EPS\": sqlalchemy.types.Numeric(precision=10, scale=2),\n",
    "            \"FI_DILUT_EPS\": sqlalchemy.types.Numeric(precision=10, scale=2),\n",
    "            \"FI_MARGIN\": sqlalchemy.types.Numeric(precision=10, scale=2),\n",
    "            \"FI_INVENTORY\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "            \"FI_ASSETS\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "            \"FI_LIABILITY\": sqlalchemy.types.Numeric(precision=15, scale=2),\n",
    "            \"FI_OUT_BASIC\": sqlalchemy.types.BigInteger,\n",
    "            \"FI_OUT_DILUT\": sqlalchemy.types.BigInteger\n",
    "        }\n",
    "\n",
    "        # data types for the DataFrame\n",
    "        dtypes = {\n",
    "            'SK_CompanyID': 'uint32',\n",
    "            'FI_YEAR': 'uint16',\n",
    "            'FI_QTR': 'uint8',\n",
    "            'FI_QTR_START_DATE': 'datetime64[ns]',\n",
    "            'FI_REVENUE': 'float64',\n",
    "            'FI_NET_EARN': 'float64',\n",
    "            'FI_BASIC_EPS': 'float64',\n",
    "            'FI_DILUT_EPS': 'float64',\n",
    "            'FI_MARGIN': 'float64',\n",
    "            'FI_INVENTORY': 'float64',\n",
    "            'FI_ASSETS': 'float64',\n",
    "            'FI_LIABILITY': 'float64',\n",
    "            'FI_OUT_BASIC': 'uint64',\n",
    "            'FI_OUT_DILUT': 'uint64'\n",
    "        }\n",
    "\n",
    "        # Create empty DataFrame\n",
    "        financial_df = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in dtypes.items()})\n",
    "\n",
    "        # copy directly\n",
    "        financial_df['FI_YEAR'] = pd.to_numeric(df_fin['Year'].str.strip(), downcast='unsigned')\n",
    "        financial_df['FI_QTR'] = pd.to_numeric(df_fin['Quarter'].str.strip(), downcast='unsigned')\n",
    "        financial_df['FI_QTR_START_DATE'] = pd.to_datetime(df_fin['QtrStartDate'], format='%Y%m%d')\n",
    "        financial_df['FI_REVENUE'] = pd.to_numeric(df_fin['Revenue'].str.strip(), downcast='float')\n",
    "        financial_df['FI_NET_EARN'] = pd.to_numeric(df_fin['Earnings'].str.strip(), downcast='float')\n",
    "        financial_df['FI_BASIC_EPS'] = pd.to_numeric(df_fin['EPS'].str.strip(), downcast='float')\n",
    "        financial_df['FI_DILUT_EPS'] = pd.to_numeric(df_fin['DilutedEPS'].str.strip(), downcast='float')\n",
    "        financial_df['FI_MARGIN'] = pd.to_numeric(df_fin['Margin'].str.strip(), downcast='float')\n",
    "        financial_df['FI_INVENTORY'] = pd.to_numeric(df_fin['Inventory'].str.strip(), downcast='float')\n",
    "        financial_df['FI_ASSETS'] = pd.to_numeric(df_fin['Assets'].str.strip(), downcast='float')\n",
    "        financial_df['FI_LIABILITY'] = pd.to_numeric(df_fin['Liabilities'].str.strip(), downcast='float')\n",
    "        financial_df['FI_OUT_BASIC'] = pd.to_numeric(df_fin['ShOut'].str.strip(), downcast='unsigned')\n",
    "        financial_df['FI_OUT_DILUT'] = pd.to_numeric(df_fin['DilutedShOut'].str.strip(), downcast='unsigned')\n",
    "\n",
    "        # Split df_fin based on the length of CoNameOrCIK\n",
    "        df_fin_id = df_fin[df_fin['CoNameOrCIK'].str.len() == 10][['PTS', 'CoNameOrCIK']]\n",
    "        df_fin_id['PTS'] = df_fin_id['PTS'].dt.strftime('%Y-%m-%d')\n",
    "        df_fin_id['CoNameOrCIK'] = pd.to_numeric(df_fin_id['CoNameOrCIK'], downcast='unsigned')\n",
    "        df_fin_name = df_fin[df_fin['CoNameOrCIK'].str.len() != 10][['PTS', 'CoNameOrCIK']]\n",
    "        df_fin_name['PTS'] = df_fin_name['PTS'].dt.strftime('%Y-%m-%d')\n",
    "        df_fin_name['CoNameOrCIK'] = df_fin_name['CoNameOrCIK'].str.strip()\n",
    "\n",
    "        def build_query(df, id_or_name_col):\n",
    "            '''Function to build SQL query for date range checks'''\n",
    "            query_parts = []\n",
    "            for _, row in df.iterrows():\n",
    "                pts = row['PTS']\n",
    "                if id_or_name_col == 'CompanyID':\n",
    "                    company_id = row['CoNameOrCIK']\n",
    "                    query_part = f\"(CompanyID = {company_id} AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "                else:  # Name\n",
    "                    company_name = row['CoNameOrCIK']\n",
    "                    query_part = f\"(Name = '{company_name}' AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "                query_parts.append(query_part)\n",
    "            return ' OR '.join(query_parts)\n",
    "\n",
    "        # Execute query and map results for ID-based records\n",
    "        if not df_fin_id.empty:\n",
    "            query_id = f\"SELECT CompanyID, SK_CompanyID FROM dimcompany WHERE \" + build_query(df_fin_id, 'CompanyID')\n",
    "            sk_id_map = pd.read_sql_query(query_id, engine).set_index('CompanyID')['SK_CompanyID']            \n",
    "            financial_df.loc[df_fin_id.index, 'SK_CompanyID'] = df_fin_id['CoNameOrCIK'].astype(int).map(sk_id_map)\n",
    "\n",
    "        # Execute query and map results for Name-based records\n",
    "        if not df_fin_name.empty:\n",
    "            query_name = f\"SELECT Name, SK_CompanyID FROM dimcompany WHERE \" + build_query(df_fin_name, 'Name')\n",
    "            sk_name_map = pd.read_sql_query(query_name, engine).set_index('Name')['SK_CompanyID']\n",
    "            financial_df.loc[df_fin_name.index, 'SK_CompanyID'] = df_fin_name['CoNameOrCIK'].map(sk_name_map)\n",
    "\n",
    "        financial_df['SK_CompanyID'] = financial_df['SK_CompanyID'].astype('uint32')\n",
    "\n",
    "        # perform migration\n",
    "        financial_df.to_sql('financial', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE Financial (\n",
    "    SK_CompanyID INT UNSIGNED NOT NULL,\n",
    "    FI_YEAR YEAR NOT NULL,\n",
    "    FI_QTR TINYINT UNSIGNED NOT NULL CHECK (FI_QTR IN (1, 2, 3, 4)),\n",
    "    FI_QTR_START_DATE DATE NOT NULL,\n",
    "    FI_REVENUE DECIMAL(15, 2) NOT NULL,\n",
    "    FI_NET_EARN DECIMAL(15, 2) NOT NULL,\n",
    "    FI_BASIC_EPS DECIMAL(10, 2) NOT NULL,\n",
    "    FI_DILUT_EPS DECIMAL(10, 2) NOT NULL,\n",
    "    FI_MARGIN DECIMAL(10, 2) NOT NULL,\n",
    "    FI_INVENTORY DECIMAL(15, 2) NOT NULL,\n",
    "    FI_ASSETS DECIMAL(15, 2) NOT NULL,\n",
    "    FI_LIABILITY DECIMAL(15, 2) NOT NULL,\n",
    "    FI_OUT_BASIC BIGINT NOT NULL,\n",
    "    FI_OUT_DILUT BIGINT NOT NULL,\n",
    "    PRIMARY KEY (SK_CompanyID, FI_YEAR, FI_QTR)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_financial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimSecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_finwire_sec(file_path):\n",
    "    # Define the column widths and names\n",
    "    col_widths = [15, 3, 15, 6, 4, 70, 6, 13, 8, 8, 12, 60]\n",
    "    col_names = [\n",
    "        \"PTS\", \"RecType\", \"Symbol\", \"IssueType\", \"Status\", \"Name\", \"ExID\",\n",
    "        \"ShOut\", \"FirstTradeDate\", \"FirstTradeExchg\", \"Dividend\", \"CoNameOrCIK\"\n",
    "    ]\n",
    "    # Read the fixed-width file\n",
    "    df_sec = pd.read_fwf(file_path, widths=col_widths, header=None, names=col_names)\n",
    "    # Filter the DataFrame for CMP records\n",
    "    df_sec = df_sec[df_sec['RecType'] == 'SEC']\n",
    "    # Convert date cols to datetime\n",
    "    df_sec['PTS'] = pd.to_datetime(df_sec['PTS'], format='%Y%m%d-%H%M%S')\n",
    "    df_sec['FirstTradeDate'] = pd.to_datetime(df_sec['FirstTradeDate'], format='%Y%m%d')\n",
    "    df_sec['FirstTradeExchg'] = pd.to_datetime(df_sec['FirstTradeExchg'], format='%Y%m%d')\n",
    "\n",
    "    return df_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"Symbol\": sqlalchemy.types.String(15),\n",
    "    \"Issue\": sqlalchemy.types.String(6),\n",
    "    \"Status\": sqlalchemy.types.String(10),\n",
    "    \"Name\": sqlalchemy.types.String(70),\n",
    "    \"ExchangeID\": sqlalchemy.types.String(6),\n",
    "    \"SK_CompanyID\": sqlalchemy.types.Integer,\n",
    "    \"SharesOutstanding\": sqlalchemy.types.Integer,\n",
    "    \"FirstTrade\": sqlalchemy.types.Date,\n",
    "    \"FirstTradeOnExchange\": sqlalchemy.types.Date,\n",
    "    \"Dividend\": sqlalchemy.types.Numeric(10, 2),\n",
    "    \"IsCurrent\": sqlalchemy.types.Boolean,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger,\n",
    "    \"EffectiveDate\": sqlalchemy.types.Date,\n",
    "    \"EndDate\": sqlalchemy.types.Date,\n",
    "}\n",
    "\n",
    "dtypes = {\n",
    "    \"SK_SecurityID\": \"uint32\",\n",
    "    \"Symbol\": \"str\",\n",
    "    \"Issue\": \"str\",\n",
    "    \"Status\": \"str\",\n",
    "    \"Name\": \"str\",\n",
    "    \"ExchangeID\": \"str\",\n",
    "    \"SK_CompanyID\": \"uint32\",\n",
    "    \"SharesOutstanding\": \"uint32\",\n",
    "    \"FirstTrade\": \"datetime64[ns]\",\n",
    "    \"FirstTradeOnExchange\": \"datetime64[ns]\",\n",
    "    \"Dividend\": \"float64\",\n",
    "    \"IsCurrent\": \"bool\",\n",
    "    \"BatchID\": \"uint8\",\n",
    "    \"EffectiveDate\": \"datetime64[ns]\",\n",
    "    \"EndDate\": \"datetime64[ns]\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query StatusType table and create a mapping dictionary\n",
    "with engine.connect() as conn:\n",
    "    statustype_df = pd.read_sql(\"SELECT * FROM statustype\", conn)\n",
    "status_mapping = dict(statustype_df[['ST_ID', 'ST_NAME']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query(df, id_or_name_col):\n",
    "    '''Function to build SQL query for date range checks'''\n",
    "    query_parts = []\n",
    "    for _, row in df.iterrows():\n",
    "        pts = row['PTS']\n",
    "        if id_or_name_col == 'CompanyID':\n",
    "            company_id = row['CoNameOrCIK']\n",
    "            query_part = f\"(CompanyID = {company_id} AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "        else:  # Name\n",
    "            company_name = row['CoNameOrCIK']\n",
    "            query_part = f\"(Name = '{company_name}' AND EffectiveDate <= '{pts}' AND '{pts}' < EndDate)\"\n",
    "        query_parts.append(query_part)\n",
    "    return ' OR '.join(query_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dimsecurity():\n",
    "    finwire_files = os.listdir(DATA_DIR)\n",
    "    finwire_files = [\n",
    "        DATA_DIR + file\n",
    "        for file in finwire_files\n",
    "        if file.startswith(\"FINWIRE\") and \"audit\" not in file\n",
    "    ]\n",
    "    for i, file in enumerate(tqdm(finwire_files)):\n",
    "        # raw data from file\n",
    "        df_sec = read_finwire_sec(file)\n",
    "        # dimension table in data warehouse\n",
    "        security_df = pd.DataFrame(\n",
    "            {col: pd.Series(dtype=typ) for col, typ in dtypes.items()}\n",
    "        )\n",
    "        # copy directly\n",
    "        security_df[\"Symbol\"] = df_sec[\"Symbol\"]\n",
    "        security_df[\"Issue\"] = df_sec[\"IssueType\"]\n",
    "        security_df[\"Name\"] = df_sec[\"Name\"]\n",
    "        security_df[\"ExchangeID\"] = df_sec[\"ExID\"]\n",
    "        security_df[\"SharesOutstanding\"] = pd.to_numeric(\n",
    "            df_sec[\"ShOut\"], downcast=\"unsigned\"\n",
    "        )\n",
    "        security_df[\"FirstTrade\"] = df_sec[\"FirstTradeDate\"]\n",
    "        security_df[\"FirstTradeOnExchange\"] = df_sec[\"FirstTradeExchg\"]\n",
    "        security_df[\"Dividend\"] = pd.to_numeric(df_sec[\"Dividend\"], downcast=\"float\")\n",
    "        # Update Status in security_df\n",
    "        security_df[\"Status\"] = df_sec[\"Status\"].map(status_mapping)\n",
    "        # BatchID is set to 1\n",
    "        security_df[\"BatchID\"] = BATCH_ID\n",
    "        # Split df_sec based on the length of CoNameOrCIK\n",
    "        df_sec_id = df_sec[df_sec[\"CoNameOrCIK\"].str.len() == 10][\n",
    "            [\"PTS\", \"CoNameOrCIK\"]\n",
    "        ]\n",
    "        df_sec_id[\"PTS\"] = df_sec_id[\"PTS\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df_sec_id[\"CoNameOrCIK\"] = pd.to_numeric(\n",
    "            df_sec_id[\"CoNameOrCIK\"], downcast=\"unsigned\"\n",
    "        )\n",
    "        df_sec_name = df_sec[df_sec[\"CoNameOrCIK\"].str.len() != 10][\n",
    "            [\"PTS\", \"CoNameOrCIK\"]\n",
    "        ]\n",
    "        df_sec_name[\"PTS\"] = df_sec_name[\"PTS\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df_sec_name[\"CoNameOrCIK\"] = df_sec_name[\"CoNameOrCIK\"].str.strip()\n",
    "        # Map results for ID-based records\n",
    "        if not df_sec_id.empty:\n",
    "            query_id = (\n",
    "                f\"SELECT CompanyID, SK_CompanyID FROM dimcompany WHERE \"\n",
    "                + build_query(df_sec_id, \"CompanyID\")\n",
    "            )\n",
    "            sk_id_map = pd.read_sql_query(query_id, engine).set_index(\"CompanyID\")[\n",
    "                \"SK_CompanyID\"\n",
    "            ]\n",
    "            # drop duplicates from the index\n",
    "            sk_id_map = sk_id_map[~sk_id_map.index.duplicated(keep=\"last\")]\n",
    "            security_df.loc[df_sec_id.index, \"SK_CompanyID\"] = (\n",
    "                df_sec_id[\"CoNameOrCIK\"].astype(int).map(sk_id_map)\n",
    "            )\n",
    "        # Map results for Name-based records\n",
    "        if not df_sec_name.empty:\n",
    "            query_name = (\n",
    "                f\"SELECT Name, SK_CompanyID FROM dimcompany WHERE \"\n",
    "                + build_query(df_sec_name, \"Name\")\n",
    "            )\n",
    "            sk_name_map = pd.read_sql_query(query_name, engine).set_index(\"Name\")[\n",
    "                \"SK_CompanyID\"\n",
    "            ]\n",
    "            # drop duplicates from the index\n",
    "            sk_name_map = sk_name_map[~sk_name_map.index.duplicated(keep=\"last\")]\n",
    "            security_df.loc[df_sec_name.index, \"SK_CompanyID\"] = df_sec_name[\n",
    "                \"CoNameOrCIK\"\n",
    "            ].map(sk_name_map)\n",
    "        # change the type back to uint32\n",
    "        security_df[\"SK_CompanyID\"] = security_df[\"SK_CompanyID\"].astype(\"uint32\")\n",
    "        # get effective date from posting date\n",
    "        security_df[\"EffectiveDate\"] = df_sec[\"PTS\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        # Identify new and existing records based on Symbol\n",
    "        is_first_batch = i == 0\n",
    "        if is_first_batch:\n",
    "            new_records = security_df\n",
    "            existing_records = pd.DataFrame(\n",
    "                {col: pd.Series(dtype=typ) for col, typ in dtypes.items()}\n",
    "            )\n",
    "            next_sk_id = 0\n",
    "        else:\n",
    "            existing_symbol = pd.read_sql_query(\n",
    "                \"SELECT Symbol FROM dimsecurity WHERE IsCurrent = 1\", engine\n",
    "            )[\"Symbol\"]\n",
    "            new_records = security_df[~security_df[\"Symbol\"].isin(existing_symbol)]\n",
    "            existing_records = security_df[security_df[\"Symbol\"].isin(existing_symbol)]\n",
    "            next_sk_id_query = \"SELECT MAX(SK_SecurityID) FROM dimsecurity\"\n",
    "            next_sk_id = pd.read_sql_query(next_sk_id_query, engine).iloc[0, 0] or 0\n",
    "        # update SK_SecurityID, IsCurrent, EndDate\n",
    "        new_records.loc[:, \"SK_SecurityID\"] = range(\n",
    "            next_sk_id + 1, next_sk_id + 1 + len(new_records)\n",
    "        )\n",
    "        new_records.loc[:, \"IsCurrent\"] = True\n",
    "        new_records.loc[:, \"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "        # Insert records with new SK_CompanyID\n",
    "        new_records.to_sql(\n",
    "            \"dimsecurity\", engine, if_exists=\"append\", index=False, dtype=sql_dtypes\n",
    "        )\n",
    "        next_sk_id = new_records[\"SK_SecurityID\"].max()\n",
    "\n",
    "        # Process existing records\n",
    "        for _, row in existing_records.iterrows():\n",
    "            effective_date = row[\"EffectiveDate\"]\n",
    "            symbol = row[\"Symbol\"]\n",
    "            # Expire the current record in MySQL\n",
    "            update_query = f\"\"\"UPDATE dimsecurity \n",
    "            SET IsCurrent = 0, EndDate = '{effective_date}' \n",
    "            WHERE Symbol = '{symbol}' AND IsCurrent = 1\n",
    "            \"\"\"\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(update_query))\n",
    "                conn.commit()\n",
    "            row[\"SK_SecurityID\"] = next_sk_id + 1\n",
    "            row[\"IsCurrent\"] = True\n",
    "            row[\"EndDate\"] = pd.Timestamp(\"9999-12-31\")\n",
    "            row_df = pd.DataFrame(row).T\n",
    "            # insert records with existing SK_CompanyID\n",
    "            row_df.to_sql(\n",
    "                \"dimsecurity\", engine, if_exists=\"append\", index=False, dtype=sql_dtypes\n",
    "            )\n",
    "            next_sk_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimSecurity (\n",
    "    SK_SecurityID INT UNSIGNED NOT NULL,\n",
    "    Symbol CHAR(15) NOT NULL,\n",
    "    Issue CHAR(6) NOT NULL,\n",
    "    Status CHAR(10) NOT NULL,\n",
    "    Name CHAR(70) NOT NULL,\n",
    "    ExchangeID CHAR(6) NOT NULL,\n",
    "    SK_CompanyID INT UNSIGNED NOT NULL,\n",
    "    SharesOutstanding BIGINT UNSIGNED NOT NULL,\n",
    "    FirstTrade DATE NOT NULL,\n",
    "    FirstTradeOnExchange DATE NOT NULL,\n",
    "    Dividend DECIMAL(10, 2) NOT NULL,\n",
    "    IsCurrent BOOLEAN NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL,\n",
    "    EffectiveDate DATE NOT NULL,\n",
    "    EndDate DATE NOT NULL,\n",
    "    PRIMARY KEY (SK_SecurityID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dimsecurity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prospect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prospect_file(filepath):\n",
    "    # Define the column names and their data types\n",
    "    columns = [\n",
    "        'AgencyID', 'LastName', 'FirstName', 'MiddleInitial', 'Gender', \n",
    "        'AddressLine1', 'AddressLine2', 'PostalCode', 'City', 'State', \n",
    "        'Country', 'Phone', 'Income', 'NumberCars', 'NumberChildren', \n",
    "        'MaritalStatus', 'Age', 'CreditRating', 'OwnOrRentFlag', \n",
    "        'Employer', 'NumberCreditCards', 'NetWorth'\n",
    "    ]\n",
    "\n",
    "    # Define the data types for reading the file\n",
    "    dtypes = {\n",
    "        'AgencyID': 'str', 'LastName': 'str', 'FirstName': 'str', \n",
    "        'MiddleInitial': 'str', 'Gender': 'str', 'AddressLine1': 'str', \n",
    "        'AddressLine2': 'str', 'PostalCode': 'str', 'City': 'str', \n",
    "        'State': 'str', 'Country': 'str', 'Phone': 'str', \n",
    "        'Income': 'Int64', 'NumberCars': 'Int8', 'NumberChildren': 'Int8', \n",
    "        'MaritalStatus': 'str', 'Age': 'Int8', 'CreditRating': 'Int16', \n",
    "        'OwnOrRentFlag': 'str', 'Employer': 'str', \n",
    "        'NumberCreditCards': 'Int8', 'NetWorth': 'Int64'\n",
    "    }\n",
    "\n",
    "    # Read the CSV file\n",
    "    raw_prospect_df = pd.read_csv(\n",
    "        filepath, \n",
    "        header=None, \n",
    "        names=columns, \n",
    "        dtype=dtypes\n",
    "    )\n",
    "\n",
    "    return raw_prospect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prospect_df = read_prospect_file(DATA_DIR + \"Prospect.csv\")\n",
    "raw_prospect_df.info()\n",
    "raw_prospect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_cols = ['LastName', 'FirstName', 'City', 'State']\n",
    "for col in not_null_cols:\n",
    "    raw_prospect_df.loc[raw_prospect_df[col].isna(), col] = ''\n",
    "raw_prospect_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'AgencyID': 'str',\n",
    "    'SK_RecordDateID': 'uint32',\n",
    "    'SK_UpdateDateID': 'uint32',\n",
    "    'BatchID': 'uint16',\n",
    "    'IsCustomer': 'boolean',\n",
    "    'LastName': 'str',\n",
    "    'FirstName': 'str',\n",
    "    'MiddleInitial': 'str',\n",
    "    'Gender': 'str',\n",
    "    'AddressLine1': 'str',\n",
    "    'AddressLine2': 'str',\n",
    "    'PostalCode': 'str',\n",
    "    'City': 'str',\n",
    "    'State': 'str',\n",
    "    'Country': 'str',\n",
    "    'Phone': 'str',\n",
    "    'Income': 'uint32',\n",
    "    'NumberCars': 'uint8',\n",
    "    'NumberChildren': 'uint8',\n",
    "    'MaritalStatus': 'str',\n",
    "    'Age': 'uint8',\n",
    "    'CreditRating': 'uint16',\n",
    "    'OwnOrRentFlag': 'str',\n",
    "    'Employer': 'str',\n",
    "    'NumberCreditCards': 'uint8',\n",
    "    'NetWorth': 'int64',\n",
    "    'MarketingNameplate': 'str'\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame with the specified schema\n",
    "prospect_df = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in dtypes.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df[\"AgencyID\"] = raw_prospect_df[\"AgencyID\"]\n",
    "prospect_df[\"LastName\"] = raw_prospect_df[\"LastName\"]\n",
    "prospect_df[\"FirstName\"] = raw_prospect_df[\"FirstName\"]\n",
    "prospect_df[\"MiddleInitial\"] = raw_prospect_df[\"MiddleInitial\"]\n",
    "prospect_df[\"Gender\"] = raw_prospect_df[\"Gender\"]\n",
    "# fix data quality issues\n",
    "prospect_df['Gender'] = prospect_df['Gender'].str.upper()\n",
    "mask = ~prospect_df['Gender'].isin([\"M\", \"F\"])\n",
    "prospect_df.loc[mask, \"Gender\"] = \"U\"\n",
    "prospect_df[\"AddressLine1\"] = raw_prospect_df[\"AddressLine1\"]\n",
    "prospect_df[\"AddressLine2\"] = raw_prospect_df[\"AddressLine2\"]\n",
    "prospect_df[\"PostalCode\"] = raw_prospect_df[\"PostalCode\"]\n",
    "prospect_df[\"City\"] = raw_prospect_df[\"City\"]\n",
    "prospect_df[\"State\"] = raw_prospect_df[\"State\"]\n",
    "prospect_df[\"Country\"] = raw_prospect_df[\"Country\"]\n",
    "prospect_df[\"Phone\"] = raw_prospect_df[\"Phone\"]\n",
    "prospect_df[\"Income\"] = raw_prospect_df[\"Income\"]\n",
    "prospect_df[\"NumberCars\"] = raw_prospect_df[\"NumberCars\"]\n",
    "prospect_df[\"NumberChildren\"] = raw_prospect_df[\"NumberChildren\"]\n",
    "prospect_df[\"MaritalStatus\"] = raw_prospect_df[\"MaritalStatus\"]\n",
    "prospect_df[\"MaritalStatus\"] = prospect_df[\"MaritalStatus\"].str.upper()\n",
    "mask = ~prospect_df[\"MaritalStatus\"].isin([\"S\", \"M\", \"D\", \"W\"])\n",
    "prospect_df.loc[mask, \"MaritalStatus\"] = \"U\"\n",
    "prospect_df[\"Age\"] = raw_prospect_df[\"Age\"]\n",
    "prospect_df[\"CreditRating\"] = raw_prospect_df[\"CreditRating\"]\n",
    "prospect_df[\"OwnOrRentFlag\"] = raw_prospect_df[\"OwnOrRentFlag\"]\n",
    "prospect_df[\"OwnOrRentFlag\"] = prospect_df[\"OwnOrRentFlag\"].str.upper()\n",
    "mask = ~prospect_df[\"OwnOrRentFlag\"].isin([\"O\", \"R\"])\n",
    "prospect_df.loc[mask, \"OwnOrRentFlag\"] = \"U\"\n",
    "prospect_df[\"Employer\"] = raw_prospect_df[\"Employer\"]\n",
    "prospect_df[\"NumberCreditCards\"] = raw_prospect_df[\"NumberCreditCards\"]\n",
    "prospect_df[\"NetWorth\"] = raw_prospect_df[\"NetWorth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_dateid = pd.read_sql_query(f\"select SK_DateID from dimdate where DateValue = '{BATCH_DATE}'\", engine).iloc[0, 0]\n",
    "# SK_RecordDateID is set to the DimDate SK_DateID field that corresponds to the Batch Date.\n",
    "prospect_df['SK_RecordDateID'] = sk_dateid\n",
    "# SK_UpdateDateID is set to the DimDate SK_DateID field that corresponds to the Batch Date\n",
    "prospect_df['SK_UpdateDateID'] = sk_dateid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions for each tag with null checks\n",
    "conditions = {\n",
    "    \"HighValue\": (prospect_df[\"NetWorth\"].notnull() & prospect_df[\"Income\"].notnull())\n",
    "    & ((prospect_df[\"NetWorth\"] > 1_000_000) | (prospect_df[\"Income\"] > 200_000)),\n",
    "    \"Expenses\": (\n",
    "        prospect_df[\"NumberChildren\"].notnull()\n",
    "        & prospect_df[\"NumberCreditCards\"].notnull()\n",
    "    )\n",
    "    & ((prospect_df[\"NumberChildren\"] > 3) | (prospect_df[\"NumberCreditCards\"] > 5)),\n",
    "    \"Boomer\": prospect_df[\"Age\"].notnull() & (prospect_df[\"Age\"] > 45),\n",
    "    \"MoneyAlert\": (\n",
    "        prospect_df[\"Income\"].notnull()\n",
    "        & prospect_df[\"CreditRating\"].notnull()\n",
    "        & prospect_df[\"NetWorth\"].notnull()\n",
    "    )\n",
    "    & (\n",
    "        (prospect_df[\"Income\"] < 50_000)\n",
    "        | (prospect_df[\"CreditRating\"] < 600)\n",
    "        | (prospect_df[\"NetWorth\"] < 100_000)\n",
    "    ),\n",
    "    \"Spender\": (\n",
    "        prospect_df[\"NumberCars\"].notnull() & prospect_df[\"NumberCreditCards\"].notnull()\n",
    "    )\n",
    "    & ((prospect_df[\"NumberCars\"] > 3) | (prospect_df[\"NumberCreditCards\"] > 7)),\n",
    "    \"Inherited\": (prospect_df[\"Age\"].notnull() & prospect_df[\"NetWorth\"].notnull())\n",
    "    & ((prospect_df[\"Age\"] < 25) & (prospect_df[\"NetWorth\"] > 1_000_000)),\n",
    "}\n",
    "\n",
    "# Apply conditions to assign tags\n",
    "prospect_df[\"MarketingNameplate\"] = \"\"\n",
    "for tag, condition in conditions.items():\n",
    "    prospect_df[\"MarketingNameplate\"] += np.where(condition, tag + \"+\", \"\")\n",
    "\n",
    "# Remove trailing '+' and replace empty strings with None\n",
    "prospect_df[\"MarketingNameplate\"] = (\n",
    "    prospect_df[\"MarketingNameplate\"].str.rstrip(\"+\").replace(\"\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IsCurrent and BatchID are set after processing dimCustomer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimCustomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_DIR + \"CustomerMgmt.xml\"\n",
    "tree = etree.parse(data_file)\n",
    "namespace = {'tpcdi': 'http://www.tpc.org/tpc-di'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'SK_CustomerID': 'int32',\n",
    "    'CustomerID': 'int32',\n",
    "    'TaxID': 'str',\n",
    "    'Status': 'str',\n",
    "    'LastName': 'str',\n",
    "    'FirstName': 'str',\n",
    "    'MiddleInitial': 'str',\n",
    "    'Gender': 'str',\n",
    "    'Tier': 'UInt8',\n",
    "    'DOB': 'datetime64[ns]',\n",
    "    'AddressLine1': 'str',\n",
    "    'AddressLine2': 'str',\n",
    "    'PostalCode': 'str',\n",
    "    'City': 'str',\n",
    "    'StateProv': 'str',\n",
    "    'Country': 'str',\n",
    "    'Phone1': 'str',\n",
    "    'Phone2': 'str',\n",
    "    'Phone3': 'str',\n",
    "    'Email1': 'str',\n",
    "    'Email2': 'str',\n",
    "    'NationalTaxRateDesc': 'str',\n",
    "    'NationalTaxRate': 'Float64',\n",
    "    'LocalTaxRateDesc': 'str',\n",
    "    'LocalTaxRate': 'Float64',\n",
    "    'AgencyID': 'str',\n",
    "    'CreditRating': 'UInt16',\n",
    "    'NetWorth': 'Float64',\n",
    "    'MarketingNameplate': 'str',\n",
    "    'IsCurrent': 'boolean',\n",
    "    'BatchID': 'uint8',\n",
    "    'EffectiveDate': 'datetime64[ns]',\n",
    "    'EndDate': 'datetime64[ns]'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_phone_number(phone_element):\n",
    "    # Extract components of the phone number\n",
    "    ctry_code = phone_element.findtext('C_CTRY_CODE', default=None, namespaces=namespace)\n",
    "    area_code = phone_element.findtext('C_AREA_CODE', default=None, namespaces=namespace)\n",
    "    local = phone_element.findtext('C_LOCAL', default=None, namespaces=namespace)\n",
    "    ext = phone_element.findtext('C_EXT', default=None, namespaces=namespace)\n",
    "\n",
    "    # Apply transformation rules\n",
    "    if ctry_code and area_code and local:\n",
    "        phone = f\"+{ctry_code} ({area_code}) {local}\"\n",
    "    elif area_code and local:\n",
    "        phone = f\"({area_code}) {local}\"\n",
    "    elif local:\n",
    "        phone = local\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Add extension if present\n",
    "    if ext:\n",
    "        phone += ext\n",
    "\n",
    "    return phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tax_info(tax_ids):\n",
    "    tax_ids_str = \"','\".join(tax_ids)\n",
    "    query = f\"SELECT TX_ID, TX_NAME, TX_RATE FROM taxrate WHERE TX_ID IN ('{tax_ids_str}')\"\n",
    "    result = pd.read_sql_query(query, engine)\n",
    "    return result.set_index('TX_ID').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the latest index of 'UPDCUST' or 'INACT' for each CustomerID\n",
    "latest_updates = {}\n",
    "\n",
    "# Get all actions\n",
    "all_actions = tree.xpath(\".//tpcdi:Action\", namespaces=namespace)\n",
    "# NEW actions\n",
    "new_actions = [action for action in all_actions if action.get('ActionType') == 'NEW']\n",
    "# UPD actions\n",
    "upd_actions = [action for action in all_actions if action.get('ActionType') == 'UPDCUST']\n",
    "# INACT actions\n",
    "inact_actions = [action for action in all_actions if action.get('ActionType') == 'INACT']\n",
    "\n",
    "# Preprocess to fill the dictionary\n",
    "for i, action in enumerate(all_actions):\n",
    "    if action.get('ActionType') in ['UPDCUST', 'INACT']:\n",
    "        customer = action.find('Customer', namespaces=namespace)\n",
    "        customer_id = customer.get('C_ID', None)\n",
    "        if customer_id:\n",
    "            latest_updates[int(customer_id)] = i\n",
    "\n",
    "# Modified has_later_update function\n",
    "def has_later_update(customer_id, current_index):\n",
    "    \"\"\"Check for subsequent 'UPDCUST' or 'INACT' actions for a given CustomerID at current_index\"\"\"\n",
    "    return latest_updates.get(customer_id, -1) > current_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified schema\n",
    "dimCustomer_df = pd.DataFrame(\n",
    "    {col: pd.Series(dtype=typ) for col, typ in dtypes.items()}\n",
    ")\n",
    "\n",
    "# Initialize lists to store tax IDs for each record\n",
    "national_tax_ids = []\n",
    "local_tax_ids = []\n",
    "\n",
    "# temporary prospect_df for matching\n",
    "prospect_df_temp = prospect_df[\n",
    "    [\n",
    "        \"AgencyID\",\n",
    "        \"CreditRating\",\n",
    "        \"NetWorth\",\n",
    "        \"MarketingNameplate\",\n",
    "        \"LastName\",\n",
    "        \"FirstName\",\n",
    "        \"AddressLine1\",\n",
    "        \"AddressLine2\",\n",
    "        \"PostalCode\",\n",
    "    ]\n",
    "].copy()\n",
    "prospect_df_temp[\"LastName\"] = prospect_df_temp[\"LastName\"].str.upper()\n",
    "prospect_df_temp[\"FirstName\"] = prospect_df_temp[\"FirstName\"].str.upper()\n",
    "prospect_df_temp[\"AddressLine1\"] = prospect_df_temp[\"AddressLine1\"].str.upper()\n",
    "prospect_df_temp[\"AddressLine2\"] = prospect_df_temp[\"AddressLine2\"].str.upper()\n",
    "prospect_df_temp[\"PostalCode\"] = prospect_df_temp[\"PostalCode\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for NEW actions\n",
    "data = {\n",
    "    \"CustomerID\": [],\n",
    "    \"TaxID\": [],\n",
    "    \"LastName\": [],\n",
    "    \"FirstName\": [],\n",
    "    \"MiddleInitial\": [],\n",
    "    \"Tier\": [],\n",
    "    \"DOB\": [],\n",
    "    \"Gender\": [],\n",
    "    \"Email1\": [],\n",
    "    \"Email2\": [],\n",
    "    \"AddressLine1\": [],\n",
    "    \"AddressLine2\": [],\n",
    "    \"PostalCode\": [],\n",
    "    \"City\": [],\n",
    "    \"StateProv\": [],\n",
    "    \"Country\": [],\n",
    "    \"Phone1\": [],\n",
    "    \"Phone2\": [],\n",
    "    \"Phone3\": [],\n",
    "    \"NationalTaxRateDesc\": [],\n",
    "    \"NationalTaxRate\": [],\n",
    "    \"LocalTaxRateDesc\": [],\n",
    "    \"LocalTaxRate\": [],\n",
    "    \"AgencyID\": [],\n",
    "    \"CreditRating\": [],\n",
    "    \"NetWorth\": [],\n",
    "    \"MarketingNameplate\": [],\n",
    "    \"EffectiveDate\": [],\n",
    "}\n",
    "\n",
    "# Iterate through each 'Action' element with ActionType=\"NEW\"\n",
    "for index, action in enumerate(tqdm(new_actions)):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    name = customer.find(\"Name\", namespaces=namespace)\n",
    "    contact_info = customer.find(\"ContactInfo\", namespaces=namespace)\n",
    "    address = customer.find(\"Address\", namespaces=namespace)\n",
    "    tax_info = customer.find(\"TaxInfo\", namespaces=namespace)\n",
    "\n",
    "    customer_id = customer.get(\"C_ID\", None)\n",
    "    customer_id = int(customer_id) if customer_id else None\n",
    "    data[\"CustomerID\"].append(customer_id)\n",
    "    data[\"TaxID\"].append(customer.get(\"C_TAX_ID\", None))\n",
    "    tier = customer.get(\"C_TIER\", None)\n",
    "    tier = int(tier) if tier else None\n",
    "    if tier is not None and tier not in (1,2,3):\n",
    "        \"\"\"\n",
    "        A record will be inserted in the DImessages table if a customer's Tier is not one of the valid\n",
    "        values (1,2,3). The MessageSource is “DimCustomer”, the MessageType is “Alert” and the\n",
    "        MessageText is “Invalid customer tier”. The MessageData field is “C_ID = ” followed by the\n",
    "        natural key value of the record, then “, C_TIER = ” and the C_TIER value.\n",
    "        \"\"\"\n",
    "        MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "        sk_customer_id = len(data[\"CustomerID\"])\n",
    "        message = f\"C_ID = {sk_customer_id}, C_TIER = {tier}\"\n",
    "        message_source = \"DimCustomer\"\n",
    "        message_type = \"Alert\"\n",
    "        message_text = \"Invalid customer tier\"\n",
    "        query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "        VALUES ('{MessageDateAndTime}', {BATCH_ID}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(query))\n",
    "            conn.commit()\n",
    "\n",
    "    data[\"Tier\"].append(tier)\n",
    "    dob = customer.get(\"C_DOB\", None)\n",
    "    dob = pd.to_datetime(dob, format=\"%Y-%m-%d\") if dob else None\n",
    "    \"\"\"A record will be reported in the DImessages table if a customer's DOB is invalid. A customer's\n",
    "    DOB is invalid if DOB < Batch Date - 100 years or DOB > Batch Date (customer is over 100\n",
    "    years old or born in the future). The MessageSource is “DimCustomer”, the MessageType is\n",
    "    “Alert” and the MessageText is “DOB out of range”. The MessageData field is “C_ID = ”\n",
    "    followed by the natural key value of the record, then “, C_DOB = ” and the C_DOB value.\"\"\"\n",
    "    if dob and (dob < BATCH_DATE - pd.Timedelta(days=100*365) or dob > BATCH_DATE):\n",
    "        MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "        batch_id = 1\n",
    "        sk_customer_id = len(data[\"CustomerID\"])\n",
    "        message = f\"C_ID = {sk_customer_id}, C_DOB = {dob}\"\n",
    "        message_source = \"DimCustomer\"\n",
    "        message_type = \"Alert\"\n",
    "        message_text = \"DOB out of range\"\n",
    "        query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "        VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(query))\n",
    "            conn.commit()\n",
    "    data[\"DOB\"].append(dob)\n",
    "    gender = customer.get(\"C_GNDR\", \"U\")\n",
    "    if gender is not None:\n",
    "        gender = gender.upper()\n",
    "    gender = \"U\" if gender not in (\"M\", \"F\") else gender\n",
    "    data[\"Gender\"].append(gender)\n",
    "    \n",
    "    first_name = name.findtext(\"C_F_NAME\", default=None, namespaces=namespace)\n",
    "    data[\"FirstName\"].append(first_name if first_name else None)\n",
    "    middle_initial = name.findtext(\"C_M_NAME\", default=None, namespaces=namespace)\n",
    "    data[\"MiddleInitial\"].append(middle_initial if middle_initial else None)\n",
    "    last_name = name.findtext(\"C_L_NAME\", default=None, namespaces=namespace)\n",
    "    data[\"LastName\"].append(last_name if last_name else None)\n",
    "\n",
    "    \n",
    "    prim_email = contact_info.findtext(\n",
    "        \"C_PRIM_EMAIL\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"Email1\"].append(prim_email if prim_email else None)\n",
    "    alt_email = contact_info.findtext(\n",
    "        \"C_ALT_EMAIL\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"Email2\"].append(alt_email if alt_email else None)\n",
    "    data[\"Phone1\"].append(\n",
    "        format_phone_number(contact_info.find(\"C_PHONE_1\", namespaces=namespace))\n",
    "    )\n",
    "    data[\"Phone2\"].append(\n",
    "        format_phone_number(contact_info.find(\"C_PHONE_2\", namespaces=namespace))\n",
    "    )\n",
    "    data[\"Phone3\"].append(\n",
    "        format_phone_number(contact_info.find(\"C_PHONE_3\", namespaces=namespace))\n",
    "    )\n",
    "\n",
    "    # Extracting address information\n",
    "    address_line1 = address.findtext(\n",
    "        \"C_ADLINE1\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"AddressLine1\"].append(address_line1 if address_line1 else None)\n",
    "    address_line2 = address.findtext(\n",
    "        \"C_ADLINE2\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"AddressLine2\"].append(address_line2 if address_line2 else None)\n",
    "    postalcode = address.findtext(\"C_ZIPCODE\", default=None, namespaces=namespace)\n",
    "    data[\"PostalCode\"].append(postalcode if postalcode else None)\n",
    "    city = address.findtext(\"C_CITY\", default=None, namespaces=namespace)\n",
    "    data[\"City\"].append(city if city else None)\n",
    "    state_prov = address.findtext(\n",
    "        \"C_STATE_PROV\", default=None, namespaces=namespace\n",
    "    )\n",
    "    data[\"StateProv\"].append(state_prov if state_prov else None)\n",
    "    country = address.findtext(\"C_CTRY\", default=None, namespaces=namespace)\n",
    "    data[\"Country\"].append(country if country else None)\n",
    "    \n",
    "    # Store TX_ID as placeholders\n",
    "    national_tax_id = tax_info.findtext(\n",
    "        \"C_NAT_TX_ID\", default=None, namespaces=namespace\n",
    "    )\n",
    "    national_tax_id = national_tax_id if national_tax_id else None\n",
    "    national_tax_ids.append(national_tax_id)\n",
    "    local_tax_id = tax_info.findtext(\n",
    "        \"C_LCL_TX_ID\", default=None, namespaces=namespace\n",
    "    )\n",
    "    local_tax_id = local_tax_id if local_tax_id else None\n",
    "    local_tax_ids.append(local_tax_id)\n",
    "\n",
    "    if not has_later_update(customer_id, index):\n",
    "        # Find matching prospect record\n",
    "        match = prospect_df_temp[\n",
    "            (prospect_df_temp[\"LastName\"] == last_name.upper())\n",
    "            & (prospect_df_temp[\"FirstName\"] == first_name.upper())\n",
    "            & (prospect_df_temp[\"AddressLine1\"] == address_line1.upper())\n",
    "            & (prospect_df_temp[\"AddressLine2\"] == address_line2.upper())\n",
    "            & (prospect_df_temp[\"PostalCode\"] == postalcode.upper())\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            # Set values from the matching prospect record\n",
    "            data[\"AgencyID\"].append(match[\"AgencyID\"].iloc[-1])\n",
    "            data[\"CreditRating\"].append(match[\"CreditRating\"].iloc[-1])\n",
    "            data[\"NetWorth\"].append(match[\"NetWorth\"].iloc[-1])\n",
    "            data[\"MarketingNameplate\"].append(match[\"MarketingNameplate\"].iloc[-1])\n",
    "        else:\n",
    "            # Set values to NULL\n",
    "            data[\"AgencyID\"].append(None)\n",
    "            data[\"CreditRating\"].append(None)\n",
    "            data[\"NetWorth\"].append(None)\n",
    "            data[\"MarketingNameplate\"].append(None)\n",
    "    else:\n",
    "        # Set values to NULL due to later 'UPDCUST' or 'INACT'\n",
    "        data[\"AgencyID\"].append(None)\n",
    "        data[\"CreditRating\"].append(None)\n",
    "        data[\"NetWorth\"].append(None)\n",
    "        data[\"MarketingNameplate\"].append(None)\n",
    "    # history tracking\n",
    "    data[\"EffectiveDate\"].append(pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "# Get unique TX_IDs and remove None values \n",
    "unique_tax_ids = set(national_tax_ids + local_tax_ids) - {None}\n",
    "all_tax_info = get_all_tax_info(unique_tax_ids)\n",
    "\n",
    "# map each tax ID to its description and rate\n",
    "for i in range(len(national_tax_ids)):\n",
    "    national_info = all_tax_info.get(\n",
    "        national_tax_ids[i], {\"TX_NAME\": None, \"TX_RATE\": None}\n",
    "    )\n",
    "    data[\"NationalTaxRateDesc\"].append(national_info[\"TX_NAME\"])\n",
    "    data[\"NationalTaxRate\"].append(national_info[\"TX_RATE\"])\n",
    "\n",
    "    local_info = all_tax_info.get(local_tax_ids[i], {\"TX_NAME\": None, \"TX_RATE\": None})\n",
    "    data[\"LocalTaxRateDesc\"].append(local_info[\"TX_NAME\"])\n",
    "    data[\"LocalTaxRate\"].append(local_info[\"TX_RATE\"])\n",
    "\n",
    "# Creating DataFrame\n",
    "dimCustomer_df = pd.concat([dimCustomer_df, pd.DataFrame(data)])\n",
    "dimCustomer_df[\"Status\"] = \"ACTIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2dict(df, exclude_columns):\n",
    "    # Remove the specified columns from the DataFrame\n",
    "    df_filtered = df.drop(columns=exclude_columns)\n",
    "    # Convert the filtered DataFrame to a dictionary\n",
    "    df_dict = df_filtered.to_dict(orient='index')\n",
    "    # Create a new dictionary that maps CustomerID to a dictionary of column values\n",
    "    customer_dict = {row['CustomerID']: {col: val for col, val in row.items() if col != 'CustomerID'} for _, row in df_dict.items()}\n",
    "    return customer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['SK_CustomerID', 'IsCurrent', 'BatchID', 'EffectiveDate', 'EndDate', 'Status']\n",
    "# dictionary to track latest values for each customer\n",
    "customer_data = df2dict(dimCustomer_df, exclude_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for NEW actions\n",
    "data = {col: [] for col in data.keys()}\n",
    "\n",
    "# Iterate through each 'Action' element with ActionType=\"UPDCUST\"\n",
    "for index, action in enumerate(upd_actions):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    name = customer.find(\"Name\", namespaces=namespace)\n",
    "    contact_info = customer.find(\"ContactInfo\", namespaces=namespace)\n",
    "    address = customer.find(\"Address\", namespaces=namespace)\n",
    "    tax_info = customer.find(\"TaxInfo\", namespaces=namespace)\n",
    "\n",
    "    customer_id = int(customer.get(\"C_ID\", None))\n",
    "    data[\"CustomerID\"].append(customer_id)\n",
    "\n",
    "    # Update tax_id\n",
    "    tax_id = customer.get(\"C_TAX_ID\", None)\n",
    "    if tax_id is None:\n",
    "        tax_id = customer_data[customer_id][\"TaxID\"]\n",
    "    else:\n",
    "        customer_data[customer_id][\"TaxID\"] = tax_id\n",
    "    data[\"TaxID\"].append(tax_id)\n",
    "    # Update tier\n",
    "    tier = customer.get(\"C_TIER\", None)\n",
    "    tier = int(tier) if tier else None\n",
    "    if tier is None:\n",
    "        tier = customer_data[customer_id][\"Tier\"]\n",
    "    else:\n",
    "        customer_data[customer_id][\"Tier\"] = tier\n",
    "        if tier is not None and tier not in (1,2,3):\n",
    "            \"\"\"\n",
    "            A record will be inserted in the DImessages table if a customer's Tier is not one of the valid\n",
    "            values (1,2,3). The MessageSource is “DimCustomer”, the MessageType is “Alert” and the\n",
    "            MessageText is “Invalid customer tier”. The MessageData field is “C_ID = ” followed by the\n",
    "            natural key value of the record, then “, C_TIER = ” and the C_TIER value.\n",
    "            \"\"\"\n",
    "            MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "            batch_id = 1\n",
    "            sk_customer_id = len(data[\"CustomerID\"]) + dimCustomer_df.shape[0]\n",
    "            message = f\"C_ID = {sk_customer_id}, C_TIER = {tier}\"\n",
    "            message_source = \"DimCustomer\"\n",
    "            message_type = \"Alert\"\n",
    "            message_text = \"Invalid customer tier\"\n",
    "            query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(query))\n",
    "                conn.commit()\n",
    "    data[\"Tier\"].append(tier)\n",
    "    # Update DOB\n",
    "    dob = customer.get(\"C_DOB\", None)\n",
    "    dob = pd.to_datetime(dob, format=\"%Y-%m-%d\") if dob else None\n",
    "    if dob is None:\n",
    "        dob = customer_data[customer_id][\"DOB\"]\n",
    "    else:\n",
    "        customer_data[customer_id][\"DOB\"] = dob\n",
    "        \"\"\"A record will be reported in the DImessages table if a customer's DOB is invalid. A customer's\n",
    "        DOB is invalid if DOB < Batch Date - 100 years or DOB > Batch Date (customer is over 100\n",
    "        years old or born in the future). The MessageSource is “DimCustomer”, the MessageType is\n",
    "        “Alert” and the MessageText is “DOB out of range”. The MessageData field is “C_ID = ”\n",
    "        followed by the natural key value of the record, then “, C_DOB = ” and the C_DOB value.\"\"\"\n",
    "        if dob and (dob < BATCH_DATE - pd.Timedelta(days=100*365) or dob > BATCH_DATE):\n",
    "            MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "            batch_id = 1\n",
    "            sk_customer_id = len(data[\"CustomerID\"])\n",
    "            message = f\"C_ID = {sk_customer_id}, C_DOB = {dob}\"\n",
    "            message_source = \"DimCustomer\"\n",
    "            message_type = \"Alert\"\n",
    "            message_text = \"DOB out of range\"\n",
    "            query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{message}')\"\"\"\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(query))\n",
    "                conn.commit()\n",
    "    data[\"DOB\"].append(dob)\n",
    "    # Update gender\n",
    "    gender = customer.get(\"C_GNDR\", None)\n",
    "    if gender is None:\n",
    "        gender = customer_data[customer_id][\"Gender\"]\n",
    "    else:\n",
    "        gender = gender.upper()\n",
    "        gender = \"U\" if gender not in (\"M\", \"F\") else gender\n",
    "        customer_data[customer_id][\"Gender\"] = gender\n",
    "    data[\"Gender\"].append(gender)\n",
    "\n",
    "    # Update first name\n",
    "    if name is None:\n",
    "        first_name = customer_data[customer_id][\"FirstName\"]\n",
    "        data[\"FirstName\"].append(first_name)\n",
    "        middle_initial = customer_data[customer_id][\"MiddleInitial\"]\n",
    "        data[\"MiddleInitial\"].append(middle_initial)\n",
    "        last_name = customer_data[customer_id][\"LastName\"]\n",
    "        data[\"LastName\"].append(last_name)\n",
    "    else:\n",
    "        first_name = name.findtext(\"C_F_NAME\", default=None, namespaces=namespace)\n",
    "        if first_name is None:\n",
    "            first_name = customer_data[customer_id][\"FirstName\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"FirstName\"] = first_name\n",
    "        data[\"FirstName\"].append(first_name)\n",
    "        # Update middle initial\n",
    "        middle_initial = name.findtext(\"C_M_NAME\", default=None, namespaces=namespace)\n",
    "        if middle_initial is None:\n",
    "            middle_initial = customer_data[customer_id][\"MiddleInitial\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"MiddleInitial\"] = middle_initial\n",
    "        data[\"MiddleInitial\"].append(middle_initial)\n",
    "        # Update last name\n",
    "        last_name = name.findtext(\"C_L_NAME\", default=None, namespaces=namespace)\n",
    "        if last_name is None:\n",
    "            last_name = customer_data[customer_id][\"LastName\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"LastName\"] = last_name\n",
    "        data[\"LastName\"].append(last_name if last_name else None)\n",
    "\n",
    "    if contact_info is None:\n",
    "        prim_email = customer_data[customer_id][\"Email1\"]\n",
    "        data[\"Email1\"].append(prim_email)\n",
    "        alt_email = customer_data[customer_id][\"Email2\"]\n",
    "        data[\"Email2\"].append(alt_email)\n",
    "        phone1 = customer_data[customer_id][\"Phone1\"]\n",
    "        data[\"Phone1\"].append(phone1)\n",
    "        phone2 = customer_data[customer_id][\"Phone2\"]\n",
    "        data[\"Phone2\"].append(phone2)\n",
    "        phone3 = customer_data[customer_id][\"Phone3\"]\n",
    "        data[\"Phone3\"].append(phone3)\n",
    "    else:\n",
    "        # update primary email\n",
    "        prim_email = contact_info.findtext(\n",
    "            \"C_PRIM_EMAIL\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if prim_email is None:\n",
    "            prim_email = customer_data[customer_id][\"Email1\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"Email1\"] = prim_email\n",
    "        data[\"Email1\"].append(prim_email if prim_email else None)\n",
    "        # update alternate email\n",
    "        alt_email = contact_info.findtext(\n",
    "            \"C_ALT_EMAIL\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if alt_email is None:\n",
    "            alt_email = customer_data[customer_id][\"Email2\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"Email2\"] = alt_email\n",
    "        data[\"Email2\"].append(alt_email if alt_email else None)\n",
    "        # update phone numbers\n",
    "        phone1 = contact_info.find(\"C_PHONE_1\", namespaces=namespace)\n",
    "        if phone1 is None:\n",
    "            phone1 = customer_data[customer_id][\"Phone1\"]\n",
    "        else:\n",
    "            phone1 = format_phone_number(phone1)\n",
    "            customer_data[customer_id][\"Phone1\"] = phone1\n",
    "        data[\"Phone1\"].append(phone1)\n",
    "        phone2 = contact_info.find(\"C_PHONE_2\", namespaces=namespace)\n",
    "        if phone2 is None:\n",
    "            phone2 = customer_data[customer_id][\"Phone2\"]\n",
    "        else:\n",
    "            phone2 = format_phone_number(phone2)\n",
    "            customer_data[customer_id][\"Phone2\"] = phone2\n",
    "        data[\"Phone2\"].append(phone2)\n",
    "        phone3 = contact_info.find(\"C_PHONE_3\", namespaces=namespace)\n",
    "        if phone3 is None:\n",
    "            phone3 = customer_data[customer_id][\"Phone3\"]\n",
    "        else:\n",
    "            phone3 = format_phone_number(phone3)\n",
    "            customer_data[customer_id][\"Phone3\"] = phone3\n",
    "        data[\"Phone3\"].append(phone3)\n",
    "\n",
    "    if address is None:\n",
    "        address_line1 = customer_data[customer_id][\"AddressLine1\"]\n",
    "        data[\"AddressLine1\"].append(address_line1)\n",
    "        address_line2 = customer_data[customer_id][\"AddressLine2\"]\n",
    "        data[\"AddressLine2\"].append(address_line2)\n",
    "        postalcode = customer_data[customer_id][\"PostalCode\"]\n",
    "        data[\"PostalCode\"].append(postalcode)\n",
    "        city = customer_data[customer_id][\"City\"]\n",
    "        data[\"City\"].append(city)\n",
    "        state_prov = customer_data[customer_id][\"StateProv\"]\n",
    "        data[\"StateProv\"].append(state_prov)\n",
    "        country = customer_data[customer_id][\"Country\"]\n",
    "        data[\"Country\"].append(country)\n",
    "    else:\n",
    "        # Extracting address information\n",
    "        address_line1 = address.findtext(\n",
    "            \"C_ADLINE1\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if address_line1 is None:\n",
    "            address_line1 = customer_data[customer_id][\"AddressLine1\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"AddressLine1\"] = address_line1\n",
    "        data[\"AddressLine1\"].append(address_line1 if address_line1 else None)\n",
    "        address_line2 = address.findtext(\n",
    "            \"C_ADLINE2\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if address_line2 is None:\n",
    "            address_line2 = customer_data[customer_id][\"AddressLine2\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"AddressLine2\"] = address_line2\n",
    "        data[\"AddressLine2\"].append(address_line2 if address_line2 else None)\n",
    "        postalcode = address.findtext(\"C_ZIPCODE\", default=None, namespaces=namespace)\n",
    "        if postalcode is None:\n",
    "            postalcode = customer_data[customer_id][\"PostalCode\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"PostalCode\"] = postalcode\n",
    "        data[\"PostalCode\"].append(postalcode if postalcode else None)\n",
    "        city = address.findtext(\"C_CITY\", default=None, namespaces=namespace)\n",
    "        if city is None:\n",
    "            city = customer_data[customer_id][\"City\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"City\"] = city\n",
    "        data[\"City\"].append(city if city else None)\n",
    "        state_prov = address.findtext(\n",
    "            \"C_STATE_PROV\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if state_prov is None:\n",
    "            state_prov = customer_data[customer_id][\"StateProv\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"StateProv\"] = state_prov\n",
    "        data[\"StateProv\"].append(state_prov if state_prov else None)\n",
    "        country = address.findtext(\"C_CTRY\", default=None, namespaces=namespace)\n",
    "        if country is None:\n",
    "            country = customer_data[customer_id][\"Country\"]\n",
    "        else:\n",
    "            customer_data[customer_id][\"Country\"] = country\n",
    "        data[\"Country\"].append(country if country else None)\n",
    "    \n",
    "    # Store TX_ID as placeholders\n",
    "    if tax_info is None:\n",
    "        national_tax_rate_desc = customer_data[customer_id][\"NationalTaxRateDesc\"]\n",
    "        data[\"NationalTaxRateDesc\"].append(national_tax_rate_desc)\n",
    "        national_tax_rate = customer_data[customer_id][\"NationalTaxRate\"]\n",
    "        data[\"NationalTaxRate\"].append(national_tax_rate)\n",
    "        local_tax_rate_desc = customer_data[customer_id][\"LocalTaxRateDesc\"]\n",
    "        data[\"LocalTaxRateDesc\"].append(local_tax_rate_desc)\n",
    "        local_tax_rate = customer_data[customer_id][\"LocalTaxRate\"]\n",
    "        data[\"LocalTaxRate\"].append(local_tax_rate)\n",
    "        national_tax_ids.append(None)\n",
    "        local_tax_ids.append(None)        \n",
    "    else:\n",
    "        national_tax_id = tax_info.findtext(\n",
    "            \"C_NAT_TX_ID\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if national_tax_id is None:\n",
    "            national_tax_rate_desc = customer_data[customer_id][\"NationalTaxRateDesc\"]\n",
    "            data[\"NationalTaxRateDesc\"].append(national_tax_rate_desc)\n",
    "            national_tax_rate = customer_data[customer_id][\"NationalTaxRate\"]\n",
    "            data[\"NationalTaxRate\"].append(national_tax_rate)\n",
    "            national_tax_ids.append(None)\n",
    "        else:\n",
    "            result = pd.read_sql(f\"SELECT TX_NAME, TX_RATE FROM taxrate WHERE TX_ID = '{national_tax_id}'\", engine)\n",
    "            national_tax_rate_desc = result.iloc[0, 0]\n",
    "            national_tax_rate = result.iloc[0, 1]\n",
    "            customer_data[customer_id][\"NationalTaxRateDesc\"] = national_tax_rate_desc\n",
    "            customer_data[customer_id][\"NationalTaxRate\"] = national_tax_rate\n",
    "            data[\"NationalTaxRateDesc\"].append(national_tax_rate_desc)\n",
    "            data[\"NationalTaxRate\"].append(national_tax_rate)\n",
    "        local_tax_id = tax_info.findtext(\n",
    "            \"C_LCL_TX_ID\", default=None, namespaces=namespace\n",
    "        )\n",
    "        if local_tax_id is None:\n",
    "            local_tax_rate_desc = customer_data[customer_id][\"LocalTaxRateDesc\"]\n",
    "            data[\"LocalTaxRateDesc\"].append(local_tax_rate_desc)\n",
    "            local_tax_rate = customer_data[customer_id][\"LocalTaxRate\"]\n",
    "            data[\"LocalTaxRate\"].append(local_tax_rate)\n",
    "        else:\n",
    "            result = pd.read_sql(f\"SELECT TX_NAME, TX_RATE FROM taxrate WHERE TX_ID = '{local_tax_id}'\", engine)\n",
    "            local_tax_rate_desc = result.iloc[0, 0]\n",
    "            local_tax_rate = result.iloc[0, 1]\n",
    "            customer_data[customer_id][\"LocalTaxRateDesc\"] = local_tax_rate_desc\n",
    "            customer_data[customer_id][\"LocalTaxRate\"] = local_tax_rate\n",
    "            data[\"LocalTaxRateDesc\"].append(local_tax_rate_desc)\n",
    "            data[\"LocalTaxRate\"].append(local_tax_rate)\n",
    "\n",
    "\n",
    "    if not has_later_update(customer_id, index):\n",
    "        # Find matching prospect record\n",
    "        match = prospect_df[\n",
    "            (prospect_df_temp[\"LastName\"] == last_name.upper())\n",
    "            & (prospect_df_temp[\"FirstName\"] == first_name.upper())\n",
    "            & (prospect_df_temp[\"AddressLine1\"] == address_line1.upper())\n",
    "            & (prospect_df_temp[\"AddressLine2\"] == address_line2.upper())\n",
    "            & (prospect_df_temp[\"PostalCode\"] == postalcode.upper())\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            # Set values from the matching prospect record\n",
    "            data[\"AgencyID\"].append(match[\"AgencyID\"].iloc[-1])\n",
    "            data[\"CreditRating\"].append(match[\"CreditRating\"].iloc[-1])\n",
    "            data[\"NetWorth\"].append(match[\"NetWorth\"].iloc[-1])\n",
    "            data[\"MarketingNameplate\"].append(match[\"MarketingNameplate\"].iloc[-1])\n",
    "        else:\n",
    "            # Set values to those in customer_data\n",
    "            data[\"AgencyID\"].append(customer_data[customer_id][\"AgencyID\"])\n",
    "            data[\"CreditRating\"].append(customer_data[customer_id][\"CreditRating\"])\n",
    "            data[\"NetWorth\"].append(customer_data[customer_id][\"NetWorth\"])\n",
    "            data[\"MarketingNameplate\"].append(customer_data[customer_id][\"MarketingNameplate\"])\n",
    "    else:\n",
    "        # Set values to those in customer_data due to later 'UPDCUST' or 'INACT'\n",
    "        data[\"AgencyID\"].append(customer_data[customer_id][\"AgencyID\"])\n",
    "        data[\"CreditRating\"].append(customer_data[customer_id][\"CreditRating\"])\n",
    "        data[\"NetWorth\"].append(customer_data[customer_id][\"NetWorth\"])\n",
    "        data[\"MarketingNameplate\"].append(customer_data[customer_id][\"MarketingNameplate\"])\n",
    "    # history tracking\n",
    "    data[\"EffectiveDate\"].append(pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "# Creating DataFrame\n",
    "dimCustomer_df = pd.concat([dimCustomer_df, pd.DataFrame(data)])\n",
    "dimCustomer_df[\"Status\"] = \"ACTIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for NEW actions\n",
    "data = {col: [] for col in data.keys()}\n",
    "\n",
    "# Iterate through each 'Action' element with ActionType=\"INACT\"\n",
    "for index, action in enumerate(inact_actions):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    customer_id = int(customer.get(\"C_ID\", None))\n",
    "    data[\"CustomerID\"].append(customer_id)\n",
    "    # Copy all fields from customer_data\n",
    "    for col in data.keys():\n",
    "        if col in (\"CustomerID\", \"EffectiveDate\"):\n",
    "            continue\n",
    "        else:\n",
    "            data[col].append(customer_data[customer_id][col])\n",
    "    # history tracking\n",
    "    data[\"EffectiveDate\"].append(pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "# Creating DataFrame\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df[\"Status\"] = \"INACTIVE\"\n",
    "dimCustomer_df = pd.concat([dimCustomer_df, data_df])\n",
    "dimCustomer_df[\"BatchID\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimCustomer_df['SK_CustomerID'] = range(1, len(dimCustomer_df) + 1)\n",
    "dimCustomer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by CustomerID and EffectiveDate\n",
    "dimCustomer_df.sort_values(by=['CustomerID', 'EffectiveDate'], inplace=True)\n",
    "# Create a shifted DataFrame\n",
    "shifted_df = dimCustomer_df.shift(-1)\n",
    "# Update EndDate: If next row has same CustomerID, use its EffectiveDate; otherwise, use default date\n",
    "dimCustomer_df['EndDate'] = pd.Timestamp('9999-12-31')\n",
    "mask = dimCustomer_df['CustomerID'] == shifted_df['CustomerID']\n",
    "dimCustomer_df.loc[mask, 'EndDate'] = shifted_df.loc[mask, 'EffectiveDate']\n",
    "\n",
    "# Update IsCurrent: True if next row has different CustomerID or is the last row\n",
    "dimCustomer_df['IsCurrent'] = ~mask\n",
    "dimCustomer_df.sort_values(by=['SK_CustomerID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'CustomerID': sqlalchemy.types.Integer,\n",
    "    'TaxID': sqlalchemy.types.String(20),\n",
    "    'Status': sqlalchemy.types.String(10),\n",
    "    'LastName': sqlalchemy.types.String(30),\n",
    "    'FirstName': sqlalchemy.types.String(30),\n",
    "    'MiddleInitial': sqlalchemy.types.String(1),\n",
    "    'Gender': sqlalchemy.types.String(1),\n",
    "    'Tier': sqlalchemy.types.SmallInteger,\n",
    "    'DOB': sqlalchemy.types.Date,\n",
    "    'AddressLine1': sqlalchemy.types.String(80),\n",
    "    'AddressLine2': sqlalchemy.types.String(80),\n",
    "    'PostalCode': sqlalchemy.types.String(12),\n",
    "    'City': sqlalchemy.types.String(25),\n",
    "    'StateProv': sqlalchemy.types.String(20),\n",
    "    'Country': sqlalchemy.types.String(24),\n",
    "    'Phone1': sqlalchemy.types.String(30),\n",
    "    'Phone2': sqlalchemy.types.String(30),\n",
    "    'Phone3': sqlalchemy.types.String(30),\n",
    "    'Email1': sqlalchemy.types.String(50),\n",
    "    'Email2': sqlalchemy.types.String(50),\n",
    "    'NationalTaxRateDesc': sqlalchemy.types.String(50),\n",
    "    'NationalTaxRate': sqlalchemy.types.Numeric(6, 5),\n",
    "    'LocalTaxRateDesc': sqlalchemy.types.String(50),\n",
    "    'LocalTaxRate': sqlalchemy.types.Numeric(6, 5),\n",
    "    'AgencyID': sqlalchemy.types.String(30),\n",
    "    'CreditRating': sqlalchemy.types.SmallInteger,\n",
    "    'NetWorth': sqlalchemy.types.Numeric(10),\n",
    "    'MarketingNameplate': sqlalchemy.types.String(100),\n",
    "    'IsCurrent': sqlalchemy.types.Boolean,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'EffectiveDate': sqlalchemy.types.Date,\n",
    "    'EndDate': sqlalchemy.types.Date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int 32\n",
    "cols = ['SK_CustomerID', 'CustomerID', 'BatchID']\n",
    "for col in cols:\n",
    "    dimCustomer_df[col] = dimCustomer_df[col].astype('int')\n",
    "dimCustomer_df['Tier'] = dimCustomer_df['Tier'].astype('UInt8')\n",
    "dimCustomer_df['NationalTaxRate'] = dimCustomer_df['NationalTaxRate'].astype('float32')\n",
    "dimCustomer_df['LocalTaxRate'] = dimCustomer_df['LocalTaxRate'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame columns to the appropriate types\n",
    "dimCustomer_df['SK_CustomerID'] = dimCustomer_df['SK_CustomerID'].astype(np.int32)\n",
    "dimCustomer_df['CustomerID'] = dimCustomer_df['CustomerID'].astype(np.int32)\n",
    "dimCustomer_df['Tier'] = dimCustomer_df['Tier'].astype(pd.Int8Dtype())\n",
    "dimCustomer_df['NationalTaxRate'] = dimCustomer_df['NationalTaxRate'].astype(np.float64)\n",
    "dimCustomer_df['LocalTaxRate'] = dimCustomer_df['LocalTaxRate'].astype(np.float64)\n",
    "dimCustomer_df['CreditRating'] = dimCustomer_df['CreditRating'].astype(pd.Int16Dtype())\n",
    "dimCustomer_df['NetWorth'] = dimCustomer_df['NetWorth'].astype(pd.Float64Dtype())\n",
    "dimCustomer_df['BatchID'] = dimCustomer_df['BatchID'].astype(np.int16)\n",
    "\n",
    "# Convert date columns to datetime.date\n",
    "dimCustomer_df['DOB'] = pd.to_datetime(dimCustomer_df['DOB']).dt.date\n",
    "dimCustomer_df['EffectiveDate'] = pd.to_datetime(dimCustomer_df['EffectiveDate']).dt.date\n",
    "dimCustomer_df['EndDate'] = pd.to_datetime(dimCustomer_df['EndDate']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimCustomer (\n",
    "    SK_CustomerID INT UNSIGNED NOT NULL,\n",
    "    CustomerID INT UNSIGNED NOT NULL,\n",
    "    TaxID CHAR(20) NOT NULL,\n",
    "    Status CHAR(10) NOT NULL,\n",
    "    LastName CHAR(30) NOT NULL,\n",
    "    FirstName CHAR(30) NOT NULL,\n",
    "    MiddleInitial CHAR(1),\n",
    "    Gender CHAR(1),\n",
    "    Tier TINYINT UNSIGNED,\n",
    "    DOB DATE NOT NULL,\n",
    "    AddressLine1 CHAR(80) NOT NULL,\n",
    "    AddressLine2 CHAR(80),\n",
    "    PostalCode CHAR(12) NOT NULL,\n",
    "    City CHAR(25) NOT NULL,\n",
    "    StateProv CHAR(20) NOT NULL,\n",
    "    Country CHAR(24),\n",
    "    Phone1 CHAR(30),\n",
    "    Phone2 CHAR(30),\n",
    "    Phone3 CHAR(30),\n",
    "    Email1 CHAR(50),\n",
    "    Email2 CHAR(50),\n",
    "    NationalTaxRateDesc CHAR(50),\n",
    "    NationalTaxRate DECIMAL(6, 5),\n",
    "    LocalTaxRateDesc CHAR(50),\n",
    "    LocalTaxRate DECIMAL(6, 5),\n",
    "    AgencyID CHAR(30),\n",
    "    CreditRating SMALLINT UNSIGNED,\n",
    "    NetWorth DECIMAL(10),\n",
    "    MarketingNameplate CHAR(100),\n",
    "    IsCurrent BOOLEAN NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL,\n",
    "    EffectiveDate DATE NOT NULL,\n",
    "    EndDate DATE NOT NULL,\n",
    "    PRIMARY KEY (SK_CustomerID)\n",
    ");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimCustomer_df.to_sql('dimcustomer', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary uppercase columns for merging in both DataFrames\n",
    "merge_fields = [\"FirstName\", \"LastName\", \"AddressLine1\", \"AddressLine2\", \"PostalCode\"]\n",
    "for field in merge_fields:\n",
    "    prospect_df[f\"temp_{field}\"] = prospect_df[field].str.upper()\n",
    "    dimCustomer_df[f\"temp_{field}\"] = dimCustomer_df[field].str.upper()\n",
    "\n",
    "# Filter dimCustomer_df for active and current customers\n",
    "active_customers = dimCustomer_df[(dimCustomer_df['IsCurrent'] == True) & (dimCustomer_df['Status'] == 'ACTIVE')]\n",
    "\n",
    "# Perform an outer merge on the temporary uppercase fields\n",
    "temp_merge_fields = [f\"temp_{field}\" for field in merge_fields]\n",
    "merged_df = prospect_df.merge(active_customers, how='left', \n",
    "                              left_on=temp_merge_fields, right_on=temp_merge_fields,\n",
    "                              indicator=True)\n",
    "\n",
    "# Update IsCustomer based on whether a match was found\n",
    "prospect_df['IsCustomer'] = merged_df['_merge'] == 'both'\n",
    "\n",
    "# Clean up by dropping the temporary columns\n",
    "prospect_df.drop(columns=temp_merge_fields, inplace=True)\n",
    "dimCustomer_df.drop(columns=temp_merge_fields, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df['BatchID'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'AgencyID': sqlalchemy.types.CHAR(30),\n",
    "    'SK_RecordDateID': sqlalchemy.types.Integer,\n",
    "    'SK_UpdateDateID': sqlalchemy.types.Integer,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'IsCustomer': sqlalchemy.types.Boolean,\n",
    "    'LastName': sqlalchemy.types.CHAR(30),\n",
    "    'FirstName': sqlalchemy.types.CHAR(30),\n",
    "    'MiddleInitial': sqlalchemy.types.CHAR(1),\n",
    "    'Gender': sqlalchemy.types.CHAR(1),\n",
    "    'AddressLine1': sqlalchemy.types.CHAR(80),\n",
    "    'AddressLine2': sqlalchemy.types.CHAR(80),\n",
    "    'PostalCode': sqlalchemy.types.CHAR(12),\n",
    "    'City': sqlalchemy.types.CHAR(25),\n",
    "    'State': sqlalchemy.types.CHAR(20),\n",
    "    'Country': sqlalchemy.types.CHAR(24),\n",
    "    'Phone': sqlalchemy.types.CHAR(30),\n",
    "    'Income': sqlalchemy.types.Integer,\n",
    "    'NumberCars': sqlalchemy.types.SmallInteger,\n",
    "    'NumberChildren': sqlalchemy.types.SmallInteger,\n",
    "    'MaritalStatus': sqlalchemy.types.CHAR(1),\n",
    "    'Age': sqlalchemy.types.SmallInteger,\n",
    "    'CreditRating': sqlalchemy.types.SmallInteger,\n",
    "    'OwnOrRentFlag': sqlalchemy.types.CHAR(1),\n",
    "    'Employer': sqlalchemy.types.CHAR(30),\n",
    "    'NumberCreditCards': sqlalchemy.types.SmallInteger,\n",
    "    'NetWorth': sqlalchemy.types.BigInteger,\n",
    "    'MarketingNameplate': sqlalchemy.types.CHAR(100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE Prospect (\n",
    "    AgencyID CHAR(30) NOT NULL,\n",
    "    SK_RecordDateID INT UNSIGNED NOT NULL,\n",
    "    SK_UpdateDateID INT UNSIGNED NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL,\n",
    "    IsCustomer BOOLEAN NOT NULL,\n",
    "    LastName CHAR(30) NOT NULL,\n",
    "    FirstName CHAR(30) NOT NULL,\n",
    "    MiddleInitial CHAR(1),\n",
    "    Gender CHAR(1) CHECK (Gender IN ('M', 'F', 'U')),\n",
    "    AddressLine1 CHAR(80),\n",
    "    AddressLine2 CHAR(80),\n",
    "    PostalCode CHAR(12),\n",
    "    City CHAR(25) NOT NULL,\n",
    "    State CHAR(20) NOT NULL,\n",
    "    Country CHAR(24),\n",
    "    Phone CHAR(30),\n",
    "    Income INT UNSIGNED,\n",
    "    NumberCars TINYINT UNSIGNED,\n",
    "    NumberChildren TINYINT UNSIGNED,\n",
    "    MaritalStatus CHAR(1) CHECK (MaritalStatus IN ('S', 'M', 'D', 'W', 'U')),\n",
    "    Age TINYINT UNSIGNED,\n",
    "    CreditRating SMALLINT UNSIGNED,\n",
    "    OwnOrRentFlag CHAR(1) CHECK (OwnOrRentFlag IN ('O', 'R', 'U')),\n",
    "    Employer CHAR(30),\n",
    "    NumberCreditCards TINYINT UNSIGNED,\n",
    "    NetWorth BIGINT,\n",
    "    MarketingNameplate CHAR(100),\n",
    "    PRIMARY KEY (AgencyID, SK_RecordDateID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df.to_sql('prospect', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_end_time = datetime.now()\n",
    "print(f\"Prospect took {(prospect_end_time - prospect_start_time).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Prospect file is processed, the number of source rows is counted. After the last\n",
    "row, a “Status” message is written to the DImessages table, with the MessageSource\n",
    "“Prospect”, MessageText “Source rows” and the MessageData field containing the\n",
    "number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = prospect_df.shape[0]\n",
    "message_type = \"Status\"\n",
    "message_source = \"Prospect\"\n",
    "message_text = f\"Inserted rows\"\n",
    "MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "batch_id = 1\n",
    "\n",
    "query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {batch_id}, '{message_source}', '{message_text}', '{message_type}', '{num_rows}')\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimAccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema as a dictionary\n",
    "schema = {\n",
    "    'SK_AccountID': 'uint32',\n",
    "    'AccountID': 'uint32',\n",
    "    'SK_BrokerID': 'uint32',\n",
    "    'SK_CustomerID': 'uint32',\n",
    "    'Status': 'str',\n",
    "    'AccountDesc': 'str',\n",
    "    'TaxStatus': 'UInt8',\n",
    "    'IsCurrent': 'bool',\n",
    "    'BatchID': 'uint8',\n",
    "    'EffectiveDate': 'datetime64[ns]',\n",
    "    'EndDate': 'datetime64[ns]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_DIR + \"CustomerMgmt.xml\"\n",
    "tree = etree.parse(data_file)\n",
    "namespace = {'tpcdi': 'http://www.tpc.org/tpc-di'}\n",
    "\n",
    "# Get all actions\n",
    "all_actions = tree.xpath(\".//tpcdi:Action\", namespaces=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified schema\n",
    "dimAccount_df = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in schema.items()})\n",
    "\n",
    "# initialize lists to store data\n",
    "relevant_cols = ['AccountID', 'SK_BrokerID', 'SK_CustomerID', 'Status', 'AccountDesc', 'TaxStatus', 'EffectiveDate']\n",
    "data = {col: [] for col in relevant_cols}\n",
    "\n",
    "# initialize dict to store most recent values for each account of a customer\n",
    "customer_accounts = dict()\n",
    "\n",
    "for index, action in enumerate(tqdm(all_actions)):\n",
    "    customer = action.find(\"Customer\", namespaces=namespace)\n",
    "    customer_id = int(customer.get(\"C_ID\", None))\n",
    "    if customer_id not in customer_accounts:\n",
    "        customer_accounts[customer_id] = dict()\n",
    "    if action.get(\"ActionType\") in (\"NEW\", \"ADDACCT\"):\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            # Customer/Account/CA_NAME\n",
    "            account_desc = account.findtext(\"CA_NAME\", default=None, namespaces=namespace)\n",
    "            data[\"AccountDesc\"].append(account_desc)\n",
    "            # Customer/Account/@CA_TAX_ST\n",
    "            tax_status = account.get(\"CA_TAX_ST\", None)\n",
    "            tax_status = int(tax_status) if tax_status else None\n",
    "            data[\"TaxStatus\"].append(tax_status)\n",
    "            #  Customer/Account/CA_B_ID\n",
    "            broker_id = account.findtext(\"CA_B_ID\", default=None, namespaces=namespace)\n",
    "            broker_id = int(broker_id) if broker_id else None\n",
    "            data[\"SK_BrokerID\"].append((broker_id, action_ts))\n",
    "            data[\"SK_CustomerID\"].append((customer_id, action_ts))\n",
    "            status = \"ACTIVE\"\n",
    "            data[\"Status\"].append(status)\n",
    "            # update customer_accounts\n",
    "            customer_accounts[customer_id][account_id] = {\n",
    "                \"AccountDesc\": account_desc,\n",
    "                \"TaxStatus\": tax_status,\n",
    "                \"SK_BrokerID\": (broker_id, action_ts),\n",
    "                \"SK_CustomerID\": (customer_id, action_ts),\n",
    "                \"Status\": status,\n",
    "            }\n",
    "    elif action.get(\"ActionType\") == \"UPDACCT\":\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            # Customer/Account/CA_NAME\n",
    "            account_desc = account.findtext(\"CA_NAME\", default=None, namespaces=namespace)\n",
    "            if account_desc is None:\n",
    "                account_desc = customer_accounts[customer_id][account_id][\"AccountDesc\"]\n",
    "            else:\n",
    "                customer_accounts[customer_id][account_id][\"AccountDesc\"] = account_desc\n",
    "            data[\"AccountDesc\"].append(account_desc)\n",
    "            # Customer/Account/@CA_TAX_ST\n",
    "            tax_status = account.get(\"CA_TAX_ST\", None)\n",
    "            tax_status = int(tax_status) if tax_status else None\n",
    "            if tax_status is None:\n",
    "                tax_status = customer_accounts[customer_id][account_id][\"TaxStatus\"]\n",
    "            else:\n",
    "                customer_accounts[customer_id][account_id][\"TaxStatus\"] = tax_status\n",
    "            data[\"TaxStatus\"].append(tax_status)\n",
    "            #  Customer/Account/CA_B_ID\n",
    "            broker_id = account.findtext(\"CA_B_ID\", default=None, namespaces=namespace)\n",
    "            broker_id = int(broker_id) if broker_id else None\n",
    "            if broker_id is None:\n",
    "                broker_id = customer_accounts[customer_id][account_id][\"SK_BrokerID\"][0]\n",
    "            else:\n",
    "                customer_accounts[customer_id][account_id][\"SK_BrokerID\"] = (broker_id, action_ts)\n",
    "            sk_brokerid = (broker_id, action_ts)\n",
    "            data[\"SK_BrokerID\"].append(sk_brokerid)\n",
    "            sk_customer_id = (customer_id, action_ts)\n",
    "            customer_accounts[customer_id][account_id][\"SK_CustomerID\"] = sk_customer_id\n",
    "            data[\"SK_CustomerID\"].append(sk_customer_id)\n",
    "            status = \"ACTIVE\"\n",
    "            data[\"Status\"].append(status)\n",
    "    elif action.get(\"ActionType\") == \"UPDCUST\":\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            # set all other fields as is\n",
    "            for col in customer_accounts[customer_id][account_id]:\n",
    "                if not col.startswith(\"SK_\"):\n",
    "                    data[col].append(customer_accounts[customer_id][account_id][col])\n",
    "            broker_id = customer_accounts[customer_id][account_id][\"SK_BrokerID\"][0]\n",
    "            customer_accounts[customer_id][account_id][\"SK_BrokerID\"] = (broker_id, action_ts)\n",
    "            sk_brokerid = (broker_id, action_ts)\n",
    "            data[\"SK_BrokerID\"].append(sk_brokerid)\n",
    "            sk_customer_id = (customer_id, action_ts)\n",
    "            customer_accounts[customer_id][account_id][\"SK_CustomerID\"] = sk_customer_id\n",
    "            data[\"SK_CustomerID\"].append(sk_customer_id)\n",
    "    elif action.get(\"ActionType\") in (\"INACT\", \"CLOSEACCT\"):\n",
    "        accounts = action.findall('Customer/Account', namespaces=namespace)\n",
    "        for account in accounts:\n",
    "            # set effective date for this account\n",
    "            action_ts = pd.to_datetime(action.get(\"ActionTS\"), format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            data[\"EffectiveDate\"].append(action_ts)\n",
    "            # Customer/Account/@CA_ID\n",
    "            account_id = account.get(\"CA_ID\", None)\n",
    "            account_id = int(account_id) if account_id else None\n",
    "            data[\"AccountID\"].append(account_id)\n",
    "            # set all other fields as is\n",
    "            for col in customer_accounts[customer_id][account_id]:\n",
    "                if col.startswith(\"SK_\"):\n",
    "                    continue\n",
    "                elif col != \"Status\":\n",
    "                    data[col].append(customer_accounts[customer_id][account_id][col])\n",
    "                else:\n",
    "                    data[col].append(\"INACTIVE\")\n",
    "                    customer_accounts[customer_id][account_id][col] = \"INACTIVE\"\n",
    "            broker_id = customer_accounts[customer_id][account_id][\"SK_BrokerID\"][0]\n",
    "            customer_accounts[customer_id][account_id][\"SK_BrokerID\"] = (broker_id, action_ts)\n",
    "            sk_brokerid = (broker_id, action_ts)\n",
    "            data[\"SK_BrokerID\"].append(sk_brokerid)\n",
    "            sk_customer_id = (customer_id, action_ts)\n",
    "            customer_accounts[customer_id][account_id][\"SK_CustomerID\"] = sk_customer_id\n",
    "            data[\"SK_CustomerID\"].append(sk_customer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the database to get all SK_BrokerID\n",
    "query_parts = [\n",
    "    f\"(BrokerID = {broker_id} AND EffectiveDate <= '{action_ts}' <= EndDate)\"\n",
    "    for broker_id, action_ts in data[\"SK_BrokerID\"]\n",
    "]\n",
    "# Joining all conditions with 'OR'\n",
    "conditions = \" OR \".join(query_parts)\n",
    "query = f\"\"\"SELECT BrokerID, EffectiveDate, EndDate, SK_BrokerID \n",
    "FROM dimbroker \n",
    "WHERE {conditions}\"\"\"\n",
    "result = pd.read_sql_query(query, engine)\n",
    "for index, pair in enumerate(data[\"SK_BrokerID\"]):\n",
    "    broker_id, action_ts = pair    \n",
    "    sk_brokerid = result[\n",
    "        (result[\"BrokerID\"] == broker_id)\n",
    "        & (result[\"EffectiveDate\"] <= action_ts.date())\n",
    "        & (action_ts.date() <= result[\"EndDate\"])\n",
    "    ].iloc[0, 3]\n",
    "    data[\"SK_BrokerID\"][index] = sk_brokerid\n",
    "\n",
    "# query the database to get all SK_CustomerID\n",
    "query_parts = [\n",
    "    f\"(CustomerID = {customer_id} AND EffectiveDate <= '{action_ts}' <= EndDate)\"\n",
    "    for customer_id, action_ts in data[\"SK_CustomerID\"]\n",
    "]\n",
    "# Joining all conditions with 'OR'\n",
    "conditions = \" OR \".join(query_parts)\n",
    "query = f\"\"\"SELECT CustomerID, EffectiveDate, EndDate, SK_CustomerID \n",
    "FROM dimcustomer \n",
    "WHERE {conditions}\"\"\"\n",
    "result = pd.read_sql_query(query, engine)\n",
    "for index, pair in enumerate(data[\"SK_CustomerID\"]):\n",
    "    customer_id, action_ts = pair\n",
    "    sk_brokerid = result[\n",
    "        (result[\"CustomerID\"] == customer_id)\n",
    "        & (result[\"EffectiveDate\"] <= action_ts.date())\n",
    "        & (action_ts.date() <= result[\"EndDate\"])\n",
    "    ].iloc[0, 3]\n",
    "    data[\"SK_CustomerID\"][index] = sk_brokerid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data:\n",
    "    dimAccount_df[col] = data[col]\n",
    "dimAccount_df['SK_AccountID'] = range(1, len(dimAccount_df) + 1)\n",
    "dimAccount_df['BatchID'] = 1\n",
    "\n",
    "# Sort the DataFrame by CustomerID and EffectiveDate\n",
    "dimAccount_df.sort_values(by=['AccountID', 'EffectiveDate'], inplace=True)\n",
    "# Create a shifted DataFrame\n",
    "shifted_df = dimAccount_df.shift(-1)\n",
    "# Update EndDate: If next row has same CustomerID, use its EffectiveDate; otherwise, use default date\n",
    "dimAccount_df['EndDate'] = pd.Timestamp('9999-12-31')\n",
    "mask = dimAccount_df['AccountID'] == shifted_df['AccountID']\n",
    "dimAccount_df.loc[mask, 'EndDate'] = shifted_df.loc[mask, 'EffectiveDate']\n",
    "\n",
    "# Update IsCurrent: True if next row has different CustomerID or is the last row\n",
    "dimAccount_df['IsCurrent'] = ~mask\n",
    "dimAccount_df.sort_values(by=['SK_AccountID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'SK_AccountID': sqlalchemy.types.Integer,\n",
    "    'AccountID': sqlalchemy.types.Integer,\n",
    "    'SK_BrokerID': sqlalchemy.types.Integer,\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'Status': sqlalchemy.types.String(10),\n",
    "    'AccountDesc': sqlalchemy.types.String(50),\n",
    "    'TaxStatus': sqlalchemy.types.SmallInteger,\n",
    "    'IsCurrent': sqlalchemy.types.Boolean,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'EffectiveDate': sqlalchemy.types.Date,\n",
    "    'EndDate': sqlalchemy.types.Date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimAccount (\n",
    "    SK_AccountID INT UNSIGNED NOT NULL,\n",
    "    AccountID INT UNSIGNED NOT NULL,\n",
    "    SK_BrokerID INT UNSIGNED NOT NULL,\n",
    "    SK_CustomerID INT UNSIGNED NOT NULL,\n",
    "    Status CHAR(10) NOT NULL,\n",
    "    AccountDesc CHAR(50),\n",
    "    TaxStatus TINYINT UNSIGNED,\n",
    "    IsCurrent BOOLEAN NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL,\n",
    "    EffectiveDate DATE NOT NULL,\n",
    "    EndDate DATE NOT NULL,\n",
    "    PRIMARY KEY (SK_AccountID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimAccount_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimAccount_df.to_sql('dimaccount', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'T_ID', 'T_DTS', 'T_ST_ID', 'T_TT_ID', 'T_IS_CASH', \n",
    "    'T_S_SYMB', 'T_QTY', 'T_BID_PRICE', 'T_CA_ID', 'T_EXEC_NAME', \n",
    "    'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX'\n",
    "]\n",
    "dtypes = {\n",
    "    'T_ID': 'uint64',\n",
    "    'T_DTS': 'str',\n",
    "    'T_ST_ID': 'str',\n",
    "    'T_TT_ID': 'str',\n",
    "    'T_IS_CASH': 'bool',\n",
    "    'T_S_SYMB': 'str',\n",
    "    'T_QTY': 'uint32',\n",
    "    'T_BID_PRICE': 'float64',\n",
    "    'T_CA_ID': 'uint32',\n",
    "    'T_EXEC_NAME': 'str',\n",
    "    'T_TRADE_PRICE': 'float64',\n",
    "    'T_CHRG': 'float64',\n",
    "    'T_COMM': 'float64',\n",
    "    'T_TAX': 'float64'\n",
    "}\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "trade_df = pd.read_csv(\n",
    "    DATA_DIR + \"Trade.txt\", \n",
    "    sep='|', \n",
    "    header=None, \n",
    "    names=columns, \n",
    "    dtype=dtypes,\n",
    "    parse_dates=['T_DTS']\n",
    ")\n",
    "trade_df['T_DTS'] = pd.to_datetime(trade_df['T_DTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"TH_T_ID\", \"TH_DTS\", \"TH_ST_ID\"]\n",
    "dtypes = {\n",
    "    \"TH_T_ID\": \"uint64\",\n",
    "    \"TH_DTS\": \"str\",\n",
    "    \"TH_ST_ID\": \"str\"\n",
    "}\n",
    "tradehistory_df = pd.read_csv(DATA_DIR + \"TradeHistory.txt\", sep=\"|\", header=None, \n",
    "                              names=columns, dtype=dtypes, parse_dates=['TH_DTS'])\n",
    "tradehistory_df['TH_DTS'] = pd.to_datetime(tradehistory_df['TH_DTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged = tradehistory_df.merge(trade_df, left_on='TH_T_ID', right_on='T_ID')\n",
    "del tradehistory_df\n",
    "del trade_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-fetch data from related tables\n",
    "date_mapping = pd.read_sql(\"SELECT DateValue, SK_DateID FROM dimdate\", engine).set_index(\"DateValue\")[\"SK_DateID\"].to_dict()\n",
    "time_mapping = pd.read_sql(\"SELECT TimeValue, SK_TimeID FROM dimtime\", engine).set_index(\"TimeValue\")[\"SK_TimeID\"].to_dict()\n",
    "status_mapping = pd.read_sql(\"SELECT ST_ID, ST_NAME FROM statustype\", engine).set_index(\"ST_ID\")[\"ST_NAME\"].to_dict()\n",
    "trade_type_mapping = pd.read_sql(\"SELECT TT_ID, TT_NAME FROM tradetype\", engine).set_index(\"TT_ID\")[\"TT_NAME\"].to_dict()\n",
    "\n",
    "# Fetching security and account info in one go\n",
    "security_info = pd.read_sql(\"SELECT Symbol, SK_SecurityID, SK_CompanyID, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])\n",
    "account_info = pd.read_sql(\"SELECT AccountID, SK_AccountID, SK_CustomerID, SK_BrokerID, EffectiveDate, EndDate FROM dimaccount\", engine)\n",
    "account_info['EffectiveDate'] = pd.to_datetime(account_info['EffectiveDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct copy\n",
    "trade_merged[\"TradeID\"] = trade_merged[\"T_ID\"]\n",
    "trade_merged[\"CashFlag\"] = trade_merged[\"T_IS_CASH\"]\n",
    "trade_merged[\"Quantity\"] = trade_merged[\"T_QTY\"]\n",
    "trade_merged[\"BidPrice\"] = trade_merged[\"T_BID_PRICE\"]\n",
    "trade_merged[\"ExecutedBy\"] = trade_merged[\"T_EXEC_NAME\"]\n",
    "trade_merged[\"TradePrice\"] = trade_merged[\"T_TRADE_PRICE\"]\n",
    "trade_merged[\"Fee\"] = trade_merged[\"T_CHRG\"]\n",
    "trade_merged[\"Commission\"] = trade_merged[\"T_COMM\"]\n",
    "trade_merged[\"Tax\"] = trade_merged[\"T_TAX\"]\n",
    "trade_merged[\"Status\"] = trade_merged[\"T_ST_ID\"].map(status_mapping)\n",
    "trade_merged[\"Type\"] = trade_merged[\"T_TT_ID\"].map(trade_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially set null\n",
    "trade_merged[\"SK_CreateDateID\"] = None\n",
    "trade_merged[\"SK_CreateTimeID\"] = None\n",
    "trade_merged[\"SK_CloseDateID\"] = None\n",
    "trade_merged[\"SK_CloseTimeID\"] = None\n",
    "\n",
    "# now populate\n",
    "create_mask = (trade_merged[\"TH_ST_ID\"] == \"PNDG\") | (trade_merged[\"TH_ST_ID\"] == \"SBMT\")\n",
    "trade_merged.loc[create_mask, \"SK_CreateDateID\"] = trade_merged.loc[create_mask, \"TH_DTS\"].dt.date.map(date_mapping)\n",
    "trade_merged.loc[create_mask, \"SK_CreateTimeID\"] = pd.to_timedelta(trade_merged.loc[create_mask, \"TH_DTS\"].dt.time.astype(str)).map(time_mapping)\n",
    "close_mask = (trade_merged[\"TH_ST_ID\"] == \"CMPT\") | (trade_merged[\"TH_ST_ID\"] == \"CNCL\")\n",
    "trade_merged.loc[close_mask, \"SK_CloseDateID\"] = trade_merged.loc[close_mask, \"TH_DTS\"].dt.date.map(date_mapping)\n",
    "trade_merged.loc[close_mask, \"SK_CloseTimeID\"] = pd.to_timedelta(trade_merged.loc[close_mask, \"TH_DTS\"].dt.time.astype(str)).map(time_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged.rename({\"T_S_SYMB\": \"Symbol\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged = pd.merge(\n",
    "    trade_merged,\n",
    "    security_info,\n",
    "    how=\"left\",\n",
    "    on=\"Symbol\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows where TH_DTS < EffectiveDate  or TH_DTS > EndDate\n",
    "trade_merged = trade_merged[\n",
    "    (trade_merged[\"TH_DTS\"] >= trade_merged[\"EffectiveDate\"])\n",
    "    & (trade_merged[\"TH_DTS\"].dt.date < trade_merged[\"EndDate\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged = pd.merge(\n",
    "    trade_merged,\n",
    "    account_info,\n",
    "    how=\"left\",\n",
    "    left_on=\"T_CA_ID\",\n",
    "    right_on=\"AccountID\",\n",
    ")\n",
    "trade_merged = trade_merged[\n",
    "    (trade_merged[\"TH_DTS\"] >= trade_merged[\"EffectiveDate_y\"])\n",
    "    & (trade_merged[\"TH_DTS\"].dt.date < trade_merged[\"EndDate_y\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [\n",
    "    \"TradeID\",\n",
    "    \"SK_BrokerID\",\n",
    "    \"SK_CreateDateID\",\n",
    "    \"SK_CreateTimeID\",\n",
    "    \"SK_CloseDateID\",\n",
    "    \"SK_CloseTimeID\",\n",
    "    \"Status\",\n",
    "    \"Type\",\n",
    "    \"CashFlag\",\n",
    "    \"SK_SecurityID\",\n",
    "    \"SK_CompanyID\",\n",
    "    \"Quantity\",\n",
    "    \"BidPrice\",\n",
    "    \"SK_CustomerID\",\n",
    "    \"SK_AccountID\",\n",
    "    \"ExecutedBy\",\n",
    "    \"TradePrice\",\n",
    "    \"Fee\",\n",
    "    \"Commission\",\n",
    "    \"Tax\",\n",
    "]\n",
    "trade_merged = trade_merged[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged = trade_merged.groupby(\"TradeID\").last().reset_index()\n",
    "trade_merged['BatchID'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'TradeID': 'uint32',\n",
    "    'SK_BrokerID': 'UInt32',\n",
    "    'SK_CreateDateID': 'uint32',\n",
    "    'SK_CreateTimeID': 'uint32',\n",
    "    'SK_CloseDateID': 'UInt32',\n",
    "    'SK_CloseTimeID': 'UInt32',\n",
    "    'Status': 'str',\n",
    "    'Type': 'str',\n",
    "    'CashFlag': 'bool',\n",
    "    'SK_SecurityID': 'uint32',\n",
    "    'SK_CompanyID': 'uint32',\n",
    "    'Quantity': 'uint32',\n",
    "    'BidPrice': 'float64',\n",
    "    'SK_CustomerID': 'uint32',\n",
    "    'SK_AccountID': 'uint32',\n",
    "    'ExecutedBy': 'str',\n",
    "    'TradePrice': 'float64',\n",
    "    'Fee': 'float64',\n",
    "    'Commission': 'float64',\n",
    "    'Tax': 'float64',\n",
    "    'BatchID': 'uint8'\n",
    "}\n",
    "trade_merged = trade_merged.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'TradeID': sqlalchemy.types.Integer,\n",
    "    'SK_BrokerID': sqlalchemy.types.Integer,\n",
    "    'SK_CreateDateID': sqlalchemy.types.Integer,\n",
    "    'SK_CreateTimeID': sqlalchemy.types.Integer,\n",
    "    'SK_CloseDateID': sqlalchemy.types.Integer,\n",
    "    'SK_CloseTimeID': sqlalchemy.types.Integer,\n",
    "    'Status': sqlalchemy.types.CHAR(10),\n",
    "    'Type': sqlalchemy.types.CHAR(12),\n",
    "    'CashFlag': sqlalchemy.types.Boolean,\n",
    "    'SK_SecurityID': sqlalchemy.types.Integer,\n",
    "    'SK_CompanyID': sqlalchemy.types.Integer,\n",
    "    'Quantity': sqlalchemy.types.Integer,\n",
    "    'BidPrice': sqlalchemy.types.Numeric(8, 2),\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'SK_AccountID': sqlalchemy.types.Integer,\n",
    "    'ExecutedBy': sqlalchemy.types.CHAR(64),\n",
    "    'TradePrice': sqlalchemy.types.Numeric(8, 2),\n",
    "    'Fee': sqlalchemy.types.Numeric(10, 2),\n",
    "    'Commission': sqlalchemy.types.Numeric(10, 2),\n",
    "    'Tax': sqlalchemy.types.Numeric(10, 2),\n",
    "    'BatchID': sqlalchemy.types.SmallInteger\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE DimTrade (\n",
    "    TradeID INT UNSIGNED NOT NULL,\n",
    "    SK_BrokerID INT UNSIGNED,\n",
    "    SK_CreateDateID INT UNSIGNED NOT NULL,\n",
    "    SK_CreateTimeID INT UNSIGNED NOT NULL,\n",
    "    SK_CloseDateID INT UNSIGNED,\n",
    "    SK_CloseTimeID INT UNSIGNED,\n",
    "    Status CHAR(10) NOT NULL,\n",
    "    Type CHAR(12) NOT NULL,\n",
    "    CashFlag BOOLEAN NOT NULL,\n",
    "    SK_SecurityID INT UNSIGNED NOT NULL,\n",
    "    SK_CompanyID INT UNSIGNED NOT NULL,\n",
    "    Quantity MEDIUMINT UNSIGNED NOT NULL,\n",
    "    BidPrice DECIMAL(8, 2) NOT NULL,\n",
    "    SK_CustomerID INT UNSIGNED NOT NULL,\n",
    "    SK_AccountID INT UNSIGNED NOT NULL,\n",
    "    ExecutedBy CHAR(64) NOT NULL,\n",
    "    TradePrice DECIMAL(8, 2),\n",
    "    Fee DECIMAL(10, 2),\n",
    "    Commission DECIMAL(10, 2),\n",
    "    Tax DECIMAL(10, 2),\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL,\n",
    "    PRIMARY KEY (TradeID)\n",
    ");\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "try:\n",
    "    for i in trange(0, trade_merged.shape[0], 100000):\n",
    "        trade_merged.iloc[i:i+100000].to_sql('dimtrade', engine, if_exists='append', index=False, dtype=sql_dtypes)\n",
    "    session.commit()\n",
    "except:\n",
    "    session.rollback()\n",
    "    raise\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "invalid_trades = trade_merged[\n",
    "    (trade_merged[\"Commission\"].notnull())\n",
    "    & (trade_merged[\"Commission\"] > (trade_merged[\"TradePrice\"] * trade_merged[\"Quantity\"]))\n",
    "]\n",
    "print(invalid_trades.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists without using iterrows\n",
    "MessageSource = [\"DimTrade\"] * len(invalid_trades)\n",
    "MessageType = [\"Alert\"] * len(invalid_trades)\n",
    "MessageText = [\"Invalid trade commission\"] * len(invalid_trades)\n",
    "MessageData = [\n",
    "    \"T_ID = \"\n",
    "    + invalid_trades[\"TradeID\"].astype(str)\n",
    "    + \", T_COMM = \"\n",
    "    + invalid_trades[\"Commission\"].astype(str)\n",
    "]\n",
    "# Convert MessageData from a list of Series to a list of strings\n",
    "MessageData = MessageData[0].tolist()\n",
    "\n",
    "print(len(MessageSource))\n",
    "print(len(MessageData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"INSERT INTO Dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "VALUES \"\"\"\n",
    "for i in range(len(MessageSource)):\n",
    "    query += f\"\"\"('{pd.Timestamp(\"now\")}', 1, '{MessageSource[i]}', '{MessageText[i]}', '{MessageType[i]}', '{MessageData[i]}'),\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query[:-1]))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for invalid trade fees\n",
    "invalid_fee_trades = trade_merged[\n",
    "    (trade_merged[\"Fee\"].notnull())\n",
    "    & (trade_merged[\"Fee\"] > (trade_merged[\"TradePrice\"] * trade_merged[\"Quantity\"]))\n",
    "]\n",
    "\n",
    "# Create the required lists\n",
    "MessageSource = [\"DimTrade\"] * len(invalid_fee_trades)\n",
    "MessageType = [\"Alert\"] * len(invalid_fee_trades)\n",
    "MessageText = [\"Invalid trade fee\"] * len(invalid_fee_trades)\n",
    "\n",
    "# Vectorized operation for MessageData\n",
    "MessageData = (\n",
    "    \"T_ID = \"\n",
    "    + invalid_fee_trades[\"TradeID\"].astype(str)\n",
    "    + \", T_CHRG = \"\n",
    "    + invalid_fee_trades[\"Fee\"].astype(str)\n",
    ")\n",
    "MessageData = MessageData.tolist()\n",
    "\n",
    "query = \"\"\"INSERT INTO Dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData) VALUES \"\"\"\n",
    "for i in range(len(MessageSource)):\n",
    "    query += f\"\"\"('{pd.Timestamp(\"now\")}', 1, '{MessageSource[i]}', '{MessageText[i]}', '{MessageType[i]}', '{MessageData[i]}'),\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query[:-1]))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactCashBalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcb_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df = pd.read_csv(\n",
    "    DATA_DIR + \"CashTransaction.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"CT_CA_ID\",\n",
    "        \"CT_DTS\",\n",
    "        \"CT_AMT\",\n",
    "        \"CT_NAME\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"CT_CA_ID\": \"uint32\",\n",
    "        \"CT_DTS\": \"str\",\n",
    "        \"CT_AMT\": \"float64\",\n",
    "        \"CT_NAME\": \"str\"\n",
    "    },\n",
    "    parse_dates=[\"CT_DTS\"],\n",
    ")\n",
    "cash_txn_df[\"CT_DTS\"] = pd.to_datetime(cash_txn_df[\"CT_DTS\"])\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SK_CustomerID and SK_AccountID are obtained from DimAccount by matching CT_CA_ID\n",
    "with AccountID, where CT_DTS is in the range given by EffectiveDate and EndDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_info = pd.read_sql(\n",
    "    \"SELECT AccountID, SK_AccountID, SK_CustomerID, EffectiveDate, EndDate FROM dimaccount\",\n",
    "    engine,\n",
    ")\n",
    "account_info[\"EffectiveDate\"] = pd.to_datetime(account_info[\"EffectiveDate\"])\n",
    "account_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df = cash_txn_df.merge(\n",
    "    account_info,\n",
    "    how=\"left\",\n",
    "    left_on=\"CT_CA_ID\",\n",
    "    right_on=\"AccountID\",\n",
    ")\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df = cash_txn_df[\n",
    "    (cash_txn_df[\"CT_DTS\"] >= cash_txn_df[\"EffectiveDate\"])\n",
    "    & (cash_txn_df[\"CT_DTS\"].dt.date < cash_txn_df[\"EndDate\"])\n",
    "]\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SK_DateID is obtained from DimDate by matching just the date portion of CT_DTS with\n",
    "DateValue to return the SK_DateID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info = pd.read_sql(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info[\"DateValue\"] = pd.to_datetime(date_info[\"DateValue\"])\n",
    "date_info.info()\n",
    "date_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df['SK_DateID'] = cash_txn_df['CT_DTS'].dt.date.map(date_info.set_index('DateValue')['SK_DateID'])\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cash is calculated as the sum of the prior Cash amount for this account plus the sum of all\n",
    "CT_AMT values from all transactions in this account on this day. If there is no previous\n",
    "FactCashBalances record for the associated account, zero is used. Remember that the net effect of all cash transactions for a given account on a given day is totaled, and only a single record is generated per account that had changes per day.\n",
    "\n",
    "The procedure used to determine the new Cash total must account for the possibility that a\n",
    "new surrogate key is created in DimAccount since the last cash transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by account ID and transaction date\n",
    "cash_txn_df.sort_values(by=['AccountID', 'CT_DTS'], inplace=True)\n",
    "\n",
    "# Create a new column to store the prior cash amount\n",
    "cash_txn_df['PriorCash'] = cash_txn_df.groupby('AccountID')['CT_AMT'].cumsum() - cash_txn_df['CT_AMT']\n",
    "cash_txn_df['PriorCash'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the cash balance\n",
    "cash_txn_df['Cash'] = cash_txn_df['PriorCash'] + cash_txn_df['CT_AMT']\n",
    "\n",
    "# Keep only the last record for each account on each day\n",
    "cash_txn_df = cash_txn_df.groupby(['AccountID', 'SK_DateID']).last().reset_index()\n",
    "\n",
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    \"SK_CustomerID\",\n",
    "    \"SK_AccountID\",\n",
    "    \"SK_DateID\",\n",
    "    \"Cash\",\n",
    "]\n",
    "cash_txn_df = cash_txn_df[keep_cols]\n",
    "cash_txn_df['BatchID'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_txn_df.info()\n",
    "cash_txn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE FactCashBalances (\n",
    "    SK_CustomerID INT UNSIGNED NOT NULL,\n",
    "    SK_AccountID INT UNSIGNED NOT NULL,\n",
    "    SK_DateID INT UNSIGNED NOT NULL,\n",
    "    Cash DECIMAL(15, 2) NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL\n",
    ");\"\"\"\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_CustomerID\": sqlalchemy.types.Integer,\n",
    "    \"SK_AccountID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID\": sqlalchemy.types.Integer,\n",
    "    \"Cash\": sqlalchemy.types.DECIMAL(precision=15, scale=2),\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(0, cash_txn_df.shape[0], 100000):\n",
    "    cash_txn_df.iloc[i:i+100000].to_sql('factcashbalances', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcb_end_time = datetime.now()\n",
    "print(f\"fcb took {(fcb_end_time - fcb_start_time).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FactHoldings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL queries\n",
    "sql_commands = [\n",
    "    \"DROP TABLE IF EXISTS TempHoldingHistory\",\n",
    "    \"\"\"\n",
    "    CREATE TEMPORARY TABLE TempHoldingHistory (\n",
    "        HH_H_T_ID INT UNSIGNED NOT NULL,\n",
    "        HH_T_ID INT UNSIGNED NOT NULL,\n",
    "        HH_BEFORE_QTY INT NOT NULL,\n",
    "        HH_AFTER_QTY INT NOT NULL\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    LOAD DATA LOCAL INFILE 'E:\\\\\\\\Documents\\\\\\\\BDMA\\\\\\\\ULB\\\\\\\\Data Warehouses\\\\\\\\tpc-di\\\\\\\\TPC-DI\\\\\\\\data\\\\\\\\sf5\\\\\\\\Batch1\\\\\\\\HoldingHistory.txt'\n",
    "    INTO TABLE TempHoldingHistory\n",
    "    FIELDS TERMINATED BY '|'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    (HH_H_T_ID, HH_T_ID, HH_BEFORE_QTY, HH_AFTER_QTY)\n",
    "    \"\"\",\n",
    "    \"DROP TABLE IF EXISTS FactHoldings\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE FactHoldings (\n",
    "        TradeID INT UNSIGNED NOT NULL,\n",
    "        CurrentTradeID INT UNSIGNED NOT NULL,\n",
    "        SK_CustomerID INT UNSIGNED NOT NULL,\n",
    "        SK_AccountID INT UNSIGNED NOT NULL,\n",
    "        SK_SecurityID INT UNSIGNED NOT NULL,\n",
    "        SK_CompanyID INT UNSIGNED NOT NULL,\n",
    "        SK_DateID INT UNSIGNED NOT NULL,\n",
    "        SK_TimeID INT UNSIGNED NOT NULL,\n",
    "        CurrentPrice DECIMAL(8, 2) NOT NULL CHECK (CurrentPrice > 0),\n",
    "        CurrentHolding INT NOT NULL,\n",
    "        BatchID SMALLINT UNSIGNED NOT NULL\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    INSERT INTO FactHoldings (TradeID, CurrentTradeID, SK_CustomerID, SK_AccountID, SK_SecurityID, SK_CompanyID, SK_DateID, SK_TimeID, CurrentPrice, CurrentHolding, BatchID)\n",
    "    SELECT \n",
    "        thh.HH_H_T_ID AS TradeID,\n",
    "        thh.HH_T_ID AS CurrentTradeID,\n",
    "        dt.SK_CustomerID,\n",
    "        dt.SK_AccountID,\n",
    "        dt.SK_SecurityID,\n",
    "        dt.SK_CompanyID,\n",
    "        dt.SK_CloseDateID AS SK_DateID,\n",
    "        dt.SK_CloseTimeID AS SK_TimeID,\n",
    "        dt.TradePrice AS CurrentPrice,\n",
    "        thh.HH_AFTER_QTY AS CurrentHolding,\n",
    "        1 AS BatchID\n",
    "    FROM \n",
    "        TempHoldingHistory thh\n",
    "    JOIN \n",
    "        DimTrade dt ON thh.HH_T_ID = dt.TradeID\n",
    "    \"\"\",\n",
    "    \"DROP TABLE IF EXISTS TempHoldingHistory\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the queries\n",
    "with engine.connect() as connection:\n",
    "    # connection.execute(text(\"SET GLOBAL local_infile = 1;\"))\n",
    "    for sql in sql_commands:\n",
    "        connection.execute(text(sql))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_end_time = datetime.now()\n",
    "print(f\"fh took {(fh_end_time - fh_start_time).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactMarketHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmh_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.read_csv(\n",
    "    DATA_DIR + \"DailyMarket.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"DM_DATE\",\n",
    "        \"DM_S_SYMB\",\n",
    "        \"DM_CLOSE\",\n",
    "        \"DM_HIGH\",\n",
    "        \"DM_LOW\",\n",
    "        \"DM_VOL\",\n",
    "    ],\n",
    "    dtype={\n",
    "        \"DM_DATE\": \"str\",\n",
    "        \"DM_S_SYMB\": \"str\",\n",
    "        \"DM_CLOSE\": \"float32\",\n",
    "        \"DM_HIGH\": \"float32\",\n",
    "        \"DM_LOW\": \"float32\",\n",
    "        \"DM_VOL\": \"int64\",\n",
    "    },\n",
    "    parse_dates=[\"DM_DATE\"],\n",
    ")\n",
    "dailymarket_df[\"DM_DATE\"] = pd.to_datetime(dailymarket_df[\"DM_DATE\"])\n",
    "dailymarket_df.info()\n",
    "dailymarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClosePrice, DayHigh, DayLow, and Volume are copied from DM_CLOSE, DM_HIGH,\n",
    "# DM_LOW, and DM_VOL respectively.\n",
    "dailymarket_df[\"ClosePrice\"] = dailymarket_df[\"DM_CLOSE\"]\n",
    "dailymarket_df[\"DayHigh\"] = dailymarket_df[\"DM_HIGH\"]\n",
    "dailymarket_df[\"DayLow\"] = dailymarket_df[\"DM_LOW\"]\n",
    "dailymarket_df[\"Volume\"] = dailymarket_df[\"DM_VOL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_info = pd.read_sql(\"SELECT Symbol AS DM_S_SYMB, SK_SecurityID, SK_CompanyID, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])\n",
    "security_info.info()\n",
    "security_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.merge(\n",
    "    dailymarket_df,\n",
    "    security_info,\n",
    "    how=\"left\",\n",
    "    on=\"DM_S_SYMB\",\n",
    ")\n",
    "dailymarket_df = dailymarket_df[\n",
    "    (dailymarket_df[\"DM_DATE\"] >= dailymarket_df[\"EffectiveDate\"])\n",
    "    & (dailymarket_df[\"DM_DATE\"].dt.date < dailymarket_df[\"EndDate\"])\n",
    "]\n",
    "# drop temp columns\n",
    "dailymarket_df.drop(columns=[\"EffectiveDate\", \"EndDate\"], inplace=True)\n",
    "dailymarket_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SK_DateID is obtained from DimDate by matching DM_DATE with DateValue to return the\n",
    "SK_DateID. The match is guaranteed to succeed because DimDate has been populated\n",
    "with date information for all dates relevant to the benchmark.\"\"\"\n",
    "date_info = pd.read_sql(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info[\"DateValue\"] = pd.to_datetime(date_info[\"DateValue\"])\n",
    "date_info = date_info.set_index(\"DateValue\")[\"SK_DateID\"].to_dict()\n",
    "dailymarket_df[\"SK_DateID\"] = dailymarket_df[\"DM_DATE\"].dt.date.map(date_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sort the DataFrame\n",
    "dailymarket_df.sort_values(by='DM_DATE', inplace=True)\n",
    "\n",
    "# Step 3 & 4: Group by 'DM_S_SYMB' and apply rolling max\n",
    "rolling_max = dailymarket_df.groupby('DM_S_SYMB').rolling('365D', on='DM_DATE')['DM_HIGH'].max()\n",
    "\n",
    "# Reset index to make merging easier\n",
    "rolling_max = rolling_max.reset_index()\n",
    "\n",
    "# Step 5: Merge with the original DataFrame\n",
    "dailymarket_df = dailymarket_df.merge(rolling_max, on=['DM_S_SYMB', 'DM_DATE'], suffixes=('', '_52WeekHigh'))\n",
    "\n",
    "# Rename the column for clarity\n",
    "dailymarket_df.rename(columns={'DM_HIGH_52WeekHigh': 'FiftyTwoWeekHigh'}, inplace=True)\n",
    "dailymarket_df[['DM_DATE', 'DM_S_SYMB', 'DM_HIGH', 'FiftyTwoWeekHigh']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly wrote this myself but dont ask me to explain...........\n",
    "rolling_rank = (\n",
    "    dailymarket_df.groupby(\"DM_S_SYMB\")\n",
    "    .rolling(\"365D\", on=\"DM_DATE\")[\"DM_HIGH\"]\n",
    "    .rank(method=\"average\", ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"DM_HIGH\": \"Rank\"})\n",
    ")\n",
    "rolling_rank[\"Rank\"] = rolling_rank[\"Rank\"].astype(\"uint32\")\n",
    "# Apply the mask to select DM_DATE only for those rows, then forward fill\n",
    "mask = rolling_rank['Rank'] == 1\n",
    "rolling_rank['SK_FiftyTwoWeekHighDate'] = rolling_rank['DM_DATE'].where(mask).ffill()\n",
    "rolling_rank['SK_FiftyTwoWeekHighDate'] = rolling_rank['SK_FiftyTwoWeekHighDate'].dt.date.map(date_info)\n",
    "rolling_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.concat([dailymarket_df, rolling_rank['SK_FiftyTwoWeekHighDate']], axis=1)\n",
    "dailymarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df.sort_values(by='DM_DATE', inplace=True)\n",
    "# Step 3 & 4: Group by 'DM_S_SYMB' and apply rolling min\n",
    "rolling_min = dailymarket_df.groupby('DM_S_SYMB').rolling('365D', on='DM_DATE')['DM_LOW'].min()\n",
    "# Reset index to make merging easier\n",
    "rolling_min = rolling_min.reset_index()\n",
    "# Step 5: Merge with the original DataFrame\n",
    "dailymarket_df = dailymarket_df.merge(rolling_min, on=['DM_S_SYMB', 'DM_DATE'], suffixes=('', '_52WeekLow'))\n",
    "# Rename the column for clarity\n",
    "dailymarket_df.rename(columns={'DM_LOW_52WeekLow': 'FiftyTwoWeekLow'}, inplace=True)\n",
    "# dailymarket_df[['DM_DATE', 'DM_S_SYMB', 'DM_LOW', 'FiftyTwoWeekLow']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same...\n",
    "rolling_rank = (\n",
    "    dailymarket_df.groupby(\"DM_S_SYMB\")\n",
    "    .rolling(\"365D\", on=\"DM_DATE\")[\"DM_LOW\"]\n",
    "    .rank(method=\"average\", ascending=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"DM_LOW\": \"Rank\"})\n",
    ")\n",
    "rolling_rank[\"Rank\"] = rolling_rank[\"Rank\"].astype(\"uint32\")\n",
    "# Apply the mask to select DM_DATE only for those rows, then forward fill\n",
    "mask = rolling_rank['Rank'] == 1\n",
    "rolling_rank['SK_FiftyTwoWeekLowDate'] = rolling_rank['DM_DATE'].where(mask).ffill()\n",
    "rolling_rank['SK_FiftyTwoWeekLowDate'] = rolling_rank['SK_FiftyTwoWeekLowDate'].dt.date.map(date_info)\n",
    "# rolling_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = pd.concat([dailymarket_df, rolling_rank['SK_FiftyTwoWeekLowDate']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df['SK_SecurityID'] = dailymarket_df['SK_SecurityID'].astype('uint32')\n",
    "dailymarket_df['SK_CompanyID'] = dailymarket_df['SK_CompanyID'].astype('uint32')\n",
    "dailymarket_df['SK_DateID'] = dailymarket_df['SK_DateID'].astype('uint32')\n",
    "dailymarket_df['FiftyTwoWeekHigh'] = dailymarket_df['FiftyTwoWeekHigh'].astype('float32')\n",
    "dailymarket_df['SK_FiftyTwoWeekHighDate'] = dailymarket_df['SK_FiftyTwoWeekHighDate'].astype('uint32')\n",
    "dailymarket_df['FiftyTwoWeekLow'] = dailymarket_df['FiftyTwoWeekLow'].astype('float32')\n",
    "dailymarket_df['SK_FiftyTwoWeekLowDate'] = dailymarket_df['SK_FiftyTwoWeekLowDate'].astype('uint32')\n",
    "dailymarket_df['DM_S_SYMB'] = dailymarket_df['DM_S_SYMB'].astype('category')\n",
    "\n",
    "dailymarket_df.drop(columns=['DM_HIGH', 'DM_LOW', 'DM_VOL', 'DM_CLOSE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_info = pd.read_sql(\"SELECT Symbol, Dividend, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df = dailymarket_df.merge(\n",
    "    security_info,\n",
    "    how=\"left\",\n",
    "    left_on=\"DM_S_SYMB\",\n",
    "    right_on=\"Symbol\",\n",
    ")\n",
    "dailymarket_df = dailymarket_df[\n",
    "    (dailymarket_df[\"DM_DATE\"] >= dailymarket_df[\"EffectiveDate\"])\n",
    "    & (dailymarket_df[\"DM_DATE\"].dt.date < dailymarket_df[\"EndDate\"])\n",
    "]\n",
    "dailymarket_df.drop(columns=[\"Symbol\", \"EffectiveDate\", \"EndDate\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df['Yield'] = dailymarket_df['Dividend'] / dailymarket_df['ClosePrice'] * 100\n",
    "dailymarket_df.drop(columns=[\"Dividend\"], inplace=True)\n",
    "dailymarket_df['BatchID'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"SK_CompanyID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID\": sqlalchemy.types.Integer,\n",
    "    # PERatio to be done in MySQL\n",
    "    # \"PERatio\": sqlalchemy.types.DECIMAL(precision=10, scale=2),\n",
    "    \"Yield\": sqlalchemy.types.DECIMAL(precision=5, scale=2),\n",
    "    \"FiftyTwoWeekHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekHighDate\": sqlalchemy.types.Integer,\n",
    "    \"FiftyTwoWeekLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekLowDate\": sqlalchemy.types.Integer,\n",
    "    \"ClosePrice\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"Volume\": sqlalchemy.types.BigInteger,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger,\n",
    "    \"DM_DATE\": sqlalchemy.types.Date,\n",
    "    \"DM_S_SYMB\": sqlalchemy.types.CHAR(16),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymarket_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(0, dailymarket_df.shape[0], 100000):\n",
    "    dailymarket_df.iloc[i:i+100000].to_sql('tempfactmarketprice', engine, if_exists='append', index=False,\n",
    "                                           dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_commands = [\n",
    "    \"CREATE INDEX idx_sk_companyid ON tempfactmarketprice(SK_CompanyID);\",\n",
    "    \"\"\"CREATE TABLE FactMarketHistory (\n",
    "        SK_SecurityID INT UNSIGNED NOT NULL,\n",
    "        SK_CompanyID INT UNSIGNED NOT NULL,\n",
    "        SK_DateID INT UNSIGNED NOT NULL,\n",
    "        PERatio DECIMAL(10, 2),\n",
    "        Yield DECIMAL(5, 2) NOT NULL,\n",
    "        FiftyTwoWeekHigh DECIMAL(8, 2) NOT NULL,\n",
    "        SK_FiftyTwoWeekHighDate INT UNSIGNED NOT NULL,\n",
    "        FiftyTwoWeekLow DECIMAL(8, 2) NOT NULL,\n",
    "        SK_FiftyTwoWeekLowDate INT UNSIGNED NOT NULL,\n",
    "        ClosePrice DECIMAL(8, 2) NOT NULL,\n",
    "        DayHigh DECIMAL(8, 2) NOT NULL,\n",
    "        DayLow DECIMAL(8, 2) NOT NULL,\n",
    "        Volume BIGINT UNSIGNED NOT NULL,\n",
    "        BatchID SMALLINT UNSIGNED NOT NULL\n",
    "    );\"\"\",\n",
    "    \"ALTER TABLE factmarkethistory ADD COLUMN DM_S_SYMB TEXT;\",\n",
    "    \"\"\"INSERT INTO factmarkethistory\n",
    "        SELECT SK_SecurityID, fmp.SK_CompanyID, SK_DateID, fmp.ClosePrice / T.Sum_EPS AS PERatio, Yield, FiftyTwoWeekHigh,\n",
    "        SK_FiftyTwoWeekHighDate, FiftyTwoWeekLow, SK_FiftyTwoWeekLowDate, ClosePrice, DayHigh, DayLow, Volume, BatchID, DM_S_SYMB\n",
    "        FROM tempfactmarketprice fmp\n",
    "        LEFT JOIN (SELECT \n",
    "            c.CompanyID, \n",
    "            c.SK_CompanyID AS SKCID, \n",
    "            f.FI_QTR_START_DATE,\n",
    "            SUM(f.FI_BASIC_EPS) OVER (\n",
    "                PARTITION BY c.CompanyID \n",
    "                ORDER BY f.FI_QTR_START_DATE \n",
    "                ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "            ) AS Sum_EPS\n",
    "        FROM financial f RIGHT JOIN dimCompany c ON f.SK_CompanyID = c.SK_CompanyID\n",
    "        ORDER BY c.CompanyID, f.FI_QTR_START_DATE) T\n",
    "        ON T.SKCID = fmp.SK_CompanyID\n",
    "        AND T.FI_QTR_START_DATE < fmp.DM_DATE \n",
    "        AND T.FI_QTR_START_DATE >= DATE_SUB(fmp.DM_DATE, INTERVAL 3 MONTH);\"\"\",\n",
    "    \"\"\"INSERT INTO dimessages\n",
    "        SELECT NOW() AS MessageDateAndTime, 1 AS BATCHID, 'FactMarketHistory' AS MessageSource, 'No earnings for company' AS MessageText,\n",
    "        'Alert' AS MessageType, CONCAT('DM_S_SYMB = ', DM_S_SYMB)\n",
    "        FROM factmarkethistory \n",
    "        WHERE PERatio IS NULL;\"\"\",\n",
    "    \"ALTER TABLE factmarkethistory DROP COLUMN DM_S_SYMB;\",\n",
    "    \"DROP TABLE tempfactmarketprice;\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the queries\n",
    "with engine.connect() as connection:\n",
    "    for sql in sql_commands:\n",
    "        connection.execute(text(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmh_end_time = datetime.now()\n",
    "print(f\"fmh took {(fmh_end_time - fmh_start_time).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactWatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_DIR + \"WatchHistory.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"W_C_ID\",\n",
    "        \"W_S_SYMB\",\n",
    "        \"W_DTS\",\n",
    "        \"W_ACTION\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"W_C_ID\": \"uint32\",\n",
    "        \"W_S_SYMB\": \"str\",\n",
    "        \"W_DTS\": \"str\",\n",
    "        \"W_ACTION\": \"str\"\n",
    "    },\n",
    "    parse_dates=[\"W_DTS\"]\n",
    ")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_info = pd.read_sql_query(\"SELECT CustomerID, SK_CustomerID, EffectiveDate, EndDate FROM dimcustomer\", engine)\n",
    "customer_info['EffectiveDate'] = pd.to_datetime(customer_info['EffectiveDate'])\n",
    "security_info = pd.read_sql_query(\"SELECT Symbol, SK_SecurityID, EffectiveDate, EndDate FROM dimsecurity\", engine)\n",
    "security_info['EffectiveDate'] = pd.to_datetime(security_info['EffectiveDate'])\n",
    "date_info = pd.read_sql_query(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info['DateValue'] = pd.to_datetime(date_info['DateValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SK_CustomerID\n",
    "df = df.merge(customer_info, how=\"left\", left_on=\"W_C_ID\", right_on=\"CustomerID\")\n",
    "# filter based on date\n",
    "df = df[\n",
    "    (df[\"W_DTS\"] >= df[\"EffectiveDate\"])\n",
    "    & (df[\"W_DTS\"].dt.date < df[\"EndDate\"])\n",
    "]\n",
    "# drop cols\n",
    "df.drop(columns=[\"CustomerID\", \"EffectiveDate\", \"EndDate\"], inplace=True)\n",
    "# get SK_SecurityID\n",
    "df = df.merge(security_info, how=\"left\", left_on=\"W_S_SYMB\", right_on=\"Symbol\")\n",
    "# filter based on date\n",
    "df = df[\n",
    "    (df[\"W_DTS\"] >= df[\"EffectiveDate\"])\n",
    "    & (df[\"W_DTS\"].dt.date < df[\"EndDate\"])\n",
    "]\n",
    "# drop cols\n",
    "df.drop(columns=[\"Symbol\", \"EffectiveDate\", \"EndDate\"], inplace=True)\n",
    "# SK_DateID_DatePlaced - set based on W_DTS.\n",
    "df['SK_DateID_DatePlaced'] = df['W_DTS'].dt.date.map(date_info.set_index('DateValue')['SK_DateID'])\n",
    "# BatchID - set to 1.\n",
    "df['BatchID'] = 1\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask for rows where W_ACTION is 'CNCL'\n",
    "mask_cncl = df['W_ACTION'] == 'CNCL'\n",
    "df.loc[mask_cncl, 'SK_DateID_DateRemoved'] = df.loc[mask_cncl, 'W_DTS'].dt.date.map(date_info.set_index('DateValue')['SK_DateID'])\n",
    "df.loc[~mask_cncl, 'SK_DateID_DateRemoved'] = None\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\"SK_CustomerID\", \"SK_SecurityID\", \"SK_DateID_DatePlaced\", \"SK_DateID_DateRemoved\", \"BatchID\"]\n",
    "df = df.groupby([\"W_C_ID\", \"W_S_SYMB\"]).first().reset_index()[keep_cols]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_CustomerID\": sqlalchemy.types.Integer,\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID_DatePlaced\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID_DateRemoved\": sqlalchemy.types.Integer,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE FactWatches (\n",
    "    SK_CustomerID INT UNSIGNED NOT NULL,\n",
    "    SK_SecurityID INT UNSIGNED NOT NULL,\n",
    "    SK_DateID_DatePlaced INT UNSIGNED NOT NULL,\n",
    "    SK_DateID_DateRemoved INT UNSIGNED,\n",
    "    BatchID SMALLINT UNSIGNED NOT NULL\n",
    ");\"\"\"\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(create_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(0, df.shape[0], 100000):\n",
    "    df.iloc[i:i+100000].to_sql('factwatches', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_end_time = datetime.now()\n",
    "print(f\"Prospect took {(fw_end_time - fw_start_time).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Batch Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"fk.sql\"\n",
    "# Read the SQL file\n",
    "with open(filepath, 'r') as file:\n",
    "    sql_file = file.read()\n",
    "queries = sql_file.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SQL commands\n",
    "with engine.connect() as connection:\n",
    "    for query in tqdm(queries):\n",
    "        query = query.strip() + \";\"\n",
    "        if len(query) < 5:\n",
    "            continue\n",
    "        connection.execute(text(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_end_time = datetime.now()\n",
    "print(f\"fk took {(fk_end_time - fk_start_time).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"..\\validation\\tpcdi_validation.sql\"\n",
    "# Read the SQL file\n",
    "with open(filepath, 'r') as file:\n",
    "    sql_file = file.read()\n",
    "\n",
    "# Execute the SQL commands\n",
    "with engine.connect() as connection:\n",
    "    for query in sql_file.split(\";\"):\n",
    "        query = query.strip() + \";\"\n",
    "        if len(query) < 5:\n",
    "            continue\n",
    "        connection.execute(text(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print(f\"Historical load took {(end_time - start_time).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"SELECT concat('ALTER TABLE `', TABLE_NAME, '` DROP FOREIGN KEY `', CONSTRAINT_NAME, '`;') \n",
    "# FROM information_schema.key_column_usage \n",
    "# WHERE CONSTRAINT_SCHEMA = 'tpcdi_sf5' \n",
    "# AND referenced_table_name IS NOT NULL;\"\"\"\n",
    "# result = pd.read_sql_query(query, engine).iloc[:, 0]\n",
    "# for value in result.values:\n",
    "#     with engine.connect() as cnxn:\n",
    "#         cnxn.execute(text(value))\n",
    "#         cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_ID = 2\n",
    "DATA_DIR = f\"..\\\\data\\\\sf5\\\\Batch{BATCH_ID}\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + \"BatchDate.txt\") as f:\n",
    "    BATCH_DATE = f.read().strip()\n",
    "BATCH_DATE = pd.to_datetime(BATCH_DATE)\n",
    "BATCH_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimCustomer & Prospect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### dimCustomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "column_names = [\n",
    "    \"CDC_FLAG\", \"CDC_DSN\", \"C_ID\", \"C_TAX_ID\", \"C_ST_ID\",\n",
    "    \"C_L_NAME\", \"C_F_NAME\", \"C_M_NAME\", \"C_GNDR\", \"C_TIER\",\n",
    "    \"C_DOB\", \"C_ADLINE1\", \"C_ADLINE2\", \"C_ZIPCODE\", \"C_CITY\",\n",
    "    \"C_STATE_PROV\", \"C_CTRY\", \"C_CTRY_1\", \"C_AREA_1\", \"C_LOCAL_1\",\n",
    "    \"C_EXT_1\", \"C_CTRY_2\", \"C_AREA_2\", \"C_LOCAL_2\", \"C_EXT_2\",\n",
    "    \"C_CTRY_3\", \"C_AREA_3\", \"C_LOCAL_3\", \"C_EXT_3\", \"C_EMAIL_1\",\n",
    "    \"C_EMAIL_2\", \"C_LCL_TX_ID\", \"C_NAT_TX_ID\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data types\n",
    "data_types = {\n",
    "    \"CDC_FLAG\": \"category\",\n",
    "    \"CDC_DSN\": \"int64\",\n",
    "    \"C_ID\": \"int64\",\n",
    "    \"C_TAX_ID\": \"str\",\n",
    "    \"C_ST_ID\": \"category\",\n",
    "    \"C_L_NAME\": \"str\",\n",
    "    \"C_F_NAME\": \"str\",\n",
    "    \"C_M_NAME\": \"str\",\n",
    "    \"C_GNDR\": \"category\",\n",
    "    \"C_TIER\": \"int64\",\n",
    "    \"C_DOB\": \"str\",\n",
    "    \"C_ADLINE1\": \"str\",\n",
    "    \"C_ADLINE2\": \"str\",\n",
    "    \"C_ZIPCODE\": \"str\",\n",
    "    \"C_CITY\": \"str\",\n",
    "    \"C_STATE_PROV\": \"str\",\n",
    "    \"C_CTRY\": \"str\",\n",
    "    \"C_CTRY_1\": \"str\",\n",
    "    \"C_AREA_1\": \"str\",\n",
    "    \"C_LOCAL_1\": \"str\",\n",
    "    \"C_EXT_1\": \"str\",\n",
    "    \"C_CTRY_2\": \"str\",\n",
    "    \"C_AREA_2\": \"str\",\n",
    "    \"C_LOCAL_2\": \"str\",\n",
    "    \"C_EXT_2\": \"str\",\n",
    "    \"C_CTRY_3\": \"str\",\n",
    "    \"C_AREA_3\": \"str\",\n",
    "    \"C_LOCAL_3\": \"str\",\n",
    "    \"C_EXT_3\": \"str\",\n",
    "    \"C_EMAIL_1\": \"str\",\n",
    "    \"C_EMAIL_2\": \"str\",\n",
    "    \"C_LCL_TX_ID\": \"str\",\n",
    "    \"C_NAT_TX_ID\": \"str\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the file\n",
    "file_path = DATA_DIR + \"Customer.txt\"\n",
    "df = pd.read_csv(file_path, sep='|', header=None, names=column_names, dtype=data_types, parse_dates=[\"C_DOB\"])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.rename(columns={\n",
    "    \"C_ID\": \"CustomerID\",\n",
    "    \"C_TAX_ID\": \"TaxID\",\n",
    "    \"C_L_NAME\": \"LastName\",\n",
    "    \"C_F_NAME\": \"FirstName\",\n",
    "    \"C_M_NAME\": \"MiddleInitial\",\n",
    "    \"C_TIER\": \"Tier\",\n",
    "    \"C_DOB\": \"DOB\",\n",
    "    \"C_EMAIL_1\": \"Email1\",\n",
    "    \"C_EMAIL_2\": \"Email2\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['C_GNDR'].str.upper()\n",
    "df.loc[~df['Gender'].isin(['M', 'F']), 'Gender'] = 'U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    \"C_ADLINE1\": \"AddressLine1\",\n",
    "    \"C_ADLINE2\": \"AddressLine2\",\n",
    "    \"C_ZIPCODE\": \"PostalCode\",\n",
    "    \"C_CITY\": \"City\",\n",
    "    \"C_STATE_PROV\": \"StateProv\",\n",
    "    \"C_CTRY\": \"Country\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status is copied from ST_NAME of the StatusType table by matching C_ST_ID with ST_ID of the StatusType table.\n",
    "status_mapping = pd.read_sql_query(\"SELECT ST_ID AS C_ST_ID, ST_NAME from StatusType\", engine).set_index(\"C_ST_ID\")[\"ST_NAME\"].to_dict()\n",
    "df.loc[:, \"Status\"] = df.loc[:, \"C_ST_ID\"].map(status_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_phone_number(row, i):\n",
    "    # Extract components of the phone number\n",
    "    ctry_code = row[f\"C_CTRY_{i}\"]\n",
    "    area_code = row[f\"C_AREA_{i}\"]\n",
    "    local = row[f\"C_LOCAL_{i}\"]\n",
    "    ext = row[f\"C_EXT_{i}\"]        \n",
    "\n",
    "    if pd.isna(ctry_code):\n",
    "        ctry_code = None\n",
    "    if pd.isna(area_code):\n",
    "        area_code = None\n",
    "    if pd.isna(local):\n",
    "        local = None\n",
    "    if pd.isna(ext):\n",
    "        ext = None\n",
    "\n",
    "    # Apply transformation rules\n",
    "    if ctry_code and area_code and local:\n",
    "        phone = f\"+{ctry_code} ({area_code}) {local}\"\n",
    "    elif area_code and local:\n",
    "        phone = f\"({area_code}) {local}\"\n",
    "    elif local:\n",
    "        phone = local\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Add extension if present\n",
    "    if ext:\n",
    "        phone += f\"{ext}\"\n",
    "\n",
    "    return phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    df[f'Phone{i}'] = df.apply(format_phone_number, axis=1, args=(i, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_info = pd.read_sql_query(\"SELECT TX_ID, TX_NAME, TX_RATE FROM TaxRate\", engine).set_index('TX_ID')\n",
    "tax_name_mapping = tax_info['TX_NAME'].to_dict()\n",
    "tax_rate_mapping = tax_info['TX_RATE'].to_dict()\n",
    "\n",
    "# NationalTaxRateDesc and NationalTaxRate are copied from TX_NAME and TX_RATE respectively by matching C_NAT_TX_ID with TX_ID.\n",
    "df.loc[:, 'NationalTaxRateDesc'] = df.loc[:, 'C_NAT_TX_ID'].map(tax_name_mapping)\n",
    "df.loc[:, 'NationalTaxRate'] = df.loc[:, 'C_NAT_TX_ID'].map(tax_rate_mapping)\n",
    "\n",
    "# LocalTaxRateDesc and LocalTaxRate are copied from TX_NAME and TX_RATE respectively by matching C_LCL_TX_ID with TX_ID.\n",
    "df.loc[:, 'LocalTaxRateDesc'] = df.loc[:, 'C_LCL_TX_ID'].map(tax_name_mapping)\n",
    "df.loc[:, 'LocalTaxRate'] = df.loc[:, 'C_LCL_TX_ID'].map(tax_rate_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'IsCurrent'] = 1\n",
    "df.loc[:, 'EffectiveDate'] = pd.to_datetime(BATCH_DATE)\n",
    "df.loc[:, 'EndDate'] = pd.Timestamp(\"9999-12-31\")\n",
    "df.loc[:, 'BatchID'] = BATCH_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Prospect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prospect_file(filepath):\n",
    "    # Define the column names and their data types\n",
    "    columns = [\n",
    "        'AgencyID', 'LastName', 'FirstName', 'MiddleInitial', 'Gender', \n",
    "        'AddressLine1', 'AddressLine2', 'PostalCode', 'City', 'State', \n",
    "        'Country', 'Phone', 'Income', 'NumberCars', 'NumberChildren', \n",
    "        'MaritalStatus', 'Age', 'CreditRating', 'OwnOrRentFlag', \n",
    "        'Employer', 'NumberCreditCards', 'NetWorth'\n",
    "    ]\n",
    "\n",
    "    # Define the data types for reading the file\n",
    "    dtypes = {\n",
    "        'AgencyID': 'str', 'LastName': 'str', 'FirstName': 'str', \n",
    "        'MiddleInitial': 'str', 'Gender': 'str', 'AddressLine1': 'str', \n",
    "        'AddressLine2': 'str', 'PostalCode': 'str', 'City': 'str', \n",
    "        'State': 'str', 'Country': 'str', 'Phone': 'str', \n",
    "        'Income': 'Int64', 'NumberCars': 'Int8', 'NumberChildren': 'Int8', \n",
    "        'MaritalStatus': 'str', 'Age': 'Int8', 'CreditRating': 'Int16', \n",
    "        'OwnOrRentFlag': 'str', 'Employer': 'str', \n",
    "        'NumberCreditCards': 'Int8', 'NetWorth': 'Int64'\n",
    "    }\n",
    "\n",
    "    # Read the CSV file\n",
    "    raw_prospect_df = pd.read_csv(\n",
    "        filepath, \n",
    "        header=None, \n",
    "        names=columns, \n",
    "        dtype=dtypes\n",
    "    )\n",
    "\n",
    "    return raw_prospect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prospect_df = read_prospect_file(DATA_DIR + \"Prospect.csv\")\n",
    "raw_prospect_df.info()\n",
    "raw_prospect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_cols = ['LastName', 'FirstName', 'City', 'State']\n",
    "for col in not_null_cols:\n",
    "    raw_prospect_df.loc[raw_prospect_df[col].isna(), col] = ''\n",
    "raw_prospect_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'AgencyID': 'str',\n",
    "    'SK_RecordDateID': 'uint32',\n",
    "    'SK_UpdateDateID': 'uint32',\n",
    "    'BatchID': 'uint16',\n",
    "    'IsCustomer': 'boolean',\n",
    "    'LastName': 'str',\n",
    "    'FirstName': 'str',\n",
    "    'MiddleInitial': 'str',\n",
    "    'Gender': 'str',\n",
    "    'AddressLine1': 'str',\n",
    "    'AddressLine2': 'str',\n",
    "    'PostalCode': 'str',\n",
    "    'City': 'str',\n",
    "    'State': 'str',\n",
    "    'Country': 'str',\n",
    "    'Phone': 'str',\n",
    "    'Income': 'uint32',\n",
    "    'NumberCars': 'uint8',\n",
    "    'NumberChildren': 'uint8',\n",
    "    'MaritalStatus': 'str',\n",
    "    'Age': 'uint8',\n",
    "    'CreditRating': 'uint16',\n",
    "    'OwnOrRentFlag': 'str',\n",
    "    'Employer': 'str',\n",
    "    'NumberCreditCards': 'uint8',\n",
    "    'NetWorth': 'int64',\n",
    "    'MarketingNameplate': 'str'\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame with the specified schema\n",
    "prospect_df = pd.DataFrame({col: pd.Series(dtype=typ) for col, typ in dtypes.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df[\"AgencyID\"] = raw_prospect_df[\"AgencyID\"]\n",
    "prospect_df[\"LastName\"] = raw_prospect_df[\"LastName\"]\n",
    "prospect_df[\"FirstName\"] = raw_prospect_df[\"FirstName\"]\n",
    "prospect_df[\"MiddleInitial\"] = raw_prospect_df[\"MiddleInitial\"]\n",
    "prospect_df[\"Gender\"] = raw_prospect_df[\"Gender\"]\n",
    "# fix data quality issues\n",
    "prospect_df['Gender'] = prospect_df['Gender'].str.upper()\n",
    "mask = ~prospect_df['Gender'].isin([\"M\", \"F\"])\n",
    "prospect_df.loc[mask, \"Gender\"] = \"U\"\n",
    "prospect_df[\"AddressLine1\"] = raw_prospect_df[\"AddressLine1\"]\n",
    "prospect_df[\"AddressLine2\"] = raw_prospect_df[\"AddressLine2\"]\n",
    "prospect_df[\"PostalCode\"] = raw_prospect_df[\"PostalCode\"]\n",
    "prospect_df[\"City\"] = raw_prospect_df[\"City\"]\n",
    "prospect_df[\"State\"] = raw_prospect_df[\"State\"]\n",
    "prospect_df[\"Country\"] = raw_prospect_df[\"Country\"]\n",
    "prospect_df[\"Phone\"] = raw_prospect_df[\"Phone\"]\n",
    "prospect_df[\"Income\"] = raw_prospect_df[\"Income\"]\n",
    "prospect_df[\"NumberCars\"] = raw_prospect_df[\"NumberCars\"]\n",
    "prospect_df[\"NumberChildren\"] = raw_prospect_df[\"NumberChildren\"]\n",
    "prospect_df[\"MaritalStatus\"] = raw_prospect_df[\"MaritalStatus\"]\n",
    "prospect_df[\"MaritalStatus\"] = prospect_df[\"MaritalStatus\"].str.upper()\n",
    "mask = ~prospect_df[\"MaritalStatus\"].isin([\"S\", \"M\", \"D\", \"W\"])\n",
    "prospect_df.loc[mask, \"MaritalStatus\"] = \"U\"\n",
    "prospect_df[\"Age\"] = raw_prospect_df[\"Age\"]\n",
    "prospect_df[\"CreditRating\"] = raw_prospect_df[\"CreditRating\"]\n",
    "prospect_df[\"OwnOrRentFlag\"] = raw_prospect_df[\"OwnOrRentFlag\"]\n",
    "prospect_df[\"OwnOrRentFlag\"] = prospect_df[\"OwnOrRentFlag\"].str.upper()\n",
    "mask = ~prospect_df[\"OwnOrRentFlag\"].isin([\"O\", \"R\"])\n",
    "prospect_df.loc[mask, \"OwnOrRentFlag\"] = \"U\"\n",
    "prospect_df[\"Employer\"] = raw_prospect_df[\"Employer\"]\n",
    "prospect_df[\"NumberCreditCards\"] = raw_prospect_df[\"NumberCreditCards\"]\n",
    "prospect_df[\"NetWorth\"] = raw_prospect_df[\"NetWorth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_dateid = pd.read_sql_query(f\"SELECT SK_DateID FROM DimDate where DateValue = '{BATCH_DATE}'\", engine).iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load old data from MySQL\n",
    "old_prospect_df = pd.read_sql('SELECT * FROM Prospect', engine)\n",
    "\n",
    "# Merge new and old data on AgencyID\n",
    "merged_df = pd.merge(prospect_df, old_prospect_df, on='AgencyID', suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "# List of fields to compare\n",
    "fields_to_compare = ['LastName', 'FirstName', 'MiddleInitial', 'Gender', 'AddressLine1', 'AddressLine2',\n",
    "                     'PostalCode', 'City', 'State', 'Country', 'Phone', 'Income', 'NumberCars',\n",
    "                     'NumberChildren', 'MaritalStatus', 'Age', 'CreditRating', 'OwnOrRentFlag',\n",
    "                     'Employer', 'NumberCreditCards', 'NetWorth']\n",
    "\n",
    "# Identify changed records\n",
    "is_changed = merged_df[[f + '_new' for f in fields_to_compare]] != merged_df[[f + '_old' for f in fields_to_compare]].values\n",
    "has_changed = is_changed.any(axis=1)\n",
    "# only update the records if the left join succeeded\n",
    "same_agency_id = ~merged_df['SK_UpdateDateID_old'].isna()\n",
    "\n",
    "# Update SK_UpdateDateID for changed records\n",
    "merged_df.loc[(has_changed) & (same_agency_id), 'SK_UpdateDateID_new'] = sk_dateid\n",
    "# the old ones keep the old SK_UpdateDateID\n",
    "merged_df.loc[(~has_changed) & (same_agency_id), 'SK_UpdateDateID_new'] = merged_df.loc[(~has_changed) & (same_agency_id), 'SK_UpdateDateID_old']\n",
    "\n",
    "# For new records (those that are NaN in old data), set SK_UpdateDateID to batch_date_sk\n",
    "is_new_record = merged_df['SK_UpdateDateID_old'].isna()\n",
    "merged_df.loc[is_new_record, 'SK_UpdateDateID_new'] = sk_dateid\n",
    "\n",
    "# Finalize the DataFrame with updated SK_UpdateDateID\n",
    "prospect_df_updated = merged_df[['AgencyID'] + [f + '_new' for f in fields_to_compare] + ['SK_UpdateDateID_new']]\n",
    "\n",
    "# rename cols\n",
    "cols = prospect_df_updated.columns.tolist()\n",
    "cols = {col: col.replace(\"_new\", \"\") for col in cols}\n",
    "prospect_df_updated.rename(columns=cols, inplace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SK_RecordDateID is set to the DimDate SK_DateID field that corresponds to the Batch Date.\n",
    "prospect_df_updated['SK_RecordDateID'] = sk_dateid\n",
    "prospect_df_updated['SK_RecordDateID'] = prospect_df_updated['SK_RecordDateID'].astype('uint32')\n",
    "\n",
    "prospect_df_updated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions for each tag with null checks\n",
    "conditions = {\n",
    "    \"HighValue\": (prospect_df_updated[\"NetWorth\"].notnull() & prospect_df_updated[\"Income\"].notnull())\n",
    "    & ((prospect_df_updated[\"NetWorth\"] > 1_000_000) | (prospect_df_updated[\"Income\"] > 200_000)),\n",
    "    \"Expenses\": (\n",
    "        prospect_df_updated[\"NumberChildren\"].notnull()\n",
    "        & prospect_df_updated[\"NumberCreditCards\"].notnull()\n",
    "    )\n",
    "    & ((prospect_df_updated[\"NumberChildren\"] > 3) | (prospect_df_updated[\"NumberCreditCards\"] > 5)),\n",
    "    \"Boomer\": prospect_df_updated[\"Age\"].notnull() & (prospect_df_updated[\"Age\"] > 45),\n",
    "    \"MoneyAlert\": (\n",
    "        prospect_df_updated[\"Income\"].notnull()\n",
    "        & prospect_df_updated[\"CreditRating\"].notnull()\n",
    "        & prospect_df_updated[\"NetWorth\"].notnull()\n",
    "    )\n",
    "    & (\n",
    "        (prospect_df_updated[\"Income\"] < 50_000)\n",
    "        | (prospect_df_updated[\"CreditRating\"] < 600)\n",
    "        | (prospect_df_updated[\"NetWorth\"] < 100_000)\n",
    "    ),\n",
    "    \"Spender\": (\n",
    "        prospect_df_updated[\"NumberCars\"].notnull() & prospect_df_updated[\"NumberCreditCards\"].notnull()\n",
    "    )\n",
    "    & ((prospect_df_updated[\"NumberCars\"] > 3) | (prospect_df_updated[\"NumberCreditCards\"] > 7)),\n",
    "    \"Inherited\": (prospect_df_updated[\"Age\"].notnull() & prospect_df_updated[\"NetWorth\"].notnull())\n",
    "    & ((prospect_df_updated[\"Age\"] < 25) & (prospect_df_updated[\"NetWorth\"] > 1_000_000)),\n",
    "}\n",
    "\n",
    "# Apply conditions to assign tags\n",
    "prospect_df_updated[\"MarketingNameplate\"] = \"\"\n",
    "for tag, condition in conditions.items():\n",
    "    prospect_df_updated[\"MarketingNameplate\"] += np.where(condition, tag + \"+\", \"\")\n",
    "\n",
    "# Remove trailing '+' and replace empty strings with None\n",
    "prospect_df_updated[\"MarketingNameplate\"] = (\n",
    "    prospect_df_updated[\"MarketingNameplate\"].str.rstrip(\"+\").replace(\"\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df_updated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df_updated['BatchID'] = BATCH_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Update Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary prospect_df for matching\n",
    "prospect_df_temp = prospect_df_updated[\n",
    "    [\n",
    "        \"AgencyID\",\n",
    "        \"CreditRating\",\n",
    "        \"NetWorth\",\n",
    "        \"MarketingNameplate\",\n",
    "        \"LastName\",\n",
    "        \"FirstName\",\n",
    "        \"AddressLine1\",\n",
    "        \"AddressLine2\",\n",
    "        \"PostalCode\",\n",
    "    ]\n",
    "].copy()\n",
    "prospect_df_temp[\"LastName\"] = prospect_df_temp[\"LastName\"].str.upper()\n",
    "prospect_df_temp[\"FirstName\"] = prospect_df_temp[\"FirstName\"].str.upper()\n",
    "prospect_df_temp[\"AddressLine1\"] = prospect_df_temp[\"AddressLine1\"].str.upper()\n",
    "prospect_df_temp[\"AddressLine2\"] = prospect_df_temp[\"AddressLine2\"].str.upper()\n",
    "prospect_df_temp[\"PostalCode\"] = prospect_df_temp[\"PostalCode\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    # first check previous batch\n",
    "    first_name, last_name = row['FirstName'].upper(), row[\"LastName\"].upper()\n",
    "    address1, address2 = row['AddressLine1'], row[\"AddressLine2\"]\n",
    "    postcode = row['PostalCode']\n",
    "    if not pd.isna(address1):\n",
    "        address1 = address1.upper()\n",
    "    if not pd.isna(address2):\n",
    "        address2 = address1.upper()\n",
    "    if not pd.isna(postcode):\n",
    "        postcode = postcode.upper()\n",
    "    \n",
    "    query = f\"\"\"SELECT AgencyID, CreditRating, NetWorth, MarketingNameplate FROM Prospect\n",
    "    WHERE UPPER(FirstName) = '{first_name}' AND UPPER(LastName) = '{last_name}'\"\"\"\n",
    "    if pd.isna(row['AddressLine1']):\n",
    "        query += \" AND AddressLine1 IS NULL\"\n",
    "    else:\n",
    "        query += f\" AND UPPER(AddressLine1) = '{address1}'\"\n",
    "    if pd.isna(row['AddressLine2']):\n",
    "        query += \" AND AddressLine2 IS NULL\"\n",
    "    else:\n",
    "        query += f\" AND UPPER(AddressLine2) = '{address2}'\"\n",
    "    if pd.isna(row['PostalCode']):\n",
    "        query += \" AND PostalCode IS NULL;\"\n",
    "    else:\n",
    "        query += f\" AND UPPER(PostalCode) = '{postcode}';\"\n",
    "\n",
    "    result = pd.read_sql_query(query, engine)\n",
    "    if len(result) > 0:\n",
    "        df.loc[index, \"AgencyID\"] = result.iloc[0, 0]\n",
    "        df.loc[index, \"CreditRating\"] = result.iloc[0, 1]\n",
    "        df.loc[index, \"NetWorth\"] = result.iloc[0, 2]\n",
    "        df.loc[index, \"MarketingNameplate\"] = result.iloc[0, 3]\n",
    "    else:\n",
    "        # check current batch\n",
    "        match = prospect_df_temp[\n",
    "            (prospect_df_temp[\"LastName\"] == last_name)\n",
    "            & (prospect_df_temp[\"FirstName\"] == first_name)\n",
    "            & (prospect_df_temp[\"AddressLine1\"] == address1)\n",
    "            & (prospect_df_temp[\"AddressLine2\"] == address2)\n",
    "            & (prospect_df_temp[\"PostalCode\"] == postcode)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            df.loc[index, \"AgencyID\"] = match[\"AgencyID\"].iloc[-1]\n",
    "            df.loc[index, \"CreditRating\"] = match[\"CreditRating\"].iloc[-1]\n",
    "            df.loc[index, \"NetWorth\"] = match[\"NetWorth\"].iloc[-1]\n",
    "            df.loc[index, \"MarketingNameplate\"] = match[\"MarketingNameplate\"].iloc[-1]\n",
    "        else:\n",
    "            # set empty\n",
    "            df.loc[index, \"AgencyID\"] = None\n",
    "            df.loc[index, \"CreditRating\"] = None\n",
    "            df.loc[index, \"NetWorth\"] = None\n",
    "            df.loc[index, \"MarketingNameplate\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sk = pd.read_sql_query('SELECT MAX(SK_CustomerID) FROM dimCustomer', engine).iloc[0, 0]\n",
    "df.loc[:, 'SK_CustomerID'] = range(max_sk + 1, max_sk + 1 + len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A record will be reported in the DImessages table if a customer’s Tier is not one of the valid\n",
    "values (1,2,3). The MessageSource is “DimCustomer”, the MessageType is “Alert” and the\n",
    "MessageText is “Invalid customer tier”. The MessageData field is “C_ID = ” followed by the\n",
    "key value of the record, then “, C_TIER = ” and the C_TIER value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_tiers = df[~df['Tier'].isin([1,2,3])][[\"CustomerID\", \"Tier\"]]\n",
    "if len(invalid_tiers) > 0:\n",
    "    invalid_tiers.loc[:, \"MessageDateAndTime\"] = datetime.now()\n",
    "    invalid_tiers['BatchID'] = BATCH_ID\n",
    "    invalid_tiers['MessageSource'] = 'DimCustomer'\n",
    "    invalid_tiers['MessageText'] = 'Invalid customer tier'\n",
    "    invalid_tiers['MessageType'] = 'Alert'\n",
    "    invalid_tiers['MessageData'] = \"C_ID = \" + invalid_tiers[\"CustomerID\"].astype(str) + \", C_TIER = \" + invalid_tiers[\"Tier\"].astype(str)\n",
    "    \n",
    "    sql_dtypes = {\n",
    "        \"MessageDateAndTime\": sqlalchemy.types.DATETIME,\n",
    "        \"BatchID\": sqlalchemy.types.Integer,\n",
    "        \"MessageSource\": sqlalchemy.types.CHAR(30),\n",
    "        \"MessageText\": sqlalchemy.types.CHAR(50),\n",
    "        \"MessageType\": sqlalchemy.types.CHAR(12),\n",
    "        \"MessageData\": sqlalchemy.types.CHAR(100)\n",
    "    }\n",
    "    invalid_tiers.to_sql('dimessages', con=engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A record will be reported in the DImessages table if a customer’s DOB is invalid. A customer’s\n",
    "DOB is invalid if DOB < Batch Date – 100 years or DOB > Batch Date (customer over 100 years\n",
    "old or born in the future). The MessageSource is “DimCustomer”, the MessageType is “Alert”\n",
    "and the MessageText is “DOB out of range”. The MessageData field is “C_ID = ” followed by\n",
    "the key value of the record, then “, C_DOB = ” and the C_DOB value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_dobs = df[(df['DOB'] < BATCH_DATE - pd.Timedelta(days=100*365)) | (df['DOB'] > BATCH_DATE + pd.Timedelta(days=100*365))]\n",
    "if len(invalid_dobs) > 0:\n",
    "    invalid_dobs.loc[:, \"MessageDateAndTime\"] = datetime.now()\n",
    "    invalid_dobs['BatchID'] = BATCH_ID\n",
    "    invalid_dobs['MessageSource'] = 'DimCustomer'\n",
    "    invalid_dobs['MessageText'] = 'DOB out of range'\n",
    "    invalid_dobs['MessageType'] = 'Alert'\n",
    "    invalid_dobs['MessageData'] = \"C_ID = \" + invalid_tiers[\"CustomerID\"].astype(str) + \", C_DOB = \" + invalid_tiers[\"DOB\"].astype(str)\n",
    "    \n",
    "    sql_dtypes = {\n",
    "        \"MessageDateAndTime\": sqlalchemy.types.DATETIME,\n",
    "        \"BatchID\": sqlalchemy.types.Integer,\n",
    "        \"MessageSource\": sqlalchemy.types.CHAR(30),\n",
    "        \"MessageText\": sqlalchemy.types.CHAR(50),\n",
    "        \"MessageType\": sqlalchemy.types.CHAR(12),\n",
    "        \"MessageData\": sqlalchemy.types.CHAR(100)\n",
    "    }\n",
    "    invalid_dobs.to_sql('dimessages', con=engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_customers_mask = df['CDC_FLAG'] == 'I'\n",
    "keep_cols = [\n",
    "    \"SK_CustomerID\", \"CustomerID\", \"TaxID\", \"Status\", \"LastName\", \n",
    "    \"FirstName\", \"MiddleInitial\", \"Gender\", \"Tier\", \"DOB\", \n",
    "    \"AddressLine1\", \"AddressLine2\", \"PostalCode\", \"City\", \"StateProv\", \n",
    "    \"Country\", \"Phone1\", \"Phone2\", \"Phone3\", \"Email1\", \n",
    "    \"Email2\", \"NationalTaxRateDesc\", \"NationalTaxRate\", \"LocalTaxRateDesc\", \n",
    "    \"LocalTaxRate\", \"AgencyID\", \"CreditRating\", \"NetWorth\", \"MarketingNameplate\", \n",
    "    \"IsCurrent\", \"BatchID\", \"EffectiveDate\", \"EndDate\"\n",
    "]\n",
    "df_insert = df.loc[insert_customers_mask, keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'CustomerID': sqlalchemy.types.Integer,\n",
    "    'TaxID': sqlalchemy.types.String(20),\n",
    "    'Status': sqlalchemy.types.String(10),\n",
    "    'LastName': sqlalchemy.types.String(30),\n",
    "    'FirstName': sqlalchemy.types.String(30),\n",
    "    'MiddleInitial': sqlalchemy.types.String(1),\n",
    "    'Gender': sqlalchemy.types.String(1),\n",
    "    'Tier': sqlalchemy.types.SmallInteger,\n",
    "    'DOB': sqlalchemy.types.Date,\n",
    "    'AddressLine1': sqlalchemy.types.String(80),\n",
    "    'AddressLine2': sqlalchemy.types.String(80),\n",
    "    'PostalCode': sqlalchemy.types.String(12),\n",
    "    'City': sqlalchemy.types.String(25),\n",
    "    'StateProv': sqlalchemy.types.String(20),\n",
    "    'Country': sqlalchemy.types.String(24),\n",
    "    'Phone1': sqlalchemy.types.String(30),\n",
    "    'Phone2': sqlalchemy.types.String(30),\n",
    "    'Phone3': sqlalchemy.types.String(30),\n",
    "    'Email1': sqlalchemy.types.String(50),\n",
    "    'Email2': sqlalchemy.types.String(50),\n",
    "    'NationalTaxRateDesc': sqlalchemy.types.String(50),\n",
    "    'NationalTaxRate': sqlalchemy.types.Numeric(6, 5),\n",
    "    'LocalTaxRateDesc': sqlalchemy.types.String(50),\n",
    "    'LocalTaxRate': sqlalchemy.types.Numeric(6, 5),\n",
    "    'AgencyID': sqlalchemy.types.String(30),\n",
    "    'CreditRating': sqlalchemy.types.SmallInteger,\n",
    "    'NetWorth': sqlalchemy.types.Numeric(10),\n",
    "    'MarketingNameplate': sqlalchemy.types.String(100),\n",
    "    'IsCurrent': sqlalchemy.types.Boolean,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'EffectiveDate': sqlalchemy.types.Date,\n",
    "    'EndDate': sqlalchemy.types.Date\n",
    "}\n",
    "df_insert.to_sql('dimcustomer', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_customers_mask = df['CDC_FLAG'] == 'U'\n",
    "keep_cols = [\n",
    "    \"SK_CustomerID\", \"CustomerID\", \"TaxID\", \"Status\", \"LastName\", \n",
    "    \"FirstName\", \"MiddleInitial\", \"Gender\", \"Tier\", \"DOB\", \n",
    "    \"AddressLine1\", \"AddressLine2\", \"PostalCode\", \"City\", \"StateProv\", \n",
    "    \"Country\", \"Phone1\", \"Phone2\", \"Phone3\", \"Email1\", \n",
    "    \"Email2\", \"NationalTaxRateDesc\", \"NationalTaxRate\", \"LocalTaxRateDesc\", \n",
    "    \"LocalTaxRate\", \"AgencyID\", \"CreditRating\", \"NetWorth\", \"MarketingNameplate\", \n",
    "    \"IsCurrent\", \"BatchID\", \"EffectiveDate\", \"EndDate\"\n",
    "]\n",
    "df_update = df.loc[update_customers_mask, keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df_update.iterrows(), total=df_update.shape[0]):\n",
    "    customer_id = row['CustomerID']\n",
    "    required_rows = df_update[df_update['CustomerID'].isin([customer_id])]\n",
    "    # only one update per day\n",
    "    if len(required_rows) > 1 and index != required_rows.index[-1]:\n",
    "        continue\n",
    "    effective_date = row['EffectiveDate']\n",
    "\n",
    "    # Obtain the current SK_CustomerID from dimCustomer\n",
    "    current_customer = pd.read_sql_query(\n",
    "        f\"SELECT SK_CustomerID, Status FROM dimCustomer WHERE IsCurrent = 1 AND CustomerID = {customer_id}\",\n",
    "        engine\n",
    "    )\n",
    "\n",
    "    sk_customer_id = current_customer['SK_CustomerID'].iloc[0]\n",
    "    customer_status = current_customer['Status'].iloc[0]\n",
    "\n",
    "    # Find all current records in dimAccount with the obtained SK_CustomerID\n",
    "    current_accounts = pd.read_sql_query(\n",
    "        f\"SELECT * FROM dimAccount WHERE IsCurrent = 1 AND SK_CustomerID = {sk_customer_id}\",\n",
    "        engine\n",
    "    )\n",
    "\n",
    "    # Update these records by setting IsCurrent to 0 and EndDate to effective_date\n",
    "    update_query = f\"\"\"\n",
    "    UPDATE dimAccount\n",
    "    SET IsCurrent = 0, EndDate = '{effective_date}'\n",
    "    WHERE IsCurrent = 1 AND SK_CustomerID = {sk_customer_id};\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(update_query))\n",
    "        connection.commit()\n",
    "    # Prepare new account records\n",
    "    new_accounts = current_accounts.copy()\n",
    "    new_accounts['EffectiveDate'] = effective_date\n",
    "    new_accounts['EndDate'] = pd.Timestamp(\"9999-12-31\")\n",
    "    new_accounts['Status'] = new_accounts['Status'].apply(lambda x: 'INACTIVE' if customer_status == 'INACTIVE' else x)\n",
    "    new_accounts['IsCurrent'] = 1\n",
    "    new_accounts['BatchID'] = BATCH_ID\n",
    "    max_sk = pd.read_sql(\"SELECT MAX(SK_AccountID) FROM dimAccount\", engine).iloc[0, 0]\n",
    "    new_accounts[\"SK_AccountID\"] = range(max_sk + 1, max_sk + 1 + new_accounts.shape[0])\n",
    "\n",
    "    # Bulk insert new account records\n",
    "    new_accounts.to_sql('dimaccount', con=engine, if_exists='append', index=False, dtype={\n",
    "        'SK_AccountID': sqlalchemy.types.Integer,\n",
    "        'AccountID': sqlalchemy.types.Integer,\n",
    "        'SK_BrokerID': sqlalchemy.types.Integer,\n",
    "        'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "        'Status': sqlalchemy.types.String(10),\n",
    "        'AccountDesc': sqlalchemy.types.String(50),\n",
    "        'TaxStatus': sqlalchemy.types.SmallInteger,\n",
    "        'IsCurrent': sqlalchemy.types.Boolean,\n",
    "        'BatchID': sqlalchemy.types.SmallInteger,\n",
    "        'EffectiveDate': sqlalchemy.types.Date,\n",
    "        'EndDate': sqlalchemy.types.Date\n",
    "    })\n",
    "\n",
    "    # History-tracking update for the customer\n",
    "    history_update_query = f\"\"\"\n",
    "    UPDATE dimCustomer\n",
    "    SET IsCurrent = 0, EndDate = '{effective_date}'\n",
    "    WHERE IsCurrent = 1 AND CustomerID = {customer_id};\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(history_update_query))\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame columns to the appropriate types\n",
    "df_update['SK_CustomerID'] = df_update['SK_CustomerID'].astype(np.int32)\n",
    "df_update['CustomerID'] = df_update['CustomerID'].astype(np.int32)\n",
    "df_update['Tier'] = df_update['Tier'].astype(pd.Int8Dtype())\n",
    "df_update['NationalTaxRate'] = df_update['NationalTaxRate'].astype(np.float64)\n",
    "df_update['LocalTaxRate'] = df_update['LocalTaxRate'].astype(np.float64)\n",
    "df_update['CreditRating'] = df_update['CreditRating'].astype(pd.Int16Dtype())\n",
    "df_update['NetWorth'] = df_update['NetWorth'].astype(pd.Float64Dtype())\n",
    "df_update['BatchID'] = df_update['BatchID'].astype(np.int16)\n",
    "\n",
    "# Convert date columns to datetime.date\n",
    "df_update['DOB'] = pd.to_datetime(df_update['DOB']).dt.date\n",
    "df_update['EffectiveDate'] = pd.to_datetime(df_update['EffectiveDate']).dt.date\n",
    "df_update['EndDate'] = pd.to_datetime(df_update['EndDate']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update.to_sql('dimcustomer', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_customers = pd.read_sql_query(\"\"\"SELECT UPPER(FirstName) FirstName , UPPER(LastName) LastName, UPPER(AddressLine1) AddressLine1,\n",
    "UPPER(AddressLine2) AddressLine2, UPPER(PostalCode) PostalCode FROM dimCustomer WHERE IsCurrent = 1 AND Status = 'ACTIVE';\"\"\", engine)\n",
    "active_customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary uppercase columns for merging in both DataFrames\n",
    "merge_fields = [\"FirstName\", \"LastName\", \"AddressLine1\", \"AddressLine2\", \"PostalCode\"]\n",
    "merged_df = prospect_df_temp.merge(active_customers, how='left', on=merge_fields, indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = merged_df['_merge'] == 'both'\n",
    "prospect_df_updated.loc[:, 'IsCustomer'] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_df_updated['SK_UpdateDateID'] = prospect_df_updated['SK_UpdateDateID'].astype('uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_ids = pd.read_sql_query(\"SELECT DISTINCT AgencyID FROM Prospect\", engine).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_records = prospect_df_updated[prospect_df_updated['AgencyID'].isin(agency_ids)]\n",
    "new_records = prospect_df_updated[~prospect_df_updated['AgencyID'].isin(agency_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the MySQL table 'prospect'\n",
    "prospect_df = pd.read_sql_table('prospect', con=engine)\n",
    "\n",
    "# Merge the DataFrame with the MySQL table data on 'AgencyID'\n",
    "merged_df = pd.merge(existing_records, prospect_df, on='AgencyID', suffixes=('_existing', '_prospect'))\n",
    "\n",
    "# Define a function to compare rows, treating NaNs as equal\n",
    "def compare_rows(row, columns):\n",
    "    for col in columns:\n",
    "        # Both values are NaN\n",
    "        if pd.isna(row[col + '_existing']) and pd.isna(row[col + '_prospect']):\n",
    "            continue\n",
    "        # One value is NaN and the other is not\n",
    "        elif pd.isna(row[col + '_existing']) or pd.isna(row[col + '_prospect']):\n",
    "            return False\n",
    "        # Both values are not NaN, but are different\n",
    "        elif row[col + '_existing'] != row[col + '_prospect']:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Filter to find records where all columns are the same except 'IsCustomer'\n",
    "filter_columns = [col.replace('_existing', '') for col in merged_df.columns if 'IsCustomer' not in col and '_existing' in col]\n",
    "diff_is_customer = merged_df[merged_df.apply(lambda row: compare_rows(row, filter_columns) and (pd.isna(row['IsCustomer_existing']) != pd.isna(row['IsCustomer_prospect']) or row['IsCustomer_existing'] != row['IsCustomer_prospect']), axis=1)]\n",
    "\n",
    "# Count these records\n",
    "count_diff_is_customer = diff_is_customer.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'AgencyID': sqlalchemy.types.CHAR(30),\n",
    "    'SK_RecordDateID': sqlalchemy.types.Integer,\n",
    "    'SK_UpdateDateID': sqlalchemy.types.Integer,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'IsCustomer': sqlalchemy.types.Boolean,\n",
    "    'LastName': sqlalchemy.types.CHAR(30),\n",
    "    'FirstName': sqlalchemy.types.CHAR(30),\n",
    "    'MiddleInitial': sqlalchemy.types.CHAR(1),\n",
    "    'Gender': sqlalchemy.types.CHAR(1),\n",
    "    'AddressLine1': sqlalchemy.types.CHAR(80),\n",
    "    'AddressLine2': sqlalchemy.types.CHAR(80),\n",
    "    'PostalCode': sqlalchemy.types.CHAR(12),\n",
    "    'City': sqlalchemy.types.CHAR(25),\n",
    "    'State': sqlalchemy.types.CHAR(20),\n",
    "    'Country': sqlalchemy.types.CHAR(24),\n",
    "    'Phone': sqlalchemy.types.CHAR(30),\n",
    "    'Income': sqlalchemy.types.Integer,\n",
    "    'NumberCars': sqlalchemy.types.SmallInteger,\n",
    "    'NumberChildren': sqlalchemy.types.SmallInteger,\n",
    "    'MaritalStatus': sqlalchemy.types.CHAR(1),\n",
    "    'Age': sqlalchemy.types.SmallInteger,\n",
    "    'CreditRating': sqlalchemy.types.SmallInteger,\n",
    "    'OwnOrRentFlag': sqlalchemy.types.CHAR(1),\n",
    "    'Employer': sqlalchemy.types.CHAR(30),\n",
    "    'NumberCreditCards': sqlalchemy.types.SmallInteger,\n",
    "    'NetWorth': sqlalchemy.types.BigInteger,\n",
    "    'MarketingNameplate': sqlalchemy.types.CHAR(100)\n",
    "}\n",
    "\n",
    "new_records.to_sql('prospect', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    for _, row in tqdm(existing_records.iterrows(), total=existing_records.shape[0]):\n",
    "        agencyid = row['AgencyID']\n",
    "        query_parts = []\n",
    "        for key, value in row.to_dict().items():\n",
    "            if key != 'AgencyID':\n",
    "                if pd.isna(value):\n",
    "                    query_part = f\"{key} = NULL\"\n",
    "                elif isinstance(value, str):\n",
    "                    value = value.replace('\"', '\\\\\"')\n",
    "                    query_part = f\"{key} = \\\"{value}\\\"\"\n",
    "                else:  # for numeric and boolean types\n",
    "                    query_part = f\"{key} = {value}\"\n",
    "                query_parts.append(query_part)\n",
    "        query = f\"UPDATE prospect SET {', '.join(query_parts)} WHERE AgencyID = \\\"{agencyid}\\\"\"\n",
    "        connection.execute(text(query))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = prospect_df_updated.shape[0]\n",
    "message_type = \"Status\"\n",
    "message_source = \"Prospect\"\n",
    "message_text = \"Source rows\"\n",
    "MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "\n",
    "query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {BATCH_ID}, '{message_source}', '{message_text}', '{message_type}', '{num_rows}')\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = new_records.shape[0]\n",
    "message_type = \"Status\"\n",
    "message_source = \"Prospect\"\n",
    "message_text = \"Inserted rows\"\n",
    "MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "\n",
    "query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {BATCH_ID}, '{message_source}', '{message_text}', '{message_type}', '{num_rows}')\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_rows = existing_records.shape[0] - count_diff_is_customer\n",
    "message_type = \"Status\"\n",
    "message_source = \"Prospect\"\n",
    "message_text = \"Updated rows\"\n",
    "MessageDateAndTime = pd.Timestamp(\"now\")\n",
    "\n",
    "query = f\"\"\"INSERT INTO dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "            VALUES ('{MessageDateAndTime}', {BATCH_ID}, '{message_source}', '{message_text}', '{message_type}', '{updated_rows}')\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(query))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dimAccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_DIR + \"Account.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"CDC_FLAG\", \"CDC_DSN\", \"CA_ID\", \"CA_B_ID\", \"CA_C_ID\", \"CA_NAME\", \"CA_TAX_ST\", \"CA_ST_ID\"],\n",
    "    dtype={\n",
    "        \"CDC_FLAG\": \"str\",\n",
    "        \"CDC_DSN\": \"int64\",\n",
    "        \"CA_ID\": \"int64\",\n",
    "        \"CA_B_ID\": \"int64\",\n",
    "        \"CA_C_ID\": \"int64\",\n",
    "        \"CA_NAME\": \"str\",\n",
    "        \"CA_TAX_ST\": \"uint8\",\n",
    "        \"CA_ST_ID\": \"str\"\n",
    "    }\n",
    ")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({\n",
    "    \"CA_ID\": \"AccountID\",\n",
    "    \"CA_NAME\": \"AccountDesc\",\n",
    "    \"CA_TAX_ST\": \"TaxStatus\"\n",
    "}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broker_mapping = pd.read_sql_query(\"SELECT BrokerID AS CA_B_ID, SK_BrokerID FROM dimbroker WHERE isCurrent = 1\", engine).set_index(\"CA_B_ID\")[\"SK_BrokerID\"].to_dict()\n",
    "customer_mapping = pd.read_sql_query(\"SELECT CustomerID AS CA_C_ID, SK_CustomerID FROM dimcustomer WHERE isCurrent = 1\", engine).set_index(\"CA_C_ID\")[\"SK_CustomerID\"].to_dict()\n",
    "status_mapping = pd.read_sql_query(\"SELECT ST_ID AS CA_ST_ID, ST_NAME FROM statustype\", engine).set_index(\"CA_ST_ID\")[\"ST_NAME\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'SK_BrokerID'] = df['CA_B_ID'].map(broker_mapping)\n",
    "df.loc[:, 'SK_CustomerID'] = df['CA_C_ID'].map(customer_mapping)\n",
    "df.loc[:, 'Status'] = df['CA_ST_ID'].map(status_mapping)\n",
    "df.loc[:, 'EffectiveDate'] = pd.to_datetime(BATCH_DATE)\n",
    "df.loc[:, 'EndDate'] = pd.Timestamp(\"9999-12-31\")\n",
    "df.loc[:, 'BatchID'] = BATCH_ID\n",
    "df.loc[:, 'IsCurrent'] = 1\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sk = pd.read_sql_query(\"SELECT MAX(SK_AccountID) FROM dimAccount\", engine).iloc[0, 0]\n",
    "df.loc[:, 'SK_AccountID'] = range(max_sk + 1, max_sk + 1 + df.shape[0])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"CDC_FLAG\", \"CDC_DSN\", \"CA_B_ID\", \"CA_C_ID\", \"CA_ST_ID\"]\n",
    "df_insert = df[df[\"CDC_FLAG\"] == \"I\"].drop(columns=drop_cols)\n",
    "df_insert.info()\n",
    "df_insert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'SK_AccountID': sqlalchemy.types.Integer,\n",
    "    'AccountID': sqlalchemy.types.Integer,\n",
    "    'SK_BrokerID': sqlalchemy.types.Integer,\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'Status': sqlalchemy.types.String(10),\n",
    "    'AccountDesc': sqlalchemy.types.String(50),\n",
    "    'TaxStatus': sqlalchemy.types.SmallInteger,\n",
    "    'IsCurrent': sqlalchemy.types.Boolean,\n",
    "    'BatchID': sqlalchemy.types.SmallInteger,\n",
    "    'EffectiveDate': sqlalchemy.types.Date,\n",
    "    'EndDate': sqlalchemy.types.Date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert.to_sql('dimaccount', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"CDC_FLAG\", \"CDC_DSN\", \"CA_B_ID\", \"CA_C_ID\", \"CA_ST_ID\"]\n",
    "df_update = df[df[\"CDC_FLAG\"] == \"U\"].drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    for index, row in df_update.iterrows():\n",
    "        account_id = row['AccountID']\n",
    "        required_rows = df_update[df_update['AccountID'].isin([account_id])]\n",
    "        # only one update per day\n",
    "        if len(required_rows) > 1 and index != required_rows.index[-1]:\n",
    "            continue\n",
    "        effective_date = row['EffectiveDate']\n",
    "        query = f\"\"\"UPDATE dimAccount\n",
    "        SET IsCurrent = 0, EndDate = '{effective_date}'\n",
    "        WHERE IsCurrent = 1 AND AccountID = {account_id};\"\"\"\n",
    "        connection.execute(text(query))\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update.to_sql('dimaccount', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    \"CDC_FLAG\", \"CDC_DSN\", \"T_ID\", \"T_DTS\", \"T_ST_ID\",\n",
    "    \"T_TT_ID\", \"T_IS_CASH\", \"T_S_SYMB\", \"T_QTY\", \"T_BID_PRICE\",\n",
    "    \"T_CA_ID\", \"T_EXEC_NAME\", \"T_TRADE_PRICE\", \"T_CHRG\", \"T_COMM\", \"T_TAX\"\n",
    "]\n",
    "\n",
    "data_types = {\n",
    "    \"CDC_FLAG\": \"category\",\n",
    "    \"CDC_DSN\": \"int64\",\n",
    "    \"T_ID\": \"int64\",\n",
    "    \"T_DTS\": \"str\",\n",
    "    \"T_ST_ID\": \"str\",\n",
    "    \"T_TT_ID\": \"str\",\n",
    "    \"T_IS_CASH\": \"boolean\",\n",
    "    \"T_S_SYMB\": \"str\",\n",
    "    \"T_QTY\": \"int64\",\n",
    "    \"T_BID_PRICE\": \"float64\",\n",
    "    \"T_CA_ID\": \"int64\",\n",
    "    \"T_EXEC_NAME\": \"str\",\n",
    "    \"T_TRADE_PRICE\": \"float64\",\n",
    "    \"T_CHRG\": \"float64\",\n",
    "    \"T_COMM\": \"float64\",\n",
    "    \"T_TAX\": \"float64\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(DATA_DIR + \"Trade.txt\", sep='|', header=None, names=column_names, dtype=data_types, parse_dates=[\"T_DTS\"])\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_mapping = pd.read_sql_query(\"SELECT SK_DateID, DateValue FROM DimDate\", engine).set_index('DateValue')['SK_DateID'].to_dict()\n",
    "time_mapping = pd.read_sql_query(\"SELECT SK_TimeID, TimeValue FROM DimTime\", engine)\n",
    "\n",
    "def timedelta_to_time(td):\n",
    "    return (datetime.min + td).time()\n",
    "\n",
    "# Applying the function to the series\n",
    "time_mapping['TimeValue'] = time_mapping['TimeValue'].apply(timedelta_to_time)\n",
    "time_mapping = time_mapping.set_index('TimeValue')['SK_TimeID'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is a new Trade record (CDC_FLAG = “I”) then SK_CreateDateID and \n",
    "# SK_CreateTimeID must be set based on T_DTS. SK_CloseDateID and SK_CloseTimeID must be set to NULL\n",
    "insert_mask = df['CDC_FLAG'] == 'I'\n",
    "df.loc[insert_mask, 'SK_CreateDateID'] = df.loc[insert_mask, 'T_DTS'].dt.date.map(date_mapping)\n",
    "df.loc[insert_mask, 'SK_CreateTimeID'] = df.loc[insert_mask, 'T_DTS'].dt.time.map(time_mapping)\n",
    "df.loc[insert_mask, 'SK_CloseDateID'] = None\n",
    "df.loc[insert_mask, 'SK_CloseTimeID'] = None\n",
    "df.info()\n",
    "df.loc[insert_mask, ['SK_CreateDateID', 'SK_CreateTimeID', 'SK_CloseDateID', 'SK_CloseTimeID']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If T_ST_ID is “CMPT” or “CNCL”, SK_CloseDateID and SK_CloseTimeID must be set based on T_DTS.\n",
    "close_mask = df['T_ST_ID'].isin(['CMPT', 'CNCL'])\n",
    "df.loc[close_mask, 'SK_CloseDateID'] = df.loc[close_mask, 'T_DTS'].dt.date.map(date_mapping)\n",
    "df.loc[close_mask, 'SK_CloseTimeID'] = df.loc[close_mask, 'T_DTS'].dt.time.map(time_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    'T_ID': 'TradeID',\n",
    "    'T_IS_CASH': 'CashFlag',\n",
    "    'T_QTY': 'Quantity',\n",
    "    'T_BID_PRICE': 'BidPrice',\n",
    "    'T_EXEC_NAME': 'ExecutedBy',\n",
    "    'T_TRADE_PRICE': 'TradePrice',\n",
    "    'T_CHRG': 'Fee',\n",
    "    'T_COMM': 'Commission',\n",
    "    'T_TAX': 'Tax'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_mapping = pd.read_sql(\"SELECT ST_ID, ST_NAME FROM statustype\", engine).set_index(\"ST_ID\")[\"ST_NAME\"].to_dict()\n",
    "trade_type_mapping = pd.read_sql(\"SELECT TT_ID, TT_NAME FROM tradetype\", engine).set_index(\"TT_ID\")[\"TT_NAME\"].to_dict()\n",
    "df[\"Status\"] = df[\"T_ST_ID\"].map(status_mapping)\n",
    "df[\"Type\"] = df[\"T_TT_ID\"].map(trade_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching security and account info in one go\n",
    "security_info = pd.read_sql(\"SELECT Symbol, SK_SecurityID, SK_CompanyID FROM dimsecurity WHERE IsCurrent = 1\", engine).set_index('Symbol')\n",
    "account_info = pd.read_sql(\"SELECT AccountID, SK_AccountID, SK_CustomerID, SK_BrokerID FROM dimaccount WHERE IsCurrent = 1\",\n",
    "                           engine).set_index('AccountID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'SK_SecurityID'] = df.loc[:, 'T_S_SYMB'].map(security_info['SK_SecurityID'].to_dict())\n",
    "df.loc[:, 'SK_CompanyID'] = df.loc[:, 'T_S_SYMB'].map(security_info['SK_CompanyID'].to_dict())\n",
    "df.loc[:, 'SK_AccountID'] = df.loc[:, 'T_CA_ID'].map(account_info['SK_AccountID'].to_dict())\n",
    "df.loc[:, 'SK_CustomerID'] = df.loc[:, 'T_CA_ID'].map(account_info['SK_CustomerID'].to_dict())\n",
    "df.loc[:, 'SK_BrokerID'] = df.loc[:, 'T_CA_ID'].map(account_info['SK_BrokerID'].to_dict())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BatchID'] = BATCH_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "invalid_trades = df[\n",
    "    (df[\"Commission\"].notnull())\n",
    "    & (df[\"Commission\"] > (df[\"TradePrice\"] * df[\"Quantity\"]))\n",
    "]\n",
    "\n",
    "if invalid_trades.shape[0] > 0:\n",
    "    # Create lists without using iterrows\n",
    "    MessageSource = [\"DimTrade\"] * len(invalid_trades)\n",
    "    MessageType = [\"Alert\"] * len(invalid_trades)\n",
    "    MessageText = [\"Invalid trade commission\"] * len(invalid_trades)\n",
    "    MessageData = [\n",
    "        \"T_ID = \"\n",
    "        + invalid_trades[\"TradeID\"].astype(str)\n",
    "        + \", T_COMM = \"\n",
    "        + invalid_trades[\"Commission\"].astype(str)\n",
    "    ]\n",
    "    # Convert MessageData from a list of Series to a list of strings\n",
    "    MessageData = MessageData[0].tolist()\n",
    "    \n",
    "    query = f\"\"\"INSERT INTO Dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData)\n",
    "    VALUES \"\"\"\n",
    "    for i in range(len(MessageSource)):\n",
    "        query += f\"\"\"('{pd.Timestamp(\"now\")}', {BATCH_ID}, '{MessageSource[i]}', '{MessageText[i]}', '{MessageType[i]}', '{MessageData[i]}'),\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(query[:-1]))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for invalid trade fees\n",
    "invalid_fee_trades = df[\n",
    "    (df[\"Fee\"].notnull())\n",
    "    & (df[\"Fee\"] > (df[\"TradePrice\"] / df[\"Quantity\"]))\n",
    "]\n",
    "\n",
    "if len(invalid_fee_trades) > 0:\n",
    "    # Create the required lists\n",
    "    MessageSource = [\"DimTrade\"] * len(invalid_fee_trades)\n",
    "    MessageType = [\"Alert\"] * len(invalid_fee_trades)\n",
    "    MessageText = [\"Invalid trade fee\"] * len(invalid_fee_trades)\n",
    "    \n",
    "    # Vectorized operation for MessageData\n",
    "    MessageData = (\n",
    "        \"T_ID = \"\n",
    "        + invalid_fee_trades[\"TradeID\"].astype(str)\n",
    "        + \", T_CHRG = \"\n",
    "        + invalid_fee_trades[\"Fee\"].astype(str)\n",
    "    )\n",
    "    MessageData = MessageData.tolist()\n",
    "    \n",
    "    query = \"\"\"INSERT INTO Dimessages (MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData) VALUES \"\"\"\n",
    "    for i in range(len(MessageSource)):\n",
    "        query += f\"\"\"('{pd.Timestamp(\"now\")}', {BATCH_ID}, '{MessageSource[i]}', '{MessageText[i]}', '{MessageType[i]}', '{MessageData[i]}'),\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(query[:-1]))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    'TradeID': sqlalchemy.types.Integer,\n",
    "    'SK_BrokerID': sqlalchemy.types.Integer,\n",
    "    'SK_CreateDateID': sqlalchemy.types.Integer,\n",
    "    'SK_CreateTimeID': sqlalchemy.types.Integer,\n",
    "    'SK_CloseDateID': sqlalchemy.types.Integer,\n",
    "    'SK_CloseTimeID': sqlalchemy.types.Integer,\n",
    "    'Status': sqlalchemy.types.CHAR(10),\n",
    "    'Type': sqlalchemy.types.CHAR(12),\n",
    "    'CashFlag': sqlalchemy.types.Boolean,\n",
    "    'SK_SecurityID': sqlalchemy.types.Integer,\n",
    "    'SK_CompanyID': sqlalchemy.types.Integer,\n",
    "    'Quantity': sqlalchemy.types.Integer,\n",
    "    'BidPrice': sqlalchemy.types.Numeric(8, 2),\n",
    "    'SK_CustomerID': sqlalchemy.types.Integer,\n",
    "    'SK_AccountID': sqlalchemy.types.Integer,\n",
    "    'ExecutedBy': sqlalchemy.types.CHAR(64),\n",
    "    'TradePrice': sqlalchemy.types.Numeric(8, 2),\n",
    "    'Fee': sqlalchemy.types.Numeric(10, 2),\n",
    "    'Commission': sqlalchemy.types.Numeric(10, 2),\n",
    "    'Tax': sqlalchemy.types.Numeric(10, 2),\n",
    "    'BatchID': sqlalchemy.types.SmallInteger\n",
    "}\n",
    "keep_cols = [col for col in df.columns if col in sql_dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert = df.loc[df[\"CDC_FLAG\"] == \"I\", keep_cols]\n",
    "df_insert.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert.to_sql('dimtrade', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update = df.loc[df[\"CDC_FLAG\"] == \"U\", keep_cols]\n",
    "df_update.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    for index, row in df_update.iterrows():\n",
    "        trade_id = row['TradeID']\n",
    "        required_rows = df_update[df_update['TradeID'] == trade_id]\n",
    "        # only update using latest value\n",
    "        if len(required_rows) > 1 and index != required_rows.index[-1]:\n",
    "            continue\n",
    "        str_cols = ['Status', 'Type']\n",
    "        query_parts = []\n",
    "        for key, value in row.to_dict().items():\n",
    "            if key in ('TradeID', 'SK_CreateDateID', 'SK_CreateTimeID'):\n",
    "                continue\n",
    "            if pd.isna(value):\n",
    "                query_part = f\"{key} = NULL\"\n",
    "            elif key in str_cols:\n",
    "                value = value.replace(\"'\", \"\\\\'\")\n",
    "                query_part = f\"{key} = '{value}'\"\n",
    "            else:  # for numeric and boolean types\n",
    "                query_part = f\"{key} = {value}\"\n",
    "            query_parts.append(query_part)\n",
    "        query = f\"UPDATE dimtrade SET {', '.join(query_parts)} WHERE TradeID = {trade_id};\"\n",
    "        connection.execute(text(query))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FactCashBalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_DIR + \"CashTransaction.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"CDC_FLAG\",\n",
    "        \"CDC_DSN\",\n",
    "        \"CT_CA_ID\",\n",
    "        \"CT_DTS\",\n",
    "        \"CT_AMT\",\n",
    "        \"CT_NAME\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"CDC_FLAG\": \"category\",\n",
    "        \"CDC_DSN\": \"int64\",\n",
    "        \"CT_CA_ID\": \"uint32\",\n",
    "        \"CT_DTS\": \"str\",\n",
    "        \"CT_AMT\": \"float64\",\n",
    "        \"CT_NAME\": \"str\"\n",
    "    },\n",
    "    parse_dates=[\"CT_DTS\"],\n",
    ")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_info = pd.read_sql(\n",
    "    \"SELECT AccountID, SK_AccountID, SK_CustomerID FROM dimaccount WHERE IsCurrent = 1\",\n",
    "    engine,\n",
    ").set_index('AccountID')\n",
    "df.loc[:, 'SK_AccountID'] = df.loc[:, 'CT_CA_ID'].map(account_info['SK_AccountID'].to_dict())\n",
    "df.loc[:, 'SK_CustomerID'] = df.loc[:, 'CT_CA_ID'].map(account_info['SK_CustomerID'].to_dict())\n",
    "\n",
    "date_info = pd.read_sql(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info[\"DateValue\"] = pd.to_datetime(date_info[\"DateValue\"])\n",
    "df.loc[:, 'SK_DateID'] = df.loc[:, 'CT_DTS'].dt.date.map(date_info.set_index('DateValue')['SK_DateID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'BatchID'] = BATCH_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by account ID and transaction date\n",
    "df.sort_values(by=['CT_CA_ID', 'CT_DTS'], inplace=True)\n",
    "\n",
    "# Create a new column to store the prior cash amount\n",
    "df['PriorCash'] = df.groupby('CT_CA_ID')['CT_AMT'].cumsum() - df['CT_AMT']\n",
    "df['PriorCash'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the cash balance\n",
    "df['Cash'] = df['PriorCash'] + df['CT_AMT']\n",
    "df = df.groupby(['CT_CA_ID', 'SK_DateID']).last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_CustomerID\": sqlalchemy.types.Integer,\n",
    "    \"SK_AccountID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID\": sqlalchemy.types.Integer,\n",
    "    \"Cash\": sqlalchemy.types.DECIMAL(precision=15, scale=2),\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger\n",
    "}\n",
    "keep_cols = [col for col in df.columns if col in sql_dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert = df.loc[df[\"CDC_FLAG\"] == \"I\", keep_cols]\n",
    "df_insert.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert.to_sql('factcashbalances', engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FactHoldings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL queries\n",
    "sql_commands = [\n",
    "    \"DROP TABLE IF EXISTS TempHoldingHistory\",\n",
    "    \"\"\"\n",
    "    CREATE TEMPORARY TABLE TempHoldingHistory (\n",
    "        CDC_FLAG CHAR(1) NOT NULL,\n",
    "        CDC_DSN INT UNSIGNED NOT NULL,\n",
    "        HH_H_T_ID INT UNSIGNED NOT NULL,\n",
    "        HH_T_ID INT UNSIGNED NOT NULL,\n",
    "        HH_BEFORE_QTY INT NOT NULL,\n",
    "        HH_AFTER_QTY INT NOT NULL\n",
    "    )\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    LOAD DATA LOCAL INFILE 'E:\\\\\\\\Documents\\\\\\\\BDMA\\\\\\\\ULB\\\\\\\\Data Warehouses\\\\\\\\tpc-di\\\\\\\\TPC-DI\\\\\\\\data\\\\\\\\sf5\\\\\\\\Batch{BATCH_ID}\\\\\\\\HoldingHistory.txt'\n",
    "    INTO TABLE TempHoldingHistory\n",
    "    FIELDS TERMINATED BY '|'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    (CDC_FLAG, CDC_DSN, HH_H_T_ID, HH_T_ID, HH_BEFORE_QTY, HH_AFTER_QTY)\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    INSERT INTO FactHoldings (TradeID, CurrentTradeID, SK_CustomerID, SK_AccountID, SK_SecurityID, SK_CompanyID, SK_DateID, SK_TimeID, CurrentPrice, CurrentHolding, BatchID)\n",
    "    SELECT \n",
    "        thh.HH_H_T_ID AS TradeID,\n",
    "        thh.HH_T_ID AS CurrentTradeID,\n",
    "        dt.SK_CustomerID,\n",
    "        dt.SK_AccountID,\n",
    "        dt.SK_SecurityID,\n",
    "        dt.SK_CompanyID,\n",
    "        dt.SK_CloseDateID AS SK_DateID,\n",
    "        dt.SK_CloseTimeID AS SK_TimeID,\n",
    "        dt.TradePrice AS CurrentPrice,\n",
    "        thh.HH_AFTER_QTY AS CurrentHolding,\n",
    "        {BATCH_ID} AS BatchID\n",
    "    FROM \n",
    "        TempHoldingHistory thh\n",
    "    JOIN \n",
    "        DimTrade dt ON thh.HH_T_ID = dt.TradeID\n",
    "    \"\"\",\n",
    "    \"DROP TABLE TempHoldingHistory\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the queries\n",
    "with engine.connect() as connection:\n",
    "    for sql in sql_commands:\n",
    "        connection.execute(text(sql))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FactWatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_DIR + \"WatchHistory.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"CDC_FLAG\",\n",
    "        \"CDC_DSN\",\n",
    "        \"W_C_ID\",\n",
    "        \"W_S_SYMB\",\n",
    "        \"W_DTS\",\n",
    "        \"W_ACTION\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"CDC_FLAG\": \"category\",\n",
    "        \"CDC_DSN\": \"int64\",\n",
    "        \"W_C_ID\": \"uint32\",\n",
    "        \"W_S_SYMB\": \"str\",\n",
    "        \"W_DTS\": \"str\",\n",
    "        \"W_ACTION\": \"str\"\n",
    "    },\n",
    "    parse_dates=[\"W_DTS\"]\n",
    ")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_mapping = pd.read_sql_query(\n",
    "    \"SELECT CustomerID, SK_CustomerID FROM dimcustomer WHERE IsCurrent = 1\",\n",
    "    engine).set_index('CustomerID')['SK_CustomerID'].to_dict()\n",
    "\n",
    "security_mapping = pd.read_sql_query(\n",
    "    \"SELECT Symbol, SK_SecurityID FROM dimsecurity WHERE IsCurrent = 1\",\n",
    "    engine).set_index('Symbol')['SK_SecurityID'].to_dict()\n",
    "\n",
    "date_info = pd.read_sql_query(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info['DateValue'] = pd.to_datetime(date_info['DateValue'])\n",
    "date_mapping = date_info.set_index('DateValue')['SK_DateID'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'SK_CustomerID'] = df.loc[:, 'W_C_ID'].map(customer_mapping)\n",
    "df.loc[:, 'SK_SecurityID'] = df.loc[:, 'W_S_SYMB'].map(security_mapping)\n",
    "df.loc[:, 'SK_DateID_DatePlaced'] = df.loc[:, 'W_DTS'].dt.date.map(date_mapping)\n",
    "df.loc[:, 'BatchID'] = BATCH_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask for rows where W_ACTION is 'CNCL'\n",
    "mask_cncl = df['W_ACTION'] == 'CNCL'\n",
    "df.loc[mask_cncl, 'SK_DateID_DateRemoved'] = df.loc[mask_cncl, 'W_DTS'].dt.date.map(date_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([\"W_C_ID\", \"W_S_SYMB\"]).first().reset_index()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_records_dict = {'SK_CustomerID': [], 'SK_SecurityID': [], 'SK_DateID_DatePlaced': [], 'SK_DateID_DateRemoved': [], 'BatchID': []}\n",
    "lst = [0, 0, 0]\n",
    "\n",
    "def update_or_insert(row, new_records):\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        # Check if the record exists in factwatches\n",
    "        existing = pd.read_sql_query(\n",
    "            \"SELECT * FROM factwatches WHERE SK_CustomerID = {} AND SK_SecurityID = {}\".format(row['SK_CustomerID'], row['SK_SecurityID']),\n",
    "            connection\n",
    "        )\n",
    "\n",
    "        # Handle NaN in SK_DateID_DateRemoved\n",
    "        date_removed = \"NULL\" if pd.isna(row['SK_DateID_DateRemoved']) else row['SK_DateID_DateRemoved']\n",
    "\n",
    "        if not existing.empty:\n",
    "            # Update the existing record\n",
    "            query = \"\"\"\n",
    "            UPDATE factwatches\n",
    "            SET SK_DateID_DatePlaced = {}, SK_DateID_DateRemoved = {}, BatchID = {}\n",
    "            WHERE SK_CustomerID = {} AND SK_SecurityID = {}\n",
    "            \"\"\".format(row['SK_DateID_DatePlaced'], date_removed, row['BatchID'], row['SK_CustomerID'], row['SK_SecurityID'])\n",
    "            connection.execute(text(query))\n",
    "            lst[0] += 1\n",
    "            connection.commit()\n",
    "        else:\n",
    "            # Check for matching records in dimcustomer and dimsecurity\n",
    "            customer_sk = pd.read_sql_query(\n",
    "                \"SELECT SK_CustomerID FROM dimcustomer WHERE CustomerID = {}\".format(row['W_C_ID']),\n",
    "                connection\n",
    "            )\n",
    "            security_sk = pd.read_sql_query(\n",
    "                \"SELECT SK_SecurityID FROM dimsecurity WHERE Symbol = '{}'\".format(row['W_S_SYMB']),\n",
    "                connection\n",
    "            )\n",
    "\n",
    "            # Check for each combination of SKs in factwatches\n",
    "            for cust_id in customer_sk['SK_CustomerID']:\n",
    "                for sec_id in security_sk['SK_SecurityID']:\n",
    "                    existing = pd.read_sql_query(\n",
    "                        \"SELECT * FROM factwatches WHERE SK_CustomerID = {} AND SK_SecurityID = {}\".format(cust_id, sec_id),\n",
    "                        connection\n",
    "                    )\n",
    "                    if not existing.empty:\n",
    "                        # Update the existing record with new values\n",
    "                        query = \"\"\"\n",
    "                        UPDATE factwatches\n",
    "                        SET SK_DateID_DatePlaced = {}, SK_DateID_DateRemoved = {}, BatchID = {}\n",
    "                        WHERE SK_CustomerID = {} AND SK_SecurityID = {}\n",
    "                        \"\"\".format(row['SK_DateID_DatePlaced'], date_removed, row['BatchID'], cust_id, sec_id)\n",
    "                        connection.execute(text(query))\n",
    "                        connection.commit()\n",
    "                        lst[1] += 1\n",
    "                        return\n",
    "\n",
    "            # Accumulate new record for bulk insert\n",
    "            new_records_dict['SK_CustomerID'].append(row['SK_CustomerID'])\n",
    "            new_records_dict['SK_SecurityID'].append(row['SK_SecurityID'])\n",
    "            new_records_dict['SK_DateID_DatePlaced'].append(row['SK_DateID_DatePlaced'])\n",
    "            new_records_dict['SK_DateID_DateRemoved'].append(row['SK_DateID_DateRemoved'] if pd.notna(row['SK_DateID_DateRemoved']) else None)\n",
    "            new_records_dict['BatchID'].append(row['BatchID'])\n",
    "            lst[2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame and accumulate new records\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    update_or_insert(row, new_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_CustomerID\": sqlalchemy.types.Integer,\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID_DatePlaced\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID_DateRemoved\": sqlalchemy.types.Integer,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger\n",
    "}\n",
    "# Bulk insert new records\n",
    "new_records_df = pd.DataFrame(new_records_dict)\n",
    "new_records_df.to_sql('factwatches', con=engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactMarketHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_DIR + \"DailyMarket.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"CDC_FLAG\",\n",
    "        \"CDC_DSN\",\n",
    "        \"DM_DATE\",\n",
    "        \"DM_S_SYMB\",\n",
    "        \"DM_CLOSE\",\n",
    "        \"DM_HIGH\",\n",
    "        \"DM_LOW\",\n",
    "        \"DM_VOL\",\n",
    "    ],\n",
    "    dtype={\n",
    "        \"CDC_FLAG\": \"category\",\n",
    "        \"CDC_DSN\": \"int64\",\n",
    "        \"DM_DATE\": \"str\",\n",
    "        \"DM_S_SYMB\": \"str\",\n",
    "        \"DM_CLOSE\": \"float32\",\n",
    "        \"DM_HIGH\": \"float32\",\n",
    "        \"DM_LOW\": \"float32\",\n",
    "        \"DM_VOL\": \"int64\",\n",
    "    },\n",
    "    parse_dates=[\"DM_DATE\"],\n",
    ")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClosePrice, DayHigh, DayLow, and Volume are copied from DM_CLOSE, DM_HIGH,\n",
    "# DM_LOW, and DM_VOL respectively.\n",
    "df[\"ClosePrice\"] = df[\"DM_CLOSE\"]\n",
    "df[\"DayHigh\"] = df[\"DM_HIGH\"]\n",
    "df[\"DayLow\"] = df[\"DM_LOW\"]\n",
    "df[\"Volume\"] = df[\"DM_VOL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_info = pd.read_sql(\"SELECT Symbol, SK_SecurityID, SK_CompanyID FROM dimsecurity WHERE IsCurrent = 1\",\n",
    "                            engine).set_index(\"Symbol\")\n",
    "security_mapping = security_info['SK_SecurityID'].to_dict()\n",
    "company_mapping = security_info['SK_CompanyID'].to_dict()\n",
    "\n",
    "df.loc[:, 'SK_SecurityID'] = df.loc[:, 'DM_S_SYMB'].map(security_mapping)\n",
    "df.loc[:, 'SK_CompanyID'] = df.loc[:, 'DM_S_SYMB'].map(company_mapping)\n",
    "\n",
    "date_info = pd.read_sql_query(\"SELECT DateValue, SK_DateID FROM dimdate\", engine)\n",
    "date_info['DateValue'] = pd.to_datetime(date_info['DateValue'])\n",
    "date_mapping = date_info.set_index('DateValue')['SK_DateID'].to_dict()\n",
    "df.loc[:, 'SK_DateID'] = df.loc[:, 'DM_DATE'].dt.date.map(date_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sort the DataFrame\n",
    "df.sort_values(by='DM_DATE', inplace=True)\n",
    "\n",
    "# Step 3 & 4: Group by 'DM_S_SYMB' and apply rolling max\n",
    "rolling_max = df.groupby('DM_S_SYMB').rolling('365D', on='DM_DATE')['DM_HIGH'].max()\n",
    "\n",
    "# Reset index to make merging easier\n",
    "rolling_max = rolling_max.reset_index()\n",
    "\n",
    "# Step 5: Merge with the original DataFrame\n",
    "df = df.merge(rolling_max, on=['DM_S_SYMB', 'DM_DATE'], suffixes=('', '_52WeekHigh'))\n",
    "\n",
    "# Rename the column for clarity\n",
    "df.rename(columns={'DM_HIGH_52WeekHigh': 'FiftyTwoWeekHigh'}, inplace=True)\n",
    "\n",
    "rolling_rank = (\n",
    "    df.groupby(\"DM_S_SYMB\")\n",
    "    .rolling(\"365D\", on=\"DM_DATE\")[\"DM_HIGH\"]\n",
    "    .rank(method=\"average\", ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"DM_HIGH\": \"Rank\"})\n",
    ")\n",
    "rolling_rank[\"Rank\"] = rolling_rank[\"Rank\"].astype(\"uint32\")\n",
    "# Apply the mask to select DM_DATE only for those rows, then forward fill\n",
    "mask = rolling_rank['Rank'] == 1\n",
    "rolling_rank['SK_FiftyTwoWeekHighDate'] = rolling_rank['DM_DATE'].where(mask).ffill()\n",
    "rolling_rank['SK_FiftyTwoWeekHighDate'] = rolling_rank['SK_FiftyTwoWeekHighDate'].dt.date.map(date_mapping)\n",
    "\n",
    "df = pd.concat([df, rolling_rank['SK_FiftyTwoWeekHighDate']], axis=1)\n",
    "\n",
    "df.sort_values(by='DM_DATE', inplace=True)\n",
    "# Step 3 & 4: Group by 'DM_S_SYMB' and apply rolling min\n",
    "rolling_min = df.groupby('DM_S_SYMB').rolling('365D', on='DM_DATE')['DM_LOW'].min()\n",
    "# Reset index to make merging easier\n",
    "rolling_min = rolling_min.reset_index()\n",
    "# Step 5: Merge with the original DataFrame\n",
    "df = df.merge(rolling_min, on=['DM_S_SYMB', 'DM_DATE'], suffixes=('', '_52WeekLow'))\n",
    "# Rename the column for clarity\n",
    "df.rename(columns={'DM_LOW_52WeekLow': 'FiftyTwoWeekLow'}, inplace=True)\n",
    "\n",
    "rolling_rank = (\n",
    "    df.groupby(\"DM_S_SYMB\")\n",
    "    .rolling(\"365D\", on=\"DM_DATE\")[\"DM_LOW\"]\n",
    "    .rank(method=\"average\", ascending=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"DM_LOW\": \"Rank\"})\n",
    ")\n",
    "rolling_rank[\"Rank\"] = rolling_rank[\"Rank\"].astype(\"uint32\")\n",
    "# Apply the mask to select DM_DATE only for those rows, then forward fill\n",
    "mask = rolling_rank['Rank'] == 1\n",
    "rolling_rank['SK_FiftyTwoWeekLowDate'] = rolling_rank['DM_DATE'].where(mask).ffill()\n",
    "rolling_rank['SK_FiftyTwoWeekLowDate'] = rolling_rank['SK_FiftyTwoWeekLowDate'].dt.date.map(date_mapping)\n",
    "\n",
    "df = pd.concat([df, rolling_rank['SK_FiftyTwoWeekLowDate']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SK_SecurityID'] = df['SK_SecurityID'].astype('uint32')\n",
    "df['SK_CompanyID'] = df['SK_CompanyID'].astype('uint32')\n",
    "df['SK_DateID'] = df['SK_DateID'].astype('uint32')\n",
    "df['FiftyTwoWeekHigh'] = df['FiftyTwoWeekHigh'].astype('float32')\n",
    "df['SK_FiftyTwoWeekHighDate'] = df['SK_FiftyTwoWeekHighDate'].astype('uint32')\n",
    "df['FiftyTwoWeekLow'] = df['FiftyTwoWeekLow'].astype('float32')\n",
    "df['SK_FiftyTwoWeekLowDate'] = df['SK_FiftyTwoWeekLowDate'].astype('uint32')\n",
    "df['DM_S_SYMB'] = df['DM_S_SYMB'].astype('category')\n",
    "\n",
    "df.drop(columns=['DM_HIGH', 'DM_LOW', 'DM_VOL', 'DM_CLOSE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_mapping = pd.read_sql(\"SELECT Symbol, Dividend FROM dimsecurity WHERE IsCurrent = 1\", engine).set_index(\"Symbol\")[\"Dividend\"]\n",
    "df.loc[:, 'Yield'] = df.loc[:, 'DM_S_SYMB'].map(security_mapping) / df.loc[:, 'ClosePrice'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BatchID'] = BATCH_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"SK_CompanyID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID\": sqlalchemy.types.Integer,\n",
    "    \"Yield\": sqlalchemy.types.DECIMAL(precision=5, scale=2),\n",
    "    \"FiftyTwoWeekHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekHighDate\": sqlalchemy.types.Integer,\n",
    "    \"FiftyTwoWeekLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekLowDate\": sqlalchemy.types.Integer,\n",
    "    \"ClosePrice\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"Volume\": sqlalchemy.types.BigInteger,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger,\n",
    "    \"DM_DATE\": sqlalchemy.types.Date,\n",
    "    \"DM_S_SYMB\": sqlalchemy.types.CHAR(16),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"CDC_FLAG\", \"CDC_DSN\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('tempfactmarketprice', engine, if_exists='replace', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_commands = [\n",
    "    \"CREATE INDEX idx_sk_companyid ON tempfactmarketprice(SK_CompanyID);\",\n",
    "    \"\"\"SELECT fmp.ClosePrice / T.Sum_EPS AS PERatio\n",
    "        FROM tempfactmarketprice fmp\n",
    "        LEFT JOIN (SELECT \n",
    "            c.CompanyID, \n",
    "            c.SK_CompanyID AS SKCID, \n",
    "            f.FI_QTR_START_DATE,\n",
    "            SUM(f.FI_BASIC_EPS) OVER (\n",
    "                PARTITION BY c.CompanyID \n",
    "                ORDER BY f.FI_QTR_START_DATE \n",
    "                ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "            ) AS Sum_EPS\n",
    "        FROM financial f RIGHT JOIN dimCompany c ON f.SK_CompanyID = c.SK_CompanyID\n",
    "        ORDER BY c.CompanyID, f.FI_QTR_START_DATE) T\n",
    "        ON T.SKCID = fmp.SK_CompanyID\n",
    "        AND T.FI_QTR_START_DATE < fmp.DM_DATE \n",
    "        AND T.FI_QTR_START_DATE >= DATE_SUB(fmp.DM_DATE, INTERVAL 3 MONTH);\"\"\",\n",
    "    \"DROP TABLE tempfactmarketprice;\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(text(sql_commands[0]))\n",
    "    connection.commit()\n",
    "peratio = pd.read_sql_query(sql_commands[1], engine)\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(sql_commands[2]))\n",
    "    connection.commit()\n",
    "peratio.info()\n",
    "peratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, peratio], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_earnings = pd.DataFrame(df.loc[df['PERatio'].isna(), 'DM_S_SYMB'])\n",
    "no_earnings['MessageDateAndTime'] = datetime.now()\n",
    "no_earnings['BatchID'] = BATCH_ID\n",
    "no_earnings['MessageSource'] = 'FactMarketHistory'\n",
    "no_earnings['MessageText'] = 'No earnings for company'\n",
    "no_earnings['MessageType'] = 'Alert'\n",
    "no_earnings.rename(columns={\"DM_S_SYMB\":\"MessageData\"}, inplace=True)\n",
    "no_earnings['MessageData'] = \"DM_S_SYMB = \" + no_earnings['MessageData'].astype(str)\n",
    "\n",
    "sql_dtypes = {\n",
    "    \"MessageDateAndTime\": sqlalchemy.types.DATETIME,\n",
    "    \"BatchID\": sqlalchemy.types.Integer,\n",
    "    \"MessageSource\": sqlalchemy.types.CHAR(30),\n",
    "    \"MessageText\": sqlalchemy.types.CHAR(50),\n",
    "    \"MessageType\": sqlalchemy.types.CHAR(12),\n",
    "    \"MessageData\": sqlalchemy.types.CHAR(100)\n",
    "}\n",
    "no_earnings.to_sql('dimessages', con=engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"DM_DATE\", \"DM_S_SYMB\"], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dtypes = {\n",
    "    \"SK_SecurityID\": sqlalchemy.types.Integer,\n",
    "    \"SK_CompanyID\": sqlalchemy.types.Integer,\n",
    "    \"SK_DateID\": sqlalchemy.types.Integer,\n",
    "    \"PERatio\": sqlalchemy.types.DECIMAL(precision=10, scale=2),\n",
    "    \"Yield\": sqlalchemy.types.DECIMAL(precision=5, scale=2),\n",
    "    \"FiftyTwoWeekHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekHighDate\": sqlalchemy.types.Integer,\n",
    "    \"FiftyTwoWeekLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"SK_FiftyTwoWeekLowDate\": sqlalchemy.types.Integer,\n",
    "    \"ClosePrice\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayHigh\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"DayLow\": sqlalchemy.types.DECIMAL(precision=8, scale=2),\n",
    "    \"Volume\": sqlalchemy.types.BigInteger,\n",
    "    \"BatchID\": sqlalchemy.types.SmallInteger,\n",
    "}\n",
    "len(sql_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('factmarkethistory', con=engine, if_exists='append', index=False, dtype=sql_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = ['..\\\\data\\\\sf5\\\\Batch1\\\\', '..\\\\data\\\\sf5\\\\Batch2\\\\', '..\\\\data\\\\sf5\\\\Batch3\\\\']\n",
    "BATCH_FILES = ['..\\\\data\\\\sf5\\\\Batch1_audit.csv', '..\\\\data\\\\sf5\\\\Batch2_audit.csv', '..\\\\data\\\\sf5\\\\Batch3_audit.csv']\n",
    "audit_dtypes = {\"DataSet\": str, \" BatchID\": int, \"Date\": 'datetime64', \" Attribute\": str, \" Value\": 'Int64', \" DValue\": 'float64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BATCH_FILES[0], dtype=audit_dtypes)\n",
    "df = pd.concat([df, pd.read_csv(BATCH_FILES[1], dtype=audit_dtypes)])\n",
    "df = pd.concat([df, pd.read_csv(BATCH_FILES[2], dtype=audit_dtypes)])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.read_csv(r\"..\\data\\sf5\\Generator_audit.csv\", dtype=audit_dtypes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in BATCH_DIRS:\n",
    "    files = os.listdir(folder)\n",
    "    files = list(filter(lambda x: \"_audit\" in x, files))\n",
    "    for file in files:\n",
    "        df = pd.concat([df, pd.read_csv(folder + file, dtype=audit_dtypes)])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"drop table audit\"\"\"\n",
    "with engine.connect() as cxn:\n",
    "    cxn.execute(text(create_table))\n",
    "    cxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = \"\"\"CREATE TABLE Audit (\n",
    "    DataSet CHAR(20) NOT NULL,\n",
    "    BatchID SMALLINT UNSIGNED,\n",
    "    Date DATE,\n",
    "    Attribute CHAR(50) NOT NULL,\n",
    "    Value BIGINT,\n",
    "    DValue NUMERIC(16, 5)\n",
    ");\"\"\"\n",
    "with engine.connect() as cxn:\n",
    "    cxn.execute(text(create_table))\n",
    "    cxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\n",
    "    \"audit\",\n",
    "    engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    dtype={\n",
    "        \"DataSet\": sqlalchemy.types.CHAR(20),\n",
    "        \"BatchID\": sqlalchemy.types.SmallInteger,\n",
    "        \"Date\": sqlalchemy.types.Date,\n",
    "        \"Attribute\": sqlalchemy.types.CHAR(50),\n",
    "        \"Value\": sqlalchemy.types.BigInteger,\n",
    "        \"DValue\": sqlalchemy.types.Numeric(precision=15, scale=5),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"E:\\Documents\\BDMA\\ULB\\Data Warehouses\\tpc-di\\TPC-DI\\validation\\tpcdi_audit.sql\", encoding='utf-8') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content.split(\"union\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
